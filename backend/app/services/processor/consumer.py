import logging
import asyncio
import redis.asyncio as redis
from typing import Dict, List, Any, Optional
import time

logger = logging.getLogger(__name__)

class StreamConsumer:
    """Redis Stream ì†Œë¹„ ë° ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, redis_url: str, adapter_factory, repository, batch_size: int = 100):
        self.redis_url = redis_url
        self.redis_client = None
        self.adapter_factory = adapter_factory
        self.repository = repository
        self.batch_size = batch_size
        self.realtime_streams = {
            # Highest priority: Stocks (Finnhub)
            "finnhub:realtime": "finnhub_group",
            "alpaca:realtime": "alpaca_group",
            # Crypto
            "binance:realtime": "binance_group",
            "coinbase:realtime": "coinbase_group",
            # FX/Other
            "swissquote:realtime": "swissquote_group",
            "polygon:realtime": "polygon_group",
            "twelvedata:realtime": "twelvedata_group",
            "kis:realtime": "kis_group",
        }
        self.last_heartbeat = 0
        self.consecutive_errors = 0

    async def connect(self) -> bool:
        """Redis ì—°ê²° ì´ˆê¸°í™”"""
        try:
            if self.redis_client:
                try:
                    await self.redis_client.ping()
                    return True
                except Exception:
                    logger.warning("Existing Redis connection failed ping, closing...")
                    await self.close()
                
            self.redis_client = await redis.from_url(
                self.redis_url, 
                decode_responses=False, # We handle decoding manually for robustness
                socket_connect_timeout=5,
                socket_keepalive=True,
                retry_on_timeout=True
            )
            await self.redis_client.ping()
            logger.info(f"âœ… Redis ì—°ê²° ì„±ê³µ: {self.redis_url}")
            self.consecutive_errors = 0
            return True
        except Exception as e:
            logger.error(f"âŒ Redis ì—°ê²° ì‹¤íŒ¨: {e}")
            return False

    async def close(self):
        """Redis ì—°ê²° ì¢…ë£Œ"""
        if self.redis_client:
            try:
                await self.redis_client.close()
            except Exception:
                pass
            self.redis_client = None

    async def _reconnect(self):
        """ì¬ì—°ê²° ì‹œë„"""
        logger.info("ğŸ”„ Redis ì¬ì—°ê²° ì‹œë„...")
        await self.close()
        await asyncio.sleep(1) # ì ì‹œ ëŒ€ê¸°
        return await self.connect()

    async def process_streams(self) -> int:
        """ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ë°ì´í„° ì²˜ë¦¬"""
        # ì—°ê²° í™•ì¸
        if not self.redis_client:
            if not await self.connect():
                return 0

        processed_count = 0
        records_to_save = []
        ack_items = []

        try:
            # Heartbeat logging (every 60s)
            now = time.time()
            if now - self.last_heartbeat > 60:
                logger.info(f"ğŸ’“ StreamConsumer Heartbeat - Connected: {bool(self.redis_client)}")
                self.last_heartbeat = now

            # Consumer Group ìƒì„±
            for stream_name, group_name in self.realtime_streams.items():
                try:
                    # mkstream=True ensures stream exists
                    await self.redis_client.xgroup_create(
                        name=stream_name, groupname=group_name, id="0", mkstream=True
                    )
                except Exception as e:
                    if "BUSYGROUP" not in str(e):
                        logger.warning(f"xgroup_create error {stream_name}: {e}")

            # Pending ë©”ì‹œì§€ ë¨¼ì € ì²˜ë¦¬
            for stream_name, group_name in self.realtime_streams.items():
                try:
                    pending_info = await self.redis_client.xpending(stream_name, group_name)
                    if pending_info and pending_info.get('pending', 0) > 0:
                        pending_data = await self.redis_client.xreadgroup(
                            groupname=group_name,
                            consumername="processor_worker",
                            streams={stream_name: "0"},
                            count=min(self.batch_size, pending_info['pending']),
                            block=0 
                        )
                        if pending_data:
                            await self._process_messages(pending_data, records_to_save, ack_items)
                except Exception as e:
                    # Connection errors should propagate to trigger reconnect
                    if "Connection" in str(e) or "reset by peer" in str(e):
                        raise e
                    logger.debug(f"Pending ì²˜ë¦¬ ì‹¤íŒ¨ {stream_name}: {e}")

            # ê° ìŠ¤íŠ¸ë¦¼ë³„ë¡œ ê°œë³„ ì²˜ë¦¬
            for stream_name, group_name in self.realtime_streams.items():
                try:
                    block_time = 100 
                    
                    new_data = await self.redis_client.xreadgroup(
                        groupname=group_name,
                        consumername="processor_worker",
                        streams={stream_name: ">"},
                        count=self.batch_size,
                        block=block_time
                    )
                    
                    if new_data:
                        total_messages = sum(len(msgs) for _, msgs in new_data)
                        if total_messages > 0:
                            # logger.info(f"ğŸ“¨ ìŠ¤íŠ¸ë¦¼ {stream_name}ì—ì„œ {total_messages}ê°œ ë©”ì‹œì§€ ì½ìŒ")
                            pass
                        await self._process_messages(new_data, records_to_save, ack_items)
                        
                except Exception as stream_error:
                    if "Connection" in str(stream_error) or "reset by peer" in str(stream_error):
                        raise stream_error
                    logger.debug(f"ìŠ¤íŠ¸ë¦¼ {stream_name} ì½ê¸° ì‹¤íŒ¨: {stream_error}")
            
            # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¶”ê°€ ëŒ€ê¸°ë¡œ CPU ë¶€í•˜ ì™„í™”
            if not records_to_save:
                await asyncio.sleep(0.1)
            else:
                logger.debug(f"ğŸ“¥ ì²˜ë¦¬í•  ë ˆì½”ë“œ: {len(records_to_save)}ê°œ")

            # DB ì €ì¥
            if records_to_save:
                tickers_in_batch = [r.get('ticker') for r in records_to_save]
                logger.info(f"ğŸ’¾ DB ì €ì¥ ì‹œë„: {len(records_to_save)}ê°œ ë ˆì½”ë“œ (Tickers: {tickers_in_batch[:5]}...)")
                success = await self.repository.bulk_save_realtime_quotes(records_to_save)
                if success:
                    processed_count = len(records_to_save)
                    # logger.info(f"âœ… DB ì €ì¥ ì„±ê³µ: {processed_count}ê°œ")
                else:
                    logger.error("âŒ DB ì €ì¥ ì‹¤íŒ¨")

            # ACK ì²˜ë¦¬
            if ack_items:
                for stream_name, group_name, message_id in ack_items:
                    try:
                        await self.redis_client.xack(stream_name, group_name, message_id)
                    except Exception as e:
                        logger.warning(f"ACK ì‹¤íŒ¨ {stream_name}:{message_id}: {e}")

            # ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰ë˜ë©´ ì—ëŸ¬ ì¹´ìš´íŠ¸ ë¦¬ì…‹
            self.consecutive_errors = 0

        except Exception as e:
            self.consecutive_errors += 1
            logger.error(f"ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ (ì‹œë„ {self.consecutive_errors}): {e}")
            
            # ì¬ì—°ê²° ë¡œì§
            if self.consecutive_errors >= 3 or "Connection" in str(e) or "reset by peer" in str(e):
                logger.warning("âš ï¸ ì—°ì† ì—ëŸ¬ ë˜ëŠ” ì—°ê²° ì—ëŸ¬ ê°ì§€, ì¬ì—°ê²° ì‹œë„...")
                await self._reconnect()
                
            await asyncio.sleep(1)

        return processed_count

    async def _process_messages(self, stream_data, records_to_save, ack_items):
        """ë©”ì‹œì§€ ì²˜ë¦¬ ë° íŒŒì‹±"""
        for stream_name_bytes, messages in stream_data:
            stream_name = stream_name_bytes.decode('utf-8') if isinstance(stream_name_bytes, bytes) else stream_name_bytes
            group_name = self.realtime_streams.get(stream_name)
            provider = stream_name.split(':')[0]

            adapter = self.adapter_factory.get_adapter(provider)

            for message_id, message_data in messages:
                try:
                    # Adapter expects raw bytes (it handles decoding internally)
                    parsed_data = adapter.parse_message(message_data)
                    if parsed_data:
                        ticker = parsed_data.get('ticker')
                        if not ticker:
                            # ë©”ì‹œì§€ IDëŠ” ACKí•˜ì—¬ ìŠ¤í‚µ
                            ack_items.append((stream_name, group_name, message_id))
                            continue
                        
                        # Ticker resolution logic
                        asset_id = None
                        if hasattr(self, 'ticker_to_asset_id'):
                            # 1. Exact match
                            if ticker in self.ticker_to_asset_id:
                                asset_id = self.ticker_to_asset_id[ticker]
                            # 2. Try removing 'USDT' (e.g. BTCUSDT -> BTC)
                            elif ticker.endswith('USDT') and len(ticker) > 4:
                                normalized = ticker[:-4]
                                if normalized in self.ticker_to_asset_id:
                                    asset_id = self.ticker_to_asset_id[normalized]
                             # 3. Try removing '-USD' (e.g. SOL-USD -> SOL)
                            elif ticker.endswith('-USD') and len(ticker) > 4:
                                normalized = ticker[:-4]
                                if normalized in self.ticker_to_asset_id:
                                    asset_id = self.ticker_to_asset_id[normalized]
                            # 4. Try removing '-USDT' (e.g. BTC-USDT -> BTC)
                            elif ticker.endswith('-USDT') and len(ticker) > 5:
                                normalized = ticker[:-5]
                                if normalized in self.ticker_to_asset_id:
                                    asset_id = self.ticker_to_asset_id[normalized]
                            # 5. Handle slash (e.g. XAU/USD -> XAU)
                            elif '/USD' in ticker:
                                normalized = ticker.split('/')[0]
                                if normalized in self.ticker_to_asset_id:
                                    asset_id = self.ticker_to_asset_id[normalized]
                        
                        if asset_id is not None:
                            parsed_data['asset_id'] = asset_id
                            records_to_save.append(parsed_data)
                            ack_items.append((stream_name, group_name, message_id))
                        else:
                            # ë§¤í•‘ ì‹¤íŒ¨ - ê°€ë” ë¡œê·¸ ì¶œë ¥
                            if not hasattr(self, '_missing_ticker_counts'):
                                self._missing_ticker_counts = {}
                            count = self._missing_ticker_counts.get(ticker, 0) + 1
                            self._missing_ticker_counts[ticker] = count
                            
                            if count == 1 or count % 100 == 0:
                                logger.warning(f"âš ï¸ Asset ID ëª»ì°¾ìŒ: {ticker} (ë°œìƒ: {count}íšŒ)")
                            
                            # ì‹¤íŒ¨í•´ë„ ACKí•˜ì—¬ ì¬ì²˜ë¦¬ ë°©ì§€
                            ack_items.append((stream_name, group_name, message_id))

                except Exception as e:
                    logger.error(f"ë©”ì‹œì§€ íŒŒì‹± ì‹¤íŒ¨ ({stream_name}:{message_id}): {e}")
                    # íŒŒì‹± ì‹¤íŒ¨ ì‹œì—ë„ ACK (DLQê°€ ì—†ìœ¼ë¯€ë¡œ)
                    ack_items.append((stream_name, group_name, message_id))

    def set_asset_map(self, asset_map: Dict[str, int]):
        self.ticker_to_asset_id = asset_map
