import logging
import asyncio
import redis.asyncio as redis
from typing import Dict, List, Any, Optional

logger = logging.getLogger(__name__)

class StreamConsumer:
    """Redis Stream ì†Œë¹„ ë° ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, redis_url: str, adapter_factory, repository, batch_size: int = 100):
        self.redis_url = redis_url
        self.redis_client = None
        self.adapter_factory = adapter_factory
        self.repository = repository
        self.batch_size = batch_size
        self.realtime_streams = {
            # High priority: Active WebSocket streams with frequent updates
            "binance:realtime": "binance_group",
            "coinbase:realtime": "coinbase_group",
            "swissquote:realtime": "swissquote_group",
            # Lower priority: May have timeouts or less frequent updates
            "alpaca:realtime": "alpaca_group",
            "finnhub:realtime": "finnhub_group",
        }

    async def connect(self) -> bool:
        """Redis ì—°ê²° ì´ˆê¸°í™”"""
        try:
            if self.redis_client:
                await self.redis_client.ping()
                return True
                
            self.redis_client = await redis.from_url(self.redis_url)
            await self.redis_client.ping()
            logger.info(f"Redis ì—°ê²° ì„±ê³µ: {self.redis_url}")
            return True
        except Exception as e:
            logger.error(f"Redis ì—°ê²° ì‹¤íŒ¨: {e}")
            return False

    async def process_streams(self) -> int:
        """ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ë°ì´í„° ì²˜ë¦¬"""
        if not self.redis_client:
            return 0

        processed_count = 0
        records_to_save = []
        ack_items = []

        try:
            # Consumer Group ìƒì„±
            for stream_name, group_name in self.realtime_streams.items():
                try:
                    await self.redis_client.xgroup_create(
                        name=stream_name, groupname=group_name, id="0", mkstream=True
                    )
                except Exception as e:
                    if "BUSYGROUP" not in str(e):
                        logger.warning(f"xgroup_create error {stream_name}: {e}")

            # Pending ë©”ì‹œì§€ ë¨¼ì € ì²˜ë¦¬
            for stream_name, group_name in self.realtime_streams.items():
                try:
                    pending_info = await self.redis_client.xpending(stream_name, group_name)
                    if pending_info and pending_info.get('pending', 0) > 0:
                        pending_data = await self.redis_client.xreadgroup(
                            groupname=group_name,
                            consumername="processor_worker",
                            streams={stream_name: "0"},
                            count=min(self.batch_size, pending_info['pending']),
                            block=0  # Non-blocking to prevent infinite wait
                        )
                        if pending_data:
                            await self._process_messages(pending_data, records_to_save, ack_items)
                except Exception as e:
                    logger.error(f"Pending ì²˜ë¦¬ ì‹¤íŒ¨ {stream_name}: {e}")

            # ê° ìŠ¤íŠ¸ë¦¼ë³„ë¡œ ê°œë³„ ì²˜ë¦¬ (ê° ìŠ¤íŠ¸ë¦¼ë§ˆë‹¤ ë‹¤ë¥¸ consumer group ì‚¬ìš©)
            # ê³ ìš°ì„ ìˆœìœ„ ìŠ¤íŠ¸ë¦¼(binance, coinbase)ì€ ì§§ì€ blockingìœ¼ë¡œ ì²˜ë¦¬
            high_priority_streams = {"binance:realtime", "coinbase:realtime"}
            for stream_name, group_name in self.realtime_streams.items():
                try:
                    # ê³ ìš°ì„ ìˆœìœ„ ìŠ¤íŠ¸ë¦¼ì€ 50ms blocking, ë‚˜ë¨¸ì§€ëŠ” non-blocking
                    block_time = 50 if stream_name in high_priority_streams else 0
                    
                    # Timeout ë³´í˜¸ (ìµœëŒ€ 500ms)
                    new_data = await asyncio.wait_for(
                        self.redis_client.xreadgroup(
                            groupname=group_name,
                            consumername="processor_worker",
                            streams={stream_name: ">"},
                            count=self.batch_size,
                            block=block_time
                        ),
                        timeout=0.5
                    )
                    if new_data:
                        total_messages = sum(len(msgs) for _, msgs in new_data)
                        if total_messages > 0:
                            logger.info(f"ğŸ“¨ ìŠ¤íŠ¸ë¦¼ {stream_name}ì—ì„œ {total_messages}ê°œ ë©”ì‹œì§€ ì½ìŒ")
                        await self._process_messages(new_data, records_to_save, ack_items)
                except asyncio.TimeoutError:
                    logger.warning(f"ìŠ¤íŠ¸ë¦¼ {stream_name} ì½ê¸° timeout")
                except Exception as stream_error:
                    # ì—ëŸ¬ ë°œìƒí•´ë„ ë‹¤ìŒ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ê³„ì†
                    logger.warning(f"ìŠ¤íŠ¸ë¦¼ {stream_name} ì½ê¸° ì‹¤íŒ¨: {stream_error}")
            
            # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¶”ê°€ ëŒ€ê¸°ë¡œ CPU ë¶€í•˜ ì™„í™”
            if not records_to_save:
                await asyncio.sleep(0.2)
            else:
                logger.debug(f"ğŸ“¥ ì²˜ë¦¬í•  ë ˆì½”ë“œ: {len(records_to_save)}ê°œ")

            # DB ì €ì¥
            if records_to_save:
                logger.info(f"ğŸ’¾ DB ì €ì¥ ì‹œë„: {len(records_to_save)}ê°œ ë ˆì½”ë“œ")
                success = await self.repository.bulk_save_realtime_quotes(records_to_save)
                if success:
                    processed_count = len(records_to_save)
                    logger.info(f"âœ… DB ì €ì¥ ì„±ê³µ: {processed_count}ê°œ")
                else:
                    logger.error("âŒ DB ì €ì¥ ì‹¤íŒ¨")

            # ACK ì²˜ë¦¬
            if ack_items:
                for stream_name, group_name, message_id in ack_items:
                    try:
                        await self.redis_client.xack(stream_name, group_name, message_id)
                    except Exception as e:
                        logger.warning(f"ACK ì‹¤íŒ¨ {stream_name}:{message_id}: {e}")

        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

        return processed_count

    async def _process_messages(self, stream_data, records_to_save, ack_items):
        """ë©”ì‹œì§€ ì²˜ë¦¬ ë° íŒŒì‹±"""
        for stream_name_bytes, messages in stream_data:
            stream_name = stream_name_bytes.decode('utf-8') if isinstance(stream_name_bytes, bytes) else stream_name_bytes
            group_name = self.realtime_streams.get(stream_name)
            provider = stream_name.split(':')[0]

            adapter = self.adapter_factory.get_adapter(provider)

            for message_id, message_data in messages:
                try:
                    parsed_data = adapter.parse_message(message_data)
                    if parsed_data:
                        # asset_id ì¡°íšŒ ë¡œì§ì´ í•„ìš”í•¨. 
                        # í˜„ì¬ AdapterëŠ” tickerë§Œ ë°˜í™˜í•˜ë¯€ë¡œ, ì—¬ê¸°ì„œ asset_idë¥¼ ë§¤í•‘í•´ì•¼ í•¨.
                        # ì„±ëŠ¥ì„ ìœ„í•´ ìºì‹±ëœ ë§¤í•‘ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŒ.
                        # í•˜ì§€ë§Œ ë¦¬íŒ©í† ë§ ë‹¨ê³„ì—ì„œëŠ” ì¼ë‹¨ DBì—ì„œ ì¡°íšŒí•˜ê±°ë‚˜ ìºì‹œë¥¼ ì£¼ì…ë°›ì•„ì•¼ í•¨.
                        # ì—¬ê¸°ì„œëŠ” ì„ì‹œë¡œ asset_idë¥¼ Noneìœ¼ë¡œ ë‘ê³  Repositoryë‚˜ Validatorì—ì„œ ì²˜ë¦¬í•˜ê²Œ í•˜ê±°ë‚˜,
                        # DataProcessorê°€ ê°€ì§€ê³  ìˆë˜ ticker_to_asset_id ë§µì„ ì£¼ì…ë°›ì•„ì•¼ í•¨.
                        
                        # TODO: Resolve asset_id efficiently. 
                        # For now, let's assume we can get it from a cache passed in or similar.
                        # Or better, let the repository handle it if it caches?
                        # No, repository saves. Validator validates.
                        # Let's add a method to inject asset_map.
                        
                        ticker = parsed_data.get('ticker')
                        if not ticker:
                            logger.warning(f"âš ï¸ Tickerê°€ ì—†ëŠ” ë©”ì‹œì§€: {stream_name}:{message_id}")
                            ack_items.append((stream_name, group_name, message_id))
                            continue
                        
                        # Ticker normalization: try original first, then try without USDT suffix
                        asset_id = None
                        if hasattr(self, 'ticker_to_asset_id'):
                            # Try original ticker first (e.g., BTCUSDT, ETH, SOL-USD)
                            if ticker in self.ticker_to_asset_id:
                                asset_id = self.ticker_to_asset_id[ticker]
                            # If not found and has USDT suffix, try without it (e.g., SOLUSDT -> SOL)
                            elif ticker.endswith('USDT') and len(ticker) > 4:
                                normalized_ticker = ticker[:-4]  # Remove 'USDT'
                                if normalized_ticker in self.ticker_to_asset_id:
                                    asset_id = self.ticker_to_asset_id[normalized_ticker]
                            # Coinbase format: SOL-USD -> SOL
                            elif ticker.endswith('-USD') and len(ticker) > 4:
                                normalized_ticker = ticker[:-4]  # Remove '-USD'
                                if normalized_ticker in self.ticker_to_asset_id:
                                    asset_id = self.ticker_to_asset_id[normalized_ticker]
                            # Some exchanges use XXX-USDT format (e.g., BTC-USDT -> BTC)
                            elif ticker.endswith('-USDT') and len(ticker) > 5:
                                normalized_ticker = ticker[:-5]  # Remove '-USDT'
                                if normalized_ticker in self.ticker_to_asset_id:
                                    asset_id = self.ticker_to_asset_id[normalized_ticker]
                            # Swissquote format: XAU/USD -> XAU
                            elif '/USD' in ticker:
                                normalized_ticker = ticker.split('/')[0]
                                if normalized_ticker in self.ticker_to_asset_id:
                                    asset_id = self.ticker_to_asset_id[normalized_ticker]
                        
                        if asset_id is not None:
                            parsed_data['asset_id'] = asset_id
                            records_to_save.append(parsed_data)
                            ack_items.append((stream_name, group_name, message_id))
                        else:
                            # ë§¤í•‘ ì‹¤íŒ¨ ì‹œ ë¡œê·¸
                            if not hasattr(self, 'ticker_to_asset_id'):
                                logger.warning(f"âš ï¸ Asset ë§µì´ ì„¤ì •ë˜ì§€ ì•ŠìŒ. Ticker: {ticker}")
                            else:
                                # Only log periodically to reduce log spam (every 100th occurrence)
                                if not hasattr(self, '_missing_ticker_counts'):
                                    self._missing_ticker_counts = {}
                                count = self._missing_ticker_counts.get(ticker, 0) + 1
                                self._missing_ticker_counts[ticker] = count
                                if count == 1 or count % 100 == 0:
                                    logger.warning(f"âš ï¸ Asset IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ. Ticker: {ticker} (ë§µ í¬ê¸°: {len(self.ticker_to_asset_id)}, ë°œìƒ: {count}íšŒ)")
                            # ACKëŠ” í•´ì•¼ í•˜ë‚˜? ì‹¤íŒ¨í•œ ë©”ì‹œì§€ëŠ” ACKí•´ì„œ ë„˜ì–´ê°€ì•¼ í•¨.
                            ack_items.append((stream_name, group_name, message_id))

                except Exception as e:
                    logger.error(f"ë©”ì‹œì§€ íŒŒì‹± ì‹¤íŒ¨ ({stream_name}:{message_id}): {e}")
                    # íŒŒì‹± ì‹¤íŒ¨í•œ ë©”ì‹œì§€ë„ ACK ì²˜ë¦¬í•˜ì—¬ ë¬´í•œ ë£¨í”„ ë°©ì§€ (ë˜ëŠ” DLQë¡œ ì´ë™)
                    ack_items.append((stream_name, group_name, message_id))

    def set_asset_map(self, asset_map: Dict[str, int]):
        self.ticker_to_asset_id = asset_map
