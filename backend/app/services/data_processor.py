"""
Data Processor Service - ì¤‘ì•™í™”ëœ ë°ì´í„° ì²˜ë¦¬ ì„œë¹„ìŠ¤
Redis Streamê³¼ Queueì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ ê²€ì¦í•˜ê³  MySQL DBì— ì €ì¥
"""
import asyncio
import json
import logging
import time
import datetime
from datetime import datetime, timezone
from typing import Dict, Any, Optional, List
from contextlib import asynccontextmanager

import redis.asyncio as redis
from sqlalchemy.orm import Session

from ..core.database import SessionLocal
from ..core.config import GLOBAL_APP_CONFIGS, config_manager
from ..models.asset import RealtimeQuote, RealtimeQuoteTimeDelay
from ..models.asset import Asset, OHLCVData
from ..crud.asset import crud_ohlcv, crud_asset
from ..utils.logger import logger
from ..utils.redis_queue_manager import RedisQueueManager
from ..utils.helpers import safe_float

logger.info("DataProcessor ëª¨ë“ˆ import ì™„ë£Œ")

class DataProcessor:
    """
    ì¤‘ì•™í™”ëœ ë°ì´í„° ì²˜ë¦¬ ì„œë¹„ìŠ¤
    - Redis Streamì—ì„œ ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬
    - Redis Queueì—ì„œ ë°°ì¹˜ ë°ì´í„° ì²˜ë¦¬
    - ë°ì´í„° ê²€ì¦ ë° ë³€í™˜
    - MySQL DB ì €ì¥
    """
    
    def __init__(self, config_manager=None, redis_queue_manager=None):
        logger.info("DataProcessor ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì¤‘...")
        self.redis_client: Optional[redis.Redis] = None
        self.running = False
        self.config_manager = config_manager
        self.redis_queue_manager = redis_queue_manager
        
        # Redis ì„¤ì •
        self.redis_host = GLOBAL_APP_CONFIGS.get("REDIS_HOST", "redis")
        self.redis_port = GLOBAL_APP_CONFIGS.get("REDIS_PORT", 6379)
        self.redis_db = GLOBAL_APP_CONFIGS.get("REDIS_DB", 0)
        self.redis_password = GLOBAL_APP_CONFIGS.get("REDIS_PASSWORD")
        
        # ì²˜ë¦¬ ì„¤ì • (DB ì„¤ì • ìš°ì„ , ê¸°ë³¸ê°’ fallback)
        self.batch_size = int(GLOBAL_APP_CONFIGS.get("REALTIME_BATCH_SIZE", 1000))
        self.processing_interval = float(GLOBAL_APP_CONFIGS.get("REALTIME_PROCESSING_INTERVAL_SECONDS", 1.0))
        self.time_window_minutes = int(GLOBAL_APP_CONFIGS.get("WEBSOCKET_TIME_WINDOW_MINUTES", 15))
        self.stream_block_ms = int(GLOBAL_APP_CONFIGS.get("REALTIME_STREAM_BLOCK_MS", 100))
        # ìš°ì„  ìˆœìœ„: DB(ConfigManager) > GLOBAL_APP_CONFIGS
        self.max_retries = (config_manager.get_retry_attempts() if config_manager else GLOBAL_APP_CONFIGS.get("MAX_API_RETRY_ATTEMPTS", 3))
        try:
            self.max_retries = int(self.max_retries)
        except Exception:
            self.max_retries = 3
        self.retry_delay = 5  # ì´ˆ
        
        # ìŠ¤íŠ¸ë¦¼ ë° í ì„¤ì • (ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ í™œì„±í™”)
        self.realtime_streams = {
            "finnhub:realtime": "finnhub_processor_group",
            "alpaca:realtime": "alpaca_processor_group",
            "binance:realtime": "binance_processor_group",
            "coinbase:realtime": "coinbase_processor_group",
            "twelvedata:realtime": "twelvedata_processor_group",
            "swissquote:realtime": "swissquote_processor_group",
            # "tiingo:realtime": "tiingo_processor_group",  # ëŒ€ì—­í­ í•œë„ë¡œ ë¹„í™œì„±í™”
        }
        
        # ì•”í˜¸í™”í ì†ŒìŠ¤ ìš°ì„ ìˆœìœ„ ì •ì˜ (1=ìµœê³  ìš°ì„ ìˆœìœ„)
        self.crypto_source_priority = {
            'binance': 1,    # ë°”ì´ë‚¸ìŠ¤: ê±°ë˜ëŸ‰ 1ìœ„, ë°ì´í„° ê°±ì‹  ë¹ ë¦„
            'coinbase': 2,   # ì½”ì¸ë² ì´ìŠ¤: ê±°ë˜ëŸ‰ 2ìœ„, ì•ˆì •ì 
        }
        
        # ì†ŒìŠ¤ë³„ ë§ˆì§€ë§‰ ë°ì´í„° ìˆ˜ì‹  ì‹œê°„ ì¶”ì 
        self.source_last_seen = {}
        self.source_health_timeout = 30  # 30ì´ˆ ì´ìƒ ë°ì´í„° ì—†ìœ¼ë©´ ë¹„í™œì„±ìœ¼ë¡œ ê°„ì£¼ (5ì´ˆ â†’ 30ì´ˆë¡œ ì¦ê°€)
        self.batch_queue = "batch_data_queue"
        logger.info("DataProcessor ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ")

        # Redis Queue Manager (for batch queue + DLQ)
        self.queue_manager = RedisQueueManager(config_manager=config_manager) if config_manager else None
        
        # ì²˜ë¦¬ í†µê³„
        self.stats = {
            "realtime_processed": 0,
            "batch_processed": 0,
            "errors": 0,
            "last_processed": None
        }
    
    def _update_source_health(self, source_name: str):
        """ì†ŒìŠ¤ë³„ ë§ˆì§€ë§‰ ë°ì´í„° ìˆ˜ì‹  ì‹œê°„ ì—…ë°ì´íŠ¸"""
        self.source_last_seen[source_name] = time.time()
        logger.info(f"ğŸ“Š {source_name} ì†ŒìŠ¤ í—¬ìŠ¤ ì—…ë°ì´íŠ¸: {datetime.now()}")
    
    def _get_active_crypto_source(self) -> str:
        """í˜„ì¬ í™œì„±í™”ëœ ì•”í˜¸í™”í ì†ŒìŠ¤ ê²°ì • (í˜ì¼ì˜¤ë²„ ë¡œì§)"""
        current_time = time.time()
        
        logger.debug(f"ğŸ” í˜ì¼ì˜¤ë²„ ë¡œì§ ì‹œì‘ - í˜„ì¬ ì‹œê°„: {current_time}")
        logger.debug(f"ğŸ“Š ì†ŒìŠ¤ í—¬ìŠ¤ ìƒíƒœ: {self.source_last_seen}")
        
        # ìš°ì„ ìˆœìœ„ ìˆœìœ¼ë¡œ ì†ŒìŠ¤ ìƒíƒœ í™•ì¸
        for source_name, priority in sorted(self.crypto_source_priority.items(), key=lambda x: x[1]):
            last_seen = self.source_last_seen.get(source_name, 0)
            time_since_last_seen = current_time - last_seen
            
            logger.debug(f"ğŸ” {source_name} ì²´í¬ - ë§ˆì§€ë§‰ ìˆ˜ì‹ : {last_seen}, ê²½ê³¼ ì‹œê°„: {time_since_last_seen:.1f}ì´ˆ")
            
            # 30ì´ˆ ì´ë‚´ì— ë°ì´í„°ë¥¼ ë°›ì•˜ë‹¤ë©´ í™œì„± ìƒíƒœ
            if time_since_last_seen <= self.source_health_timeout:
                logger.info(f"âœ… {source_name} í™œì„± ì†ŒìŠ¤ë¡œ ì„ íƒ (ìš°ì„ ìˆœìœ„: {priority}, ë§ˆì§€ë§‰ ìˆ˜ì‹ : {time_since_last_seen:.1f}ì´ˆ ì „)")
                return source_name
            else:
                logger.debug(f"âš ï¸ {source_name} ë¹„í™œì„± ìƒíƒœ (ë§ˆì§€ë§‰ ìˆ˜ì‹ : {time_since_last_seen:.1f}ì´ˆ ì „, ì„ê³„ê°’: {self.source_health_timeout}ì´ˆ)")
        
        # ëª¨ë“  ì†ŒìŠ¤ê°€ ë¹„í™œì„±ì¸ ê²½ìš°, ìš°ì„ ìˆœìœ„ê°€ ê°€ì¥ ë†’ì€ ì†ŒìŠ¤ ë°˜í™˜
        fallback_source = min(self.crypto_source_priority.items(), key=lambda x: x[1])[0]
        logger.warning(f"ğŸš¨ ëª¨ë“  ì•”í˜¸í™”í ì†ŒìŠ¤ ë¹„í™œì„±, {fallback_source}ë¡œ í˜ì¼ì˜¤ë²„")
        return fallback_source

    async def _connect_redis(self) -> bool:
        """Redis ì—°ê²° ì´ˆê¸°í™”"""
        try:
            if self.redis_client:
                await self.redis_client.ping()
                return True
                
            redis_url = f"redis://{self.redis_host}:{self.redis_port}/{self.redis_db}"
            if self.redis_password:
                redis_url = f"redis://:{self.redis_password}@{self.redis_host}:{self.redis_port}/{self.redis_db}"
            
            self.redis_client = await redis.from_url(redis_url)
            await self.redis_client.ping()
            logger.info(f"Redis ì—°ê²° ì„±ê³µ: {self.redis_host}:{self.redis_port}")
            return True
            
        except Exception as e:
            logger.error(f"Redis ì—°ê²° ì‹¤íŒ¨: {e}")
            return False

    def _get_time_window(self, timestamp: datetime, interval_minutes: int = None) -> datetime:
        """ì§€ì •ëœ ë¶„ ë‹¨ìœ„ë¡œ ì‹œê°„ ìœˆë„ìš° ê³„ì‚° (ì„¤ì •ê°’ ë˜ëŠ” ê¸°ë³¸ 15ë¶„ ë‹¨ìœ„ë¡œ ë°˜ì˜¬ë¦¼)"""
        try:
            if interval_minutes is None:
                interval_minutes = self.time_window_minutes
            
            # ë¶„ ë‹¨ìœ„ë¡œ ë°˜ì˜¬ë¦¼ (12:07 -> 12:00, 12:22 -> 12:15)
            minute = (timestamp.minute // interval_minutes) * interval_minutes
            return timestamp.replace(minute=minute, second=0, microsecond=0)
        except Exception as e:
            logger.warning(f"ì‹œê°„ ìœˆë„ìš° ê³„ì‚° ì‹¤íŒ¨: {e}")
            return timestamp

    def _parse_timestamp(self, timestamp_str: str, provider: str = None) -> datetime:
        """íƒ€ì„ìŠ¤íƒ¬í”„ ë¬¸ìì—´ì„ íŒŒì‹±í•˜ê³  UTCë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        try:
            # ë¨¼ì € í‘œì¤€ ISO í˜•ì‹ìœ¼ë¡œ ì‹œë„
            parsed_time = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
            # UTCë¡œ ë³€í™˜
            if parsed_time.tzinfo is not None:
                return parsed_time.astimezone(timezone.utc).replace(tzinfo=None)
            return parsed_time
        except ValueError:
            try:
                # Unix timestamp (milliseconds) í˜•íƒœì¸ì§€ í™•ì¸
                if timestamp_str.isdigit() and len(timestamp_str) >= 10:
                    # ë°€ë¦¬ì´ˆ ë‹¨ìœ„ Unix timestampë¥¼ ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜
                    timestamp_ms = int(timestamp_str)
                    if len(timestamp_str) > 10:  # ë°€ë¦¬ì´ˆê°€ í¬í•¨ëœ ê²½ìš°
                        timestamp_seconds = timestamp_ms / 1000.0
                    else:  # ì´ˆ ë‹¨ìœ„ì¸ ê²½ìš°
                        timestamp_seconds = timestamp_ms
                    # Unix timestampëŠ” ì´ë¯¸ UTC ê¸°ì¤€ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    return datetime.fromtimestamp(timestamp_seconds)
                
                # ë§ˆì´í¬ë¡œì´ˆê°€ 6ìë¦¬ë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš° ì²˜ë¦¬
                if '.' in timestamp_str and len(timestamp_str.split('.')[1]) > 6:
                    # ë§ˆì´í¬ë¡œì´ˆë¥¼ 6ìë¦¬ë¡œ ìë¥´ê¸°
                    parts = timestamp_str.split('.')
                    if len(parts) == 2:
                        base_time = parts[0]
                        microseconds = parts[1][:6]  # 6ìë¦¬ë¡œ ìë¥´ê¸°
                        timezone_part = ''
                        if '-' in microseconds or '+' in microseconds:
                            # íƒ€ì„ì¡´ ì •ë³´ê°€ ë§ˆì´í¬ë¡œì´ˆì— í¬í•¨ëœ ê²½ìš°
                            for i, char in enumerate(microseconds):
                                if char in ['-', '+']:
                                    microseconds = microseconds[:i]
                                    timezone_part = parts[1][i:]
                                    break
                        timestamp_str = f"{base_time}.{microseconds}{timezone_part}"
                        parsed_time = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
                        # UTCë¡œ ë³€í™˜
                        if parsed_time.tzinfo is not None:
                            return parsed_time.astimezone(timezone.utc).replace(tzinfo=None)
                        return parsed_time
            except (ValueError, OSError):
                pass
            
            # ëª¨ë“  íŒŒì‹±ì´ ì‹¤íŒ¨í•˜ë©´ í˜„ì¬ UTC ì‹œê°„ ë°˜í™˜
            logger.warning(f"íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì‹± ì‹¤íŒ¨: {timestamp_str}, í˜„ì¬ UTC ì‹œê°„ ì‚¬ìš©")
            return datetime.utcnow()

    def _determine_actual_interval(self, current_ts: datetime, items: List[Dict], current_index: int) -> Optional[str]:
        """timestamp_utcë¥¼ ë¶„ì„í•´ì„œ ì‹¤ì œ ì£¼ê¸°ë¥¼ íŒë‹¨í•©ë‹ˆë‹¤."""
        try:
            if not isinstance(current_ts, datetime):
                return None
            
            # ì¼ë´‰ ë°ì´í„°ì˜ ê²½ìš° ê¸°ë³¸ì ìœ¼ë¡œ 1d ë°˜í™˜
            # ì£¼ë´‰/ì›”ë´‰ì€ ë³„ë„ì˜ ë¡œì§ìœ¼ë¡œ íŒë‹¨í•´ì•¼ í•¨
            
            # í˜„ì¬ ë°ì´í„°ê°€ ì›”ë§ì¸ì§€ í™•ì¸ (ì›”ì˜ ë§ˆì§€ë§‰ ë‚ )
            from calendar import monthrange
            year, month = current_ts.year, current_ts.month
            last_day_of_month = monthrange(year, month)[1]
            is_month_end = current_ts.day == last_day_of_month
            
            # í˜„ì¬ ë°ì´í„°ê°€ ì£¼ë§ì¸ì§€ í™•ì¸ (ê¸ˆìš”ì¼)
            is_friday = current_ts.weekday() == 4  # 4 = ê¸ˆìš”ì¼
            
            # ì£¼ë´‰/ì›”ë´‰ íŒë‹¨ì€ ë” ì •êµí•œ ë¡œì§ì´ í•„ìš”
            # í˜„ì¬ëŠ” ì¼ë´‰ ë°ì´í„°ë§Œ ì²˜ë¦¬í•˜ë¯€ë¡œ ê¸°ë³¸ì ìœ¼ë¡œ 1d ë°˜í™˜
            # TODO: í–¥í›„ ì£¼ë´‰/ì›”ë´‰ ë°ì´í„° ìˆ˜ì§‘ ì‹œ ë³„ë„ ë¡œì§ êµ¬í˜„
            
            # ì›”ë§ì´ë©´ì„œ ê¸ˆìš”ì¼ì¸ ê²½ìš°ì—ë§Œ ì›”ë´‰ìœ¼ë¡œ íŒë‹¨
            if is_month_end and is_friday:
                return "1m"  # ì›”ë§ ê¸ˆìš”ì¼ì´ë©´ ì›”ë´‰
            # ê¸ˆìš”ì¼ì´ì§€ë§Œ ì›”ë§ì´ ì•„ë‹Œ ê²½ìš°ëŠ” ì¼ë´‰ìœ¼ë¡œ ì²˜ë¦¬
            elif is_friday:
                return "1d"  # ê¸ˆìš”ì¼ì´ì§€ë§Œ ì›”ë§ì´ ì•„ë‹ˆë©´ ì¼ë´‰
            else:
                return "1d"  # ê¸°ë³¸ ì¼ë´‰
                
        except Exception as e:
            logger.warning(f"ì£¼ê¸° íŒë‹¨ ì‹¤íŒ¨: {e}")
            return "1d"  # ì‹¤íŒ¨ ì‹œì—ë„ ê¸°ë³¸ê°’ìœ¼ë¡œ 1d ë°˜í™˜

    @asynccontextmanager
    async def get_db_session(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
        db = SessionLocal()
        try:
            yield db
        except Exception as e:
            db.rollback()
            raise
        finally:
            db.close()

    async def _process_realtime_streams(self) -> int:
        """ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ë°ì´í„° ì²˜ë¦¬ - Consumer Group ì‚¬ìš©"""
        logger.debug("ğŸš€ _process_realtime_streams ì‹œì‘")
        
        if not self.redis_client:
            logger.info("Redis client not available for realtime streams")
            return 0
            
        processed_count = 0
        logger.info(f"Processing realtime streams: {list(self.realtime_streams.keys())}")
        
        try:
            logger.debug("âœ… try ë¸”ë¡ ì§„ì…")
            logger.debug("ğŸ”§ Consumer Group ìƒì„± ì‹œì‘")
            # Consumer Group ìƒì„± ë° TTL ì„¤ì • (ê° ìŠ¤íŠ¸ë¦¼ë³„)
            for stream_name in self.realtime_streams.keys():
                try:
                    group_name = self.realtime_streams[stream_name]
                    logger.debug(f"ğŸ”§ Consumer Group ìƒì„± ì‹œë„: {stream_name} -> {group_name}")
                    await self.redis_client.xgroup_create(
                        name=stream_name, 
                        groupname=group_name, 
                        id="0", 
                        mkstream=True
                    )
                    logger.info(f"âœ… Created consumer group {group_name} on {stream_name}")
                    
                    # Stream TTL ì„¤ì • (ìµœëŒ€ 1000ê°œ ë©”ì‹œì§€ ìœ ì§€)
                    await self.redis_client.xtrim(stream_name, maxlen=1000, approximate=True)
                    logger.debug(f"ğŸ§¹ Stream TTL ì„¤ì •: {stream_name} (ìµœëŒ€ 1000ê°œ ë©”ì‹œì§€)")
                    
                except Exception as e:
                    if "BUSYGROUP" not in str(e):
                        logger.warning(f"âš ï¸ xgroup_create skip {stream_name}: {e}")
                    else:
                        logger.debug(f"â„¹ï¸ Consumer group {group_name} already exists on {stream_name}")
                        # ê¸°ì¡´ ìŠ¤íŠ¸ë¦¼ì—ë„ TTL ì ìš©
                        try:
                            await self.redis_client.xtrim(stream_name, maxlen=1000, approximate=True)
                            logger.debug(f"ğŸ§¹ ê¸°ì¡´ Stream TTL ì„¤ì •: {stream_name}")
                        except Exception as trim_e:
                            logger.warning(f"âš ï¸ Stream TTL ì„¤ì • ì‹¤íŒ¨ {stream_name}: {trim_e}")
            
            logger.debug("ğŸ“– Consumer Groupìœ¼ë¡œ ë°ì´í„° ì½ê¸° ì‹œì‘")
            # Consumer Groupìœ¼ë¡œ ë°ì´í„° ì½ê¸° (ê° ìŠ¤íŠ¸ë¦¼ë³„ë¡œ ê°œë³„ ì²˜ë¦¬)
            all_stream_data = []
            for stream_name in self.realtime_streams.keys():
                group_name = self.realtime_streams[stream_name]
                try:
                    logger.debug(f"ğŸ“– ìŠ¤íŠ¸ë¦¼ {stream_name} ì½ê¸° ì‹œë„ (group: {group_name})")
                    
                    # ìŠ¤íŠ¸ë¦¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                    stream_exists = await self.redis_client.exists(stream_name)
                    if not stream_exists:
                        logger.debug(f"ğŸ“­ ìŠ¤íŠ¸ë¦¼ {stream_name}ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ, ê±´ë„ˆëœ€")
                        continue
                    
                    stream_data = await self.redis_client.xreadgroup(
                        groupname=group_name,
                        consumername="data_processor_worker",
                        streams={stream_name: ">"},
                        count=self.batch_size,
                        block=self.stream_block_ms  # ì„¤ì • ê°€ëŠ¥í•œ ë¸”ë¡ ì‹œê°„
                    )
                    logger.info(f"ğŸ“– ìŠ¤íŠ¸ë¦¼ {stream_name} ì½ê¸° ê²°ê³¼: {len(stream_data) if stream_data else 0}ê°œ ë©”ì‹œì§€")
                    if stream_data:
                        all_stream_data.extend(stream_data)
                except Exception as e:
                    import traceback
                    logger.error(f"âŒ ìŠ¤íŠ¸ë¦¼ {stream_name} ì½ê¸° ì‹¤íŒ¨: {e}")
                    logger.error(f"ğŸ” ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
                    # ìŠ¤íŠ¸ë¦¼ë³„ ì˜¤ë¥˜ë¥¼ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  ê³„ì† ì§„í–‰
                    continue
            
            if not all_stream_data:
                return 0
                
            records_to_save = []
            ack_items = []
            
            # ìì‚° ì •ë³´ ìºì‹œ (ì„±ëŠ¥ ìµœì í™”)
            async with self.get_db_session() as db:
                assets = db.query(Asset.ticker, Asset.asset_id).all()
                ticker_to_asset_id = {ticker: asset_id for ticker, asset_id in assets}
            
            # í˜„ì¬ í™œì„±í™”ëœ ì•”í˜¸í™”í ì†ŒìŠ¤ ê²°ì •
            active_crypto_source = self._get_active_crypto_source()
            logger.info(f"ğŸ¯ í˜„ì¬ í™œì„± ì•”í˜¸í™”í ì†ŒìŠ¤: {active_crypto_source}")
            
            # ë©”ì‹œì§€ ì²˜ë¦¬
            for stream_name, messages in all_stream_data:
                # Redisì—ì„œ ë°˜í™˜ë˜ëŠ” ìŠ¤íŠ¸ë¦¼ ì´ë¦„ì´ bytes íƒ€ì…ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¬¸ìì—´ë¡œ ë³€í™˜
                stream_name_str = stream_name.decode('utf-8') if isinstance(stream_name, bytes) else stream_name
                group_name = self.realtime_streams[stream_name_str]
                source_name = stream_name_str.split(':')[0]  # 'binance:realtime' -> 'binance'
                
                logger.info(f"ğŸ“¥ ìŠ¤íŠ¸ë¦¼ {stream_name_str}ì—ì„œ {len(messages)}ê°œ ë©”ì‹œì§€ ì²˜ë¦¬ ì‹œì‘")
                
                # ì‹¤ì œ ë©”ì‹œì§€ê°€ ìˆì„ ë•Œë§Œ ë§ˆì§€ë§‰ ìˆ˜ì‹  ì‹œê°„ ì—…ë°ì´íŠ¸
                if messages:
                    self._update_source_health(source_name)
                    logger.info(f"ğŸ“Š {source_name} ì†ŒìŠ¤ í—¬ìŠ¤ ì—…ë°ì´íŠ¸: {self.source_last_seen.get(source_name)}")
                else:
                    logger.info(f"ğŸ“­ {source_name} ì†ŒìŠ¤ì— ë©”ì‹œì§€ ì—†ìŒ, í—¬ìŠ¤ ì—…ë°ì´íŠ¸ ì•ˆí•¨")
                
                # ì•”í˜¸í™”í ìŠ¤íŠ¸ë¦¼ì¸ ê²½ìš°, í™œì„± ì†ŒìŠ¤ì˜ ë°ì´í„°ë§Œ ì²˜ë¦¬
                if source_name in self.crypto_source_priority:
                    if source_name != active_crypto_source:
                        # í™œì„± ì†ŒìŠ¤ê°€ ì•„ë‹ˆë©´ ë©”ì‹œì§€ëŠ” ì†Œë¹„í•˜ë˜, ë°ì´í„° ì²˜ë¦¬ëŠ” ê±´ë„ˆëœ€ (ì¤‘ë³µ ë°©ì§€)
                        logger.info(f"â­ï¸ {source_name} ë¹„í™œì„± ì†ŒìŠ¤, ë©”ì‹œì§€ ì†Œë¹„ë§Œ ìˆ˜í–‰ (í™œì„±: {active_crypto_source})")
                        if messages:
                            # ë©”ì‹œì§€ ACKë§Œ ìˆ˜í–‰í•˜ê³  ë°ì´í„° ì²˜ë¦¬ëŠ” ê±´ë„ˆëœ€
                            for message_id, _ in messages:
                                ack_items.append((stream_name_str, group_name, message_id))
                        continue  # ë‹¤ìŒ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ë„˜ì–´ê°
                    else:
                        logger.info(f"âœ… {source_name} í™œì„± ì†ŒìŠ¤, ë°ì´í„° ì²˜ë¦¬ ì§„í–‰")
                
                for message_id, message_data in messages:
                    try:
                        logger.debug(f"ğŸ” ë©”ì‹œì§€ {message_id} ì²˜ë¦¬ ì‹œì‘")
                        logger.debug(f"ğŸ“‹ ì›ë³¸ ë©”ì‹œì§€ ë°ì´í„°: {message_data}")
                        
                        # Redis ìŠ¤íŠ¸ë¦¼ ë°ì´í„° íŒŒì‹± (í”„ë¡œë°”ì´ë”ë³„ í˜•ì‹ ì²˜ë¦¬)
                        symbol = None
                        price = None
                        volume = None
                        raw_timestamp = None
                        provider = None
                        
                        # í‘œì¤€ í•„ë“œ ìŠ¤í‚¤ë§ˆ (ëª¨ë“  í”„ë¡œë°”ì´ë” ë™ì¼)
                        symbol = message_data.get(b'symbol', b'').decode('utf-8').upper()
                        price = safe_float(message_data.get(b'price', b'').decode('utf-8'))
                        volume = safe_float(message_data.get(b'volume', b'').decode('utf-8'))
                        raw_timestamp = message_data.get(b'raw_timestamp', b'').decode('utf-8')
                        provider = message_data.get(b'provider', b'unknown').decode('utf-8')
                        
                        logger.debug(f"ğŸ“Š íŒŒì‹±ëœ ë°ì´í„° - symbol: {symbol}, price: {price}, volume: {volume}, provider: {provider}")
                        
                        # ì´ì „ í˜•ì‹ í˜¸í™˜ì„± (data í•„ë“œê°€ ìˆëŠ” ê²½ìš°)
                        if not symbol and b'data' in message_data:
                            logger.debug("ğŸ”„ ì´ì „ í˜•ì‹ ë°ì´í„° ê°ì§€, JSON íŒŒì‹± ì‹œë„")
                            try:
                                data_json = json.loads(message_data[b'data'].decode('utf-8'))
                                symbol = data_json.get('symbol', '').upper()
                                price = safe_float(data_json.get('price'))
                                volume = safe_float(data_json.get('volume'))
                                raw_timestamp = str(data_json.get('raw_timestamp', ''))
                                provider = message_data.get(b'provider', b'finnhub').decode('utf-8')
                                logger.debug(f"âœ… JSON íŒŒì‹± ì„±ê³µ - symbol: {symbol}, price: {price}")
                            except (json.JSONDecodeError, KeyError) as e:
                                logger.warning(f"âŒ Legacy data JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                                continue
                        
                        if not symbol or price is None:
                            logger.warning(f"âš ï¸ í•„ìˆ˜ ë°ì´í„° ëˆ„ë½ - symbol: {symbol}, price: {price}")
                            continue
                            
                        # ì‹¬ë³¼ ì •ê·œí™” (BINANCE:BTCUSDT -> BTCUSDT)
                        original_symbol = symbol
                        if ':' in symbol:
                            symbol = symbol.split(':')[-1]
                            logger.debug(f"ğŸ”„ ì‹¬ë³¼ ì •ê·œí™”: {original_symbol} -> {symbol}")
                        
                        # Coinbase providerì˜ ê²½ìš° ì‹¬ë³¼ í˜•ì‹ ë³€í™˜ (ETH-USD -> ETHUSDT, DOGE-USD -> DOGEUSDT)
                        if provider == 'coinbase':
                            coinbase_mapping = {
                                'BTC-USD': 'BTCUSDT',
                                'ETH-USD': 'ETHUSDT',
                                'ADA-USD': 'ADAUSDT',
                                'DOT-USD': 'DOTUSDT',
                                'LTC-USD': 'LTCUSDT',
                                'XRP-USD': 'XRPUSDT',
                                'DOGE-USD': 'DOGEUSDT',
                                'BCH-USD': 'BCHUSDT'
                            }
                            if symbol in coinbase_mapping:
                                symbol = coinbase_mapping[symbol]
                                logger.debug(f"ğŸ”„ Coinbase ì‹¬ë³¼ ë³€í™˜: {original_symbol} -> {symbol}")
                        
                        # Swissquote providerì˜ ê²½ìš° ì‹¬ë³¼ ì—­ì •ê·œí™” (XAU/USD -> GCUSD, XAG/USD -> SIUSD)
                        if provider == 'swissquote':
                            swissquote_mapping = {
                                'XAU/USD': 'GCUSD',
                                'XAG/USD': 'SIUSD'
                            }
                            if symbol in swissquote_mapping:
                                symbol = swissquote_mapping[symbol]
                                logger.debug(f"ğŸ”„ Swissquote ì‹¬ë³¼ ì—­ì •ê·œí™”: {original_symbol} -> {symbol}")
                            
                        asset_id = ticker_to_asset_id.get(symbol)
                        if not asset_id:
                            logger.warning(f"âŒ ìì‚° ë§¤ì¹­ ì‹¤íŒ¨ - symbol: {symbol} (ì‚¬ìš© ê°€ëŠ¥í•œ ìì‚°: {list(ticker_to_asset_id.keys())[:10]}...)")
                            continue
                            
                        logger.debug(f"âœ… ìì‚° ë§¤ì¹­ ì„±ê³µ - symbol: {symbol} -> asset_id: {asset_id}")
                        
                        # íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì‹± (UTCë¡œ ë³€í™˜)
                        timestamp_utc = self._parse_timestamp(raw_timestamp, provider) if raw_timestamp else datetime.utcnow()
                        logger.debug(f"â° íƒ€ì„ìŠ¤íƒ¬í”„: {timestamp_utc}")
                        
                        # Change ê³„ì‚° (prev_close ì¡°íšŒ)
                        change_amount, change_percent = await self._calculate_change(db, asset_id, price)
                        logger.debug(f"ğŸ“ˆ Change ê³„ì‚° - amount: {change_amount}, percent: {change_percent}")
                        
                        quote_data = {
                            "asset_id": asset_id,
                            "timestamp_utc": timestamp_utc,
                            "price": price,
                            "volume": volume,
                            "change_amount": change_amount,
                            "change_percent": change_percent,
                            "data_source": provider[:32]  # 32ì ì œí•œ
                        }
                        
                        logger.debug(f"ğŸ’¾ ì €ì¥í•  ë°ì´í„°: {quote_data}")
                        records_to_save.append(quote_data)
                        ack_items.append((stream_name_str, group_name, message_id))
                        logger.debug(f"âœ… ë©”ì‹œì§€ {message_id} ì²˜ë¦¬ ì™„ë£Œ")
                        
                    except Exception as e:
                        import traceback
                        logger.error(f"âŒ ìŠ¤íŠ¸ë¦¼ ë©”ì‹œì§€ {message_id} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        logger.error(f"ğŸ” ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
                        self.stats["errors"] += 1

            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            if records_to_save:
                logger.info(f"ğŸ’¾ {len(records_to_save)}ê°œ ë ˆì½”ë“œë¥¼ DBì— ì €ì¥ ì‹œì‘")
                save_success = await self._bulk_save_realtime_quotes(records_to_save)
                if save_success:
                    processed_count = len(records_to_save)
                    logger.info(f"âœ… DB ì €ì¥ ì„±ê³µ: {processed_count}ê°œ ë ˆì½”ë“œ")
                else:
                    logger.error("âŒ DB ì €ì¥ ì‹¤íŒ¨")
            else:
                logger.info("ğŸ“­ ì €ì¥í•  ë ˆì½”ë“œê°€ ì—†ìŒ")
            
            # ëª¨ë“  ë©”ì‹œì§€ ACK (í™œì„±/ë¹„í™œì„± ì†ŒìŠ¤ êµ¬ë¶„ ì—†ì´)
            if ack_items:
                ack_count = 0
                for stream_name, group_name, message_id in ack_items:
                    try:
                        await self.redis_client.xack(stream_name, group_name, message_id)
                        ack_count += 1
                    except Exception as e:
                        logger.warning(f"âŒ ACK ì‹¤íŒ¨ {stream_name}:{message_id}: {e}")
                logger.info(f"âœ… ACK ì™„ë£Œ: {ack_count}/{len(ack_items)}ê°œ ë©”ì‹œì§€")
            
            # ì£¼ê¸°ì  Stream TTL ì •ë¦¬ (1000ê°œ ë©”ì‹œì§€ ì´ˆê³¼ ì‹œ)
            for stream_name in self.realtime_streams.keys():
                try:
                    current_length = await self.redis_client.xlen(stream_name)
                    if current_length > 1000:
                        await self.redis_client.xtrim(stream_name, maxlen=1000, approximate=True)
                        logger.debug(f"ğŸ§¹ Stream TTL ì •ë¦¬: {stream_name} ({current_length} -> 1000ê°œ)")
                except Exception as e:
                    logger.warning(f"âš ï¸ Stream TTL ì •ë¦¬ ì‹¤íŒ¨ {stream_name}: {e}")
                    
        except Exception as e:
            import traceback
            # ì˜ˆì™¸ ë©”ì‹œì§€ê°€ ìŠ¤íŠ¸ë¦¼ ì´ë¦„ì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
            error_msg = str(e)
            if error_msg.startswith("b'") and error_msg.endswith("'"):
                logger.error(f"ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì¤‘ ìŠ¤íŠ¸ë¦¼ ì´ë¦„ ì˜¤ë¥˜: {error_msg}")
                logger.error("ì´ëŠ” ìŠ¤íŠ¸ë¦¼ ë°ì´í„° í˜•ì‹ ì˜¤ë¥˜ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.")
            else:
                logger.error(f"ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {error_msg}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            self.stats["errors"] += 1
            
        return processed_count

    async def _process_batch_queue(self) -> int:
        """ë°°ì¹˜ í ë°ì´í„° ì²˜ë¦¬"""
        if not self.redis_client:
            return 0
            
        processed_count = 0
        
        try:
            # íì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ìµœëŒ€ 100ê°œì”©)
            for _ in range(100):
                # Prefer RedisQueueManager pop; fallback to direct BLPOP
                task_wrapper = None
                if self.queue_manager:
                    task_wrapper = await self.queue_manager.pop_batch_task(timeout_seconds=1)
                else:
                    result = await self.redis_client.blpop(self.batch_queue, timeout=0.5)
                    if result:
                        _, task_data = result
                        try:
                            task_wrapper = json.loads(task_data)
                        except json.JSONDecodeError:
                            task_wrapper = None

                if not task_wrapper:
                    break
                    
                # Retry loop per task
                attempts = 0
                while attempts <= self.max_retries:
                    attempts += 1
                    try:
                        success = await self._process_batch_task(task_wrapper)
                        if success:
                            logger.info(f"Task {task_wrapper.get('type')} processed successfully.")
                            processed_count += 1
                            break  # ì„±ê³µ ì‹œ ë£¨í”„ ì¢…ë£Œ
                        else:
                            # ì²˜ë¦¬ ë¡œì§ì—ì„œ Falseë¥¼ ë°˜í™˜í•œ ê²½ìš° (ì¼ì‹œì  ì˜¤ë¥˜ì¼ ìˆ˜ ìˆìŒ)
                            raise RuntimeError(f"Task processing for {task_wrapper.get('type')} returned False.")
                    except Exception as e:
                        logger.warning(f"Attempt {attempts}/{self.max_retries} failed for task {task_wrapper.get('type')}: {e}")
                        if attempts > self.max_retries:
                            # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ ì‹œ DLQë¡œ ì´ë™
                            try:
                                raw_json = json.dumps(task_wrapper, ensure_ascii=False)
                            except Exception:
                                raw_json = str(task_wrapper)
                            if self.queue_manager:
                                await self.queue_manager.move_to_dlq(raw_json, str(e))
                            logger.error(f"Task failed after max retries, moving to DLQ: {e}")
                            self.stats["errors"] += 1
                            break
                        # ì¬ì‹œë„ ì „ ì ì‹œ ëŒ€ê¸°
                        await asyncio.sleep(self.retry_delay)
                    
        except Exception as e:
            logger.error(f"ë°°ì¹˜ í ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            self.stats["errors"] += 1
            
        return processed_count

    async def _process_batch_task(self, task: Dict[str, Any]) -> bool:
        """ë°°ì¹˜ íƒœìŠ¤í¬ ì²˜ë¦¬"""
        try:
            task_type = task.get("type")
            payload = task.get("payload")
            
            if not task_type or not payload:
                return False
                
            # í‘œì¤€ í˜ì´ë¡œë“œ: {"items": [...]} ìš°ì„  ì‚¬ìš©, ì•„ë‹ˆë©´ ê¸°ì¡´ payloadë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë˜í•‘
            items = payload.get("items") if isinstance(payload, dict) else None
            if items is None:
                items = payload if isinstance(payload, list) else [payload]

            # íƒœìŠ¤í¬ íƒ€ì…ë³„ ì²˜ë¦¬ ë¡œì§ (ì‹ /êµ¬ í‚¤ ëª¨ë‘ ì§€ì›)
            if task_type == "stock_profile":
                return await self._save_stock_profile(items)
            elif task_type == "stock_financials":
                return await self._save_stock_financials(items)
            elif task_type == "stock_estimate":
                return await self._save_stock_estimate(items)
            elif task_type == "etf_info":
                return await self._save_etf_info(items)
            elif task_type in ("crypto_info", "crypto_data"):
                return await self._save_crypto_data(items)
            elif task_type in ("ohlcv_data", "ohlcv_day_data", "ohlcv_intraday_data"):
                # metadata ì •ë³´ ì¶”ì¶œ
                metadata = payload.get("metadata", {}) if isinstance(payload, dict) else {}
                logger.info(f"Processing {task_type} task: items_count={len(items)}, metadata={metadata}")
                return await self._save_ohlcv_data(items, metadata)
            elif task_type == "index_data":
                return await self._save_index_data(items)
            elif task_type == "technical_indicators":
                return await self._save_technical_indicators(items)
            elif task_type == "onchain_metric":
                return await self._save_onchain_metric(items)
            elif task_type == "asset_settings_update":
                return await self._update_asset_settings(payload)
            else:
                logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” íƒœìŠ¤í¬ íƒ€ì…: {task_type}")
                return False
                
        except Exception as e:
            logger.error(f"ë°°ì¹˜ íƒœìŠ¤í¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return False

    async def _calculate_change(self, db: Session, asset_id: int, current_price: float) -> tuple:
        """Change ê³„ì‚° - prev_close ì¡°íšŒ ìš°ì„ ìˆœìœ„"""
        try:
            # 1. ë¡œì»¬ ìŠ¤ëƒ…ìƒ·/í€ë”ë©˜í„¸ í…Œì´ë¸”ì˜ ì „ì¼ì¢…ê°€ ì¡°íšŒ
            from ..models.asset import OHLCVData
            from sqlalchemy import desc
            
            # ìµœê·¼ ì¼ë´‰ ë°ì´í„°ì—ì„œ ì „ì¼ ì¢…ê°€ ì¡°íšŒ
            latest_day_data = db.query(OHLCVData).filter(
                OHLCVData.asset_id == asset_id
            ).order_by(desc(OHLCVData.timestamp_utc)).first()
            
            if latest_day_data and latest_day_data.close_price:
                prev_close = float(latest_day_data.close_price)
                change_amount = current_price - prev_close
                change_percent = (change_amount / prev_close) * 100.0 if prev_close != 0 else None
                return change_amount, change_percent
            
            # 2. ì™¸ë¶€ API ë°±ì—…ê°’ (í–¥í›„ êµ¬í˜„)
            # TODO: ì™¸ë¶€ APIì—ì„œ prev_close ì¡°íšŒ
            
            # 3. ì—†ìœ¼ë©´ null
            return None, None
            
        except Exception as e:
            logger.warning(f"Change ê³„ì‚° ì‹¤íŒ¨ asset_id={asset_id}: {e}")
            return None, None

    async def _bulk_save_realtime_quotes(self, records: List[Dict[str, Any]]) -> bool:
        """ì‹¤ì‹œê°„ ì¸ìš© ë°ì´í„° ì¼ê´„ ì €ì¥ - ì´ì¤‘ ì“°ê¸° (MySQL + PostgreSQL)"""
        try:
            logger.info(f"ğŸ’¾ RealtimeQuote ì´ì¤‘ ì“°ê¸° ì‹œì‘: {len(records)}ê°œ ë ˆì½”ë“œ")
            
            # MySQLê³¼ PostgreSQL ì„¸ì…˜ ìƒì„±
            from ..core.database import get_mysql_db, get_postgres_db
            logger.debug("ğŸ”— ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ìƒì„± ì¤‘...")
            mysql_db = next(get_mysql_db())
            postgres_db = next(get_postgres_db())
            logger.debug("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ìƒì„± ì™„ë£Œ")
            
            try:
                success_count = 0
                for i, record_data in enumerate(records):
                    try:
                        logger.debug(f"ğŸ” ë ˆì½”ë“œ {i+1}/{len(records)} ì²˜ë¦¬ ì‹œì‘")
                        logger.debug(f"ğŸ“‹ ë ˆì½”ë“œ ë°ì´í„°: asset_id={record_data.get('asset_id')}, data_source={record_data.get('data_source')}, price={record_data.get('price')}")
                        
                        # 1. MySQL ì‹¤ì‹œê°„ í…Œì´ë¸” ì €ì¥ (UPSERT) - asset_idë§Œìœ¼ë¡œ ìœ ë‹ˆí¬
                        logger.debug("ğŸ”„ 1ë‹¨ê³„: MySQL ì‹¤ì‹œê°„ í…Œì´ë¸” ì²˜ë¦¬ ì‹œì‘")
                        existing_quote = mysql_db.query(RealtimeQuote).filter(
                            RealtimeQuote.asset_id == record_data['asset_id']
                        ).first()
                        
                        if existing_quote:
                            # ê¸°ì¡´ ë ˆì½”ë“œ ì—…ë°ì´íŠ¸
                            logger.debug(f"ğŸ”„ MySQL ì‹¤ì‹œê°„ ë ˆì½”ë“œ ì—…ë°ì´íŠ¸: ID={existing_quote.id}")
                            existing_quote.timestamp_utc = record_data['timestamp_utc']
                            existing_quote.price = record_data['price']
                            existing_quote.volume = record_data['volume']
                            existing_quote.change_amount = record_data['change_amount']
                            existing_quote.change_percent = record_data['change_percent']
                            existing_quote.data_source = record_data['data_source']
                            logger.debug("âœ… MySQL ì‹¤ì‹œê°„ ë ˆì½”ë“œ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                        else:
                            # ìƒˆ ë ˆì½”ë“œ ìƒì„±
                            logger.debug(f"â• MySQL ì‹¤ì‹œê°„ ìƒˆ ë ˆì½”ë“œ ìƒì„±")
                            quote = RealtimeQuote(**record_data)
                            mysql_db.add(quote)
                            logger.debug("âœ… MySQL ì‹¤ì‹œê°„ ìƒˆ ë ˆì½”ë“œ ì¶”ê°€ ì™„ë£Œ")
                        
                        # 2. PostgreSQL ì‹¤ì‹œê°„ í…Œì´ë¸” ì €ì¥ (UPSERT)
                        logger.debug("ğŸ”„ 2ë‹¨ê³„: PostgreSQL ì‹¤ì‹œê°„ í…Œì´ë¸” ì²˜ë¦¬ ì‹œì‘")
                        from sqlalchemy.dialects.postgresql import insert
                        from sqlalchemy import func
                        
                        pg_data = record_data.copy()
                        logger.debug(f"ğŸ“‹ PostgreSQL ë°ì´í„° ì¤€ë¹„: {pg_data}")
                        
                        stmt = insert(RealtimeQuote).values(**pg_data)
                        stmt = stmt.on_conflict_do_update(
                            index_elements=['asset_id'],  # asset_idë¡œ ìœ ë‹ˆí¬ ì œì•½
                            set_={
                                'timestamp_utc': stmt.excluded.timestamp_utc,
                                'price': stmt.excluded.price,
                                'volume': stmt.excluded.volume,
                                'change_amount': stmt.excluded.change_amount,
                                'change_percent': stmt.excluded.change_percent,
                                'data_source': stmt.excluded.data_source,
                                'updated_at': func.now()
                            }
                        )
                        logger.debug("ğŸ”„ PostgreSQL INSERT/UPSERT ì‹¤í–‰ ì¤‘...")
                        postgres_db.execute(stmt)
                        logger.debug(f"âœ… PostgreSQL ì‹¤ì‹œê°„ ë ˆì½”ë“œ ì €ì¥/ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                        
                        # 3. MySQL ì‹œê°„ ìœˆë„ìš° ì§€ì—° í…Œì´ë¸” ì €ì¥ (UPSERT)
                        logger.debug("ğŸ”„ 3ë‹¨ê³„: MySQL ì§€ì—° í…Œì´ë¸” ì²˜ë¦¬ ì‹œì‘")
                        time_window = self._get_time_window(record_data['timestamp_utc'])
                        delay_record_data = record_data.copy()
                        delay_record_data['timestamp_utc'] = time_window
                        delay_record_data['data_interval'] = f'{self.time_window_minutes}m'
                        logger.debug(f"ğŸ“Š ì‹œê°„ ìœˆë„ìš° ê³„ì‚°: {time_window} (ì›ë³¸: {record_data['timestamp_utc']})")
                        
                        # ê¸°ì¡´ ì§€ì—° ë ˆì½”ë“œ í™•ì¸
                        existing_delay_quote = mysql_db.query(RealtimeQuoteTimeDelay).filter(
                            RealtimeQuoteTimeDelay.asset_id == delay_record_data['asset_id'],
                            RealtimeQuoteTimeDelay.timestamp_utc == time_window,
                            RealtimeQuoteTimeDelay.data_source == delay_record_data['data_source']
                        ).first()
                        
                        if existing_delay_quote:
                            # ê¸°ì¡´ ë ˆì½”ë“œ ì—…ë°ì´íŠ¸
                            logger.debug(f"ğŸ”„ MySQL ì§€ì—° ë ˆì½”ë“œ ì—…ë°ì´íŠ¸: ID={existing_delay_quote.id}")
                            existing_delay_quote.price = delay_record_data['price']
                            existing_delay_quote.volume = delay_record_data['volume']
                            existing_delay_quote.change_amount = delay_record_data['change_amount']
                            existing_delay_quote.change_percent = delay_record_data['change_percent']
                            logger.debug("âœ… MySQL ì§€ì—° ë ˆì½”ë“œ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                        else:
                            # ìƒˆ ë ˆì½”ë“œ ìƒì„±
                            logger.debug(f"â• MySQL ì§€ì—° ìƒˆ ë ˆì½”ë“œ ìƒì„±")
                            delay_quote = RealtimeQuoteTimeDelay(**delay_record_data)
                            mysql_db.add(delay_quote)
                            logger.debug("âœ… MySQL ì§€ì—° ìƒˆ ë ˆì½”ë“œ ì¶”ê°€ ì™„ë£Œ")
                        
                        # 4. PostgreSQL ì‹œê°„ ìœˆë„ìš° ì§€ì—° í…Œì´ë¸” ì €ì¥ (UPSERT)
                        logger.debug("ğŸ”„ 4ë‹¨ê³„: PostgreSQL ì§€ì—° í…Œì´ë¸” ì²˜ë¦¬ ì‹œì‘")
                        from ..models.asset import RealtimeQuoteTimeDelay as PGRealtimeQuoteTimeDelay
                        
                        pg_delay_data = delay_record_data.copy()
                        logger.debug(f"ğŸ“‹ PostgreSQL ì§€ì—° ë°ì´í„° ì¤€ë¹„: {pg_delay_data}")
                        
                        delay_stmt = insert(PGRealtimeQuoteTimeDelay).values(**pg_delay_data)
                        delay_stmt = delay_stmt.on_conflict_do_update(
                            index_elements=['asset_id', 'timestamp_utc', 'data_source'],
                            set_={
                                'price': delay_stmt.excluded.price,
                                'volume': delay_stmt.excluded.volume,
                                'change_amount': delay_stmt.excluded.change_amount,
                                'change_percent': delay_stmt.excluded.change_percent,
                                'updated_at': func.now()
                            }
                        )
                        logger.debug("ğŸ”„ PostgreSQL ì§€ì—° INSERT/UPSERT ì‹¤í–‰ ì¤‘...")
                        postgres_db.execute(delay_stmt)
                        logger.debug(f"âœ… PostgreSQL ì§€ì—° ë ˆì½”ë“œ ì €ì¥/ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                        
                        logger.debug(f"ğŸ“Š {self.time_window_minutes}ë¶„ ì§€ì—° ë ˆì½”ë“œ ì²˜ë¦¬: {time_window}")
                        
                        # ê° ë ˆì½”ë“œë§ˆë‹¤ ê°œë³„ì ìœ¼ë¡œ ì»¤ë°‹í•˜ì—¬ race condition ë°©ì§€
                        logger.debug("ğŸ”„ MySQL ì»¤ë°‹ ì‹¤í–‰ ì¤‘...")
                        mysql_db.commit()
                        logger.debug("âœ… MySQL ì»¤ë°‹ ì™„ë£Œ")
                        
                        logger.debug("ğŸ”„ PostgreSQL ì»¤ë°‹ ì‹¤í–‰ ì¤‘...")
                        postgres_db.commit()
                        logger.debug("âœ… PostgreSQL ì»¤ë°‹ ì™„ë£Œ")
                        
                        success_count += 1
                        logger.debug(f"âœ… ë ˆì½”ë“œ {i+1} ì´ì¤‘ ì“°ê¸° ì„±ê³µ")
                        
                    except Exception as e:
                        import traceback
                        logger.error(f"âŒ ë ˆì½”ë“œ {i+1} ì´ì¤‘ ì“°ê¸° ì‹¤íŒ¨: {e}")
                        logger.error(f"ğŸ” ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
                        logger.error(f"ğŸ“‹ ì‹¤íŒ¨í•œ ë ˆì½”ë“œ ë°ì´í„°: {record_data}")
                        logger.debug("ğŸ”„ MySQL ë¡¤ë°± ì‹¤í–‰ ì¤‘...")
                        mysql_db.rollback()
                        logger.debug("âœ… MySQL ë¡¤ë°± ì™„ë£Œ")
                        logger.debug("ğŸ”„ PostgreSQL ë¡¤ë°± ì‹¤í–‰ ì¤‘...")
                        postgres_db.rollback()
                        logger.debug("âœ… PostgreSQL ë¡¤ë°± ì™„ë£Œ")
                        continue
                        
            finally:
                logger.debug("ğŸ”— ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ì¢…ë£Œ ì¤‘...")
                mysql_db.close()
                postgres_db.close()
                logger.debug("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ì¢…ë£Œ ì™„ë£Œ")
                        
            logger.info(f"âœ… RealtimeQuote ì´ì¤‘ ì“°ê¸° ì™„ë£Œ: {success_count}/{len(records)}ê°œ ì„±ê³µ")
            return success_count > 0
        except Exception as e:
            import traceback
            logger.error(f"âŒ RealtimeQuote ì´ì¤‘ ì“°ê¸° ì‹¤íŒ¨: {e}")
            logger.error(f"ğŸ” ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            return False

    async def _save_stock_profile(self, items: List[Dict[str, Any]]) -> bool:
        """ì£¼ì‹ í”„ë¡œí•„ ë°ì´í„° ì €ì¥ (ì—…ì„œíŠ¸)"""
        try:
            if not items:
                return True

            logger.info(f"ì£¼ì‹ í”„ë¡œí•„ ë°ì´í„° ì €ì¥: {len(items)}ê°œ ë ˆì½”ë“œ")

            async with self.get_db_session() as db:
                from ..models.asset import StockProfile

                for item in items:
                    try:
                        asset_id = item.get("asset_id") or item.get("assetId") or item.get("asset_id".lower())
                        data = item.get("data") if "data" in item else item
                        if not asset_id or not isinstance(data, dict):
                            continue

                        # ë§¤í•‘: CompanyProfileData -> StockProfile ì»¬ëŸ¼
                        company_name = data.get("name") or data.get("company_name")
                        description = data.get("description")
                        sector = data.get("sector")
                        industry = data.get("industry")
                        website = data.get("website")
                        employees_count = data.get("employees") or data.get("fullTimeEmployees")
                        country = data.get("country")
                        address = data.get("address")
                        city = data.get("city")
                        state = data.get("state")  # ì£¼/ë„
                        zip_code = data.get("zip_code") or data.get("zip")  # ìš°í¸ë²ˆí˜¸
                        ceo = data.get("ceo") or data.get("CEO")
                        phone = data.get("phone")
                        logo_image_url = data.get("image") or data.get("logo")
                        # ê±°ë˜ì†Œ ë° ì‹ë³„ì ì •ë³´
                        exchange = data.get("exchange")
                        exchange_full_name = data.get("exchange_full_name") or data.get("exchangeFullName")
                        cik = data.get("cik")
                        isin = data.get("isin")
                        cusip = data.get("cusip")
                        # ipo_date íŒŒì‹±
                        ipo_date_val = data.get("ipoDate") or data.get("ipo_date")
                        ipo_date = None
                        if ipo_date_val:
                            try:
                                if isinstance(ipo_date_val, str):
                                    ipo_date = datetime.strptime(ipo_date_val.split("T")[0], "%Y-%m-%d").date()
                            except Exception:
                                ipo_date = None

                        profile: StockProfile = db.query(StockProfile).filter(StockProfile.asset_id == asset_id).first()
                        if profile:
                            if company_name is not None:
                                profile.company_name = company_name
                            if description is not None:
                                profile.description = description
                            if sector is not None:
                                profile.sector = sector
                            if industry is not None:
                                profile.industry = industry
                            if website is not None:
                                profile.website = website
                            if employees_count is not None:
                                profile.employees_count = employees_count
                            if country is not None:
                                profile.country = country
                            if address is not None:
                                profile.address = address
                            if city is not None:
                                profile.city = city
                            if ceo is not None:
                                profile.ceo = ceo
                            if phone is not None:
                                profile.phone = phone
                            if logo_image_url is not None:
                                profile.logo_image_url = logo_image_url
                            if ipo_date is not None:
                                profile.ipo_date = ipo_date
                            # ìƒˆë¡œìš´ ì£¼ì†Œ í•„ë“œë“¤
                            if state is not None:
                                profile.state = state
                            if zip_code is not None:
                                profile.zip_code = zip_code
                            # ìƒˆë¡œìš´ ê±°ë˜ì†Œ í•„ë“œë“¤
                            if exchange is not None:
                                profile.exchange = exchange
                            if exchange_full_name is not None:
                                profile.exchange_full_name = exchange_full_name
                            if cik is not None:
                                profile.cik = cik
                            if isin is not None:
                                profile.isin = isin
                            if cusip is not None:
                                profile.cusip = cusip
                        else:
                            profile = StockProfile(
                                asset_id=asset_id,
                                company_name=company_name or "",
                                description=description,
                                sector=sector,
                                industry=industry,
                                website=website,
                                employees_count=employees_count,
                                country=country,
                                address=address,
                                city=city,
                                state=state,  # ì£¼/ë„
                                zip_code=zip_code,  # ìš°í¸ë²ˆí˜¸
                                ceo=ceo,
                                phone=phone,
                                logo_image_url=logo_image_url,
                                ipo_date=ipo_date,
                                # ê±°ë˜ì†Œ ë° ì‹ë³„ì ì •ë³´
                                exchange=exchange,
                                exchange_full_name=exchange_full_name,
                                cik=cik,
                                isin=isin,
                                cusip=cusip,
                            )
                            db.add(profile)

                        db.commit()
                    except Exception as e:
                        logger.warning(f"ê°œë³„ ì£¼ì‹ í”„ë¡œí•„ ì €ì¥ ì‹¤íŒ¨(asset_id={item.get('asset_id')}): {e}")
                        db.rollback()
                        continue

            return True
        except Exception as e:
            logger.error(f"ì£¼ì‹ í”„ë¡œí•„ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    async def _save_etf_info(self, items: List[Dict[str, Any]]) -> bool:
        """ETF ì •ë³´ ë°ì´í„° ì €ì¥ (UPSERT ë¡œì§)"""
        try:
            if not items:
                return True

            logger.info(f"ETF ì •ë³´ ë°ì´í„° ì €ì¥: {len(items)}ê°œ ë ˆì½”ë“œ")

            async with self.get_db_session() as db:
                from ..models.asset import ETFInfo
                from datetime import datetime, date

                for item in items:
                    try:
                        asset_id = item.get("asset_id") or item.get("assetId")
                        data = item.get("data") if isinstance(item, dict) and "data" in item else item
                        if not asset_id or not isinstance(data, dict):
                            continue

                        # snapshot_date íŒŒì‹±
                        snapshot = data.get("snapshot_date") or data.get("snapshotDate")
                        parsed_snapshot = None
                        if snapshot:
                            try:
                                if isinstance(snapshot, str):
                                    s = snapshot.split("T")[0]
                                    parsed_snapshot = datetime.strptime(s, "%Y-%m-%d").date()
                                elif isinstance(snapshot, datetime):
                                    parsed_snapshot = snapshot.date()
                            except Exception:
                                parsed_snapshot = None
                        if parsed_snapshot is None:
                            parsed_snapshot = datetime.utcnow().date()

                        # ê¸°ì¡´ ETF ì •ë³´ ì¡°íšŒ (asset_idë¡œë§Œ ì¡°íšŒ - unique constraint)
                        existing: ETFInfo = (
                            db.query(ETFInfo)
                            .filter(ETFInfo.asset_id == asset_id)
                            .first()
                        )

                        if existing:
                            # ê¸°ì¡´ ë ˆì½”ë“œ ì—…ë°ì´íŠ¸
                            if data.get("net_assets") is not None:
                                existing.net_assets = data.get("net_assets")
                            if data.get("net_expense_ratio") is not None:
                                existing.net_expense_ratio = data.get("net_expense_ratio")
                            if data.get("portfolio_turnover") is not None:
                                existing.portfolio_turnover = data.get("portfolio_turnover")
                            if data.get("dividend_yield") is not None:
                                existing.dividend_yield = data.get("dividend_yield")
                            if data.get("inception_date") is not None:
                                existing.inception_date = data.get("inception_date")
                            if data.get("leveraged") is not None:
                                existing.leveraged = data.get("leveraged")
                            if data.get("sectors") is not None:
                                existing.sectors = data.get("sectors")
                            if data.get("holdings") is not None:
                                existing.holdings = data.get("holdings")
                            existing.snapshot_date = parsed_snapshot
                        else:
                            # ìƒˆ ë ˆì½”ë“œ ìƒì„±
                            etf_info = ETFInfo(
                                asset_id=asset_id,
                                snapshot_date=parsed_snapshot,
                                net_assets=data.get("net_assets"),
                                net_expense_ratio=data.get("net_expense_ratio"),
                                portfolio_turnover=data.get("portfolio_turnover"),
                                dividend_yield=data.get("dividend_yield"),
                                inception_date=data.get("inception_date"),
                                leveraged=data.get("leveraged"),
                                sectors=data.get("sectors"),
                                holdings=data.get("holdings")
                            )
                            db.add(etf_info)

                        db.commit()
                        logger.info(f"ETF ì •ë³´ ì €ì¥ ì™„ë£Œ: asset_id={asset_id}")
                        
                    except Exception as e:
                        logger.warning(f"ê°œë³„ ETF ì •ë³´ ì €ì¥ ì‹¤íŒ¨(asset_id={item.get('asset_id')}): {e}")
                        db.rollback()
                        continue

            return True
        except Exception as e:
            logger.error(f"ETF ì •ë³´ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    async def _save_crypto_data(self, items: List[Dict[str, Any]]) -> bool:
        """í¬ë¦½í†  ë°ì´í„° ì €ì¥"""
        if not items:
            return True
            
        try:
            logger.info(f"í¬ë¦½í†  ë°ì´í„° ì €ì¥: {len(items)}ê°œ ë ˆì½”ë“œ")
            
            async with self.get_db_session() as db:
                from app.crud.asset import crud_crypto_data
                
                saved_count = 0
                for item in items:
                    try:
                        # asset_id ì¶”ì¶œ
                        asset_id = item.get('asset_id')
                        if not asset_id:
                            logger.warning(f"crypto_data ì €ì¥ ì‹¤íŒ¨: asset_id ì—†ìŒ - {item}")
                            continue
                        
                        # CryptoData ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ë°ì´í„° ë³€í™˜
                        crypto_data_dict = {
                            'asset_id': asset_id,
                            'symbol': item.get('symbol', ''),
                            'name': item.get('name', ''),
                            'market_cap': item.get('market_cap'),
                            'circulating_supply': item.get('circulating_supply'),
                            'total_supply': item.get('total_supply'),
                            'max_supply': item.get('max_supply'),
                            'current_price': item.get('price') or item.get('current_price'),
                            'volume_24h': item.get('volume_24h'),
                            'percent_change_1h': item.get('percent_change_1h'),
                            'percent_change_24h': item.get('change_24h') or item.get('percent_change_24h'),
                            'percent_change_7d': item.get('percent_change_7d'),
                            'percent_change_30d': item.get('percent_change_30d'),
                            'cmc_rank': item.get('rank'),
                            'category': item.get('category'),
                            'description': item.get('description'),
                            'logo_url': item.get('logo_url'),
                            'website_url': item.get('website_url'),
                            'price': item.get('price'),
                            'slug': item.get('slug'),
                            'date_added': item.get('date_added'),
                            'platform': item.get('platform'),
                            'explorer': item.get('explorer'),
                            'source_code': item.get('source_code'),
                            'tags': item.get('tags'),
                            'is_active': True
                        }
                        
                        # None ê°’ ì œê±°
                        crypto_data_dict = {k: v for k, v in crypto_data_dict.items() if v is not None}
                        
                        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                        result = crud_crypto_data.upsert_crypto_data(db, crypto_data_dict)
                        if result:
                            saved_count += 1
                            logger.debug(f"crypto_data ì €ì¥ ì„±ê³µ: asset_id={asset_id}, symbol={item.get('symbol')}")
                        else:
                            logger.warning(f"crypto_data ì €ì¥ ì‹¤íŒ¨: asset_id={asset_id}")
                            
                    except Exception as e:
                        logger.error(f"crypto_data ì €ì¥ ì¤‘ ì˜¤ë¥˜: asset_id={item.get('asset_id')}, error={e}")
                        continue
                
                logger.info(f"í¬ë¦½í†  ë°ì´í„° ì €ì¥ ì™„ë£Œ: {saved_count}/{len(items)}ê°œ ë ˆì½”ë“œ")
                return saved_count > 0
            
        except Exception as e:
            logger.error(f"crypto_data ì €ì¥ ì¤‘ ì „ì²´ ì˜¤ë¥˜: {e}")
            return False

    async def _save_ohlcv_data(self, items: List[Dict[str, Any]], metadata: Dict[str, Any] = None) -> bool:
        """OHLCV ë°ì´í„° ì €ì¥ - ì¼ë´‰ê³¼ ì¸íŠ¸ë¼ë°ì´ ë°ì´í„°ë¥¼ ì ì ˆí•œ í…Œì´ë¸”ì— ë¶„ë¦¬ ì €ì¥"""
        if not items:
            return True
        
        # metadataì—ì„œ asset_id, interval, is_backfill ì¶”ì¶œ
        asset_id = metadata.get("asset_id") if metadata else None
        interval = metadata.get("interval") if metadata else None
        is_backfill = metadata.get("is_backfill", False) if metadata else False
        
        if not asset_id or not interval:
            logger.warning(f"OHLCV ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: asset_id={asset_id}, interval={interval} ì •ë³´ ë¶€ì¡±")
            return False
        
        # intervalì— ë”°ë¼ ì €ì¥í•  í…Œì´ë¸” ê²°ì • (ìš”ì²­ ì£¼ê¸° ê¸°ë°˜, ì´í›„ ì‹¤ì œ ì£¼ê¸° ê²€ì¦ë¡œì§ì—ì„œ ì¬ì¡°ì •)
        # 1d, 1w, 1mëŠ” ohlcv_day_data, ë‚˜ë¨¸ì§€ëŠ” ohlcv_intraday_data
        is_daily_request = interval in ["1d", "daily", "1w", "1m"] or interval is None
        table_name = "ohlcv_day_data" if is_daily_request else "ohlcv_intraday_data"
        
        logger.info(f"OHLCV ë°ì´í„° ì €ì¥ ì‹œì‘: asset_id={asset_id}, interval={interval}, table={table_name}, records={len(items)}")
        
        async with self.get_db_session() as db:
            try:
                # 0) ì‚¬ì „ ë°©ì–´: items ë‚´ timestamp_utcë¥¼ ë¨¼ì € í‘œì¤€í™” (UTC naive datetime)
                from datetime import datetime, timezone
                from app.utils.helpers import normalize_timestamp_to_date, normalize_timestamp_to_trading_hour
                
                def _normalize_ts_val(val: Any, is_daily_data: bool = False) -> Any:
                    try:
                        if isinstance(val, datetime):
                            if val.tzinfo is not None and val.tzinfo.utcoffset(val) is not None:
                                val = val.astimezone(timezone.utc).replace(tzinfo=None)
                            
                            # ì¼ë´‰ ë°ì´í„°ì˜ ê²½ìš° ë‚ ì§œë§Œìœ¼ë¡œ ì •ê·œí™” (00:00:00)
                            if is_daily_data:
                                return normalize_timestamp_to_date(val)
                            else:
                                return val.replace(microsecond=0)
                        
                        s = str(val)
                        if not s:
                            return val
                        if s.endswith('Z'):
                            s = s[:-1]
                        s = s.replace('T', ' ')
                        if '+' in s:
                            s = s.split('+')[0]
                        if '.' in s:
                            s = s.split('.', 1)[0]
                        
                        parsed_dt = datetime.strptime(s, "%Y-%m-%d %H:%M:%S")
                        
                        # ì¼ë´‰ ë°ì´í„°ì˜ ê²½ìš° ë‚ ì§œë§Œìœ¼ë¡œ ì •ê·œí™” (00:00:00)
                        if is_daily_data:
                            return normalize_timestamp_to_date(parsed_dt)
                        else:
                            return parsed_dt
                    except Exception:
                        return val

                for it in items:
                    if isinstance(it, dict) and 'timestamp_utc' in it:
                        it['timestamp_utc'] = _normalize_ts_val(it.get('timestamp_utc'), is_daily_data=is_daily_request)

                # DB ì €ì¥ì„ ìœ„í•´ Pydantic ëª¨ë¸ ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                from app.external_apis.base.schemas import OhlcvDataPoint
                ohlcv_list = [OhlcvDataPoint(**item) for item in items]

                # OHLCV ë°ì´í„°ì— asset_idì™€ data_interval ì¶”ê°€
                # MySQL DATETIME ì»¬ëŸ¼ê³¼ í˜¸í™˜ë˜ë„ë¡ timestamp_utcëŠ” "YYYY-MM-DD HH:MM:SS" ë˜ëŠ” naive UTC datetimeìœ¼ë¡œ ì „ë‹¬
                from datetime import datetime, timezone

                ohlcv_data_list = []
                # ì¸íŠ¸ë¼ë°ì´ ìš”ì²­ì‹œ ì‹¤ì œ ì£¼ê¸° ë¶ˆì¼ì¹˜ ê°ì§€ë¥¼ ìœ„í•œ í”Œë˜ê·¸
                intraday_request = not is_daily_request
                wrong_interval_count = 0
                for i, ohlcv_item in enumerate(ohlcv_list):
                    # model_dump(mode='python')ì„ ì‚¬ìš©í•˜ì—¬ datetimeì„ ê·¸ëŒ€ë¡œ ìœ ì§€
                    item_dict = ohlcv_item.model_dump(mode='python')

                    ts = item_dict.get('timestamp_utc')
                    # Pydanticì—ì„œ datetimeìœ¼ë¡œ ìœ ì§€ëœ ê²½ìš°ë§Œ ì²˜ë¦¬
                    if isinstance(ts, datetime):
                        # tz-awareì´ë©´ UTCë¡œ ë³€í™˜ í›„ naiveë¡œ ë§Œë“¤ê¸°
                        if ts.tzinfo is not None and ts.tzinfo.utcoffset(ts) is not None:
                            ts = ts.astimezone(timezone.utc).replace(tzinfo=None)
                        # ì´ˆ ë‹¨ìœ„ë¡œ ë§ì¶”ê¸° (ë§ˆì´í¬ë¡œì´ˆ ì œê±°)
                        ts = ts.replace(microsecond=0)
                        item_dict['timestamp_utc'] = ts
                    else:
                        # í˜¹ì‹œ ë¬¸ìì—´ë¡œ ë“¤ì–´ì˜¨ ê²½ìš° ì•ˆì „í•˜ê²Œ íŒŒì‹±í•´ UTC naiveë¡œ ë³€í™˜
                        try:
                            # ì§€ì› í¬ë§·: 2025-08-05T04:00:00Z, 2025-08-05 04:00:00+00:00, ë“±
                            s = str(ts)
                            if s.endswith('Z'):
                                s = s[:-1]
                            # ê³µë°±/"T" ëª¨ë‘ í—ˆìš©
                            s = s.replace('T', ' ')
                            # íƒ€ì„ì¡´ ì œê±°
                            if '+' in s:
                                s = s.split('+')[0]
                            if '.' in s:
                                base, frac = s.split('.', 1)
                                s = base
                            parsed = datetime.strptime(s, "%Y-%m-%d %H:%M:%S")
                            item_dict['timestamp_utc'] = parsed
                        except Exception:
                            # ë§ˆì§€ë§‰ ìˆ˜ë‹¨: ê·¸ëŒ€ë¡œ ë‘ë˜, ë’¤ì˜ CRUDì—ì„œ ì‹¤íŒ¨í•˜ë©´ ìŠ¤í‚µë  ê²ƒ
                            pass

                    item_dict['asset_id'] = asset_id
                    
                    # ì¼ë´‰ ë°ì´í„°ëŠ” í•­ìƒ '1d'ë¡œ ì„¤ì • (ì£¼ë´‰/ì›”ë´‰ì€ ë³„ë„ ë°°ì¹˜ ì‘ì—…ì—ì„œ ìƒì„±)
                    if is_daily_request:
                        item_dict['data_interval'] = '1d'
                    else:
                        # ì¸íŠ¸ë¼ë°ì´ ë°ì´í„°ì˜ ê²½ìš° ì›ë˜ interval ì‚¬ìš©
                        item_dict['data_interval'] = interval if interval else '1h'

                    ohlcv_data_list.append(item_dict)

                    # ì¸íŠ¸ë¼ë°ì´ ìš”ì²­ì¸ë° ì‹¤ì œê°€ 1d/1w/1më©´ ì¹´ìš´íŠ¸
                    if intraday_request and item_dict['data_interval'] in ["1d", "daily", "1w", "1m"]:
                        wrong_interval_count += 1

                # ì¸íŠ¸ë¼ë°ì´ ìš”ì²­ì— ëŒ€í•œ ê°€ë“œ: ì „ì²´ì˜ ê³¼ë°˜ì´ ì¼ë´‰/ì£¼ë´‰ì´ë©´ ë¦¬ë¼ìš°íŒ…
                if intraday_request and wrong_interval_count > 0:
                    if wrong_interval_count >= max(1, len(ohlcv_data_list) // 2):
                        logger.warning(
                            f"ì¸íŠ¸ë¼ë°ì´ ìš”ì²­(interval={interval})ì´ì§€ë§Œ ì‹¤ì œ ë°ì´í„°ì˜ ê³¼ë°˜({wrong_interval_count}/{len(ohlcv_data_list)})ê°€ ì¼ë´‰/ì£¼ë´‰ì…ë‹ˆë‹¤. ì¼ë´‰ í…Œì´ë¸”ë¡œ ë¦¬ë¼ìš°íŒ…í•©ë‹ˆë‹¤.")
                        table_name = "ohlcv_day_data"
                        is_daily_request = True
                
                # CRUDë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì €ì¥ - í…Œì´ë¸”ë³„ë¡œ ë¶„ë¦¬
                from app.crud.asset import crud_ohlcv
                if is_daily_request:
                    added_count = crud_ohlcv.bulk_upsert_ohlcv_daily(db, ohlcv_data_list)
                else:
                    added_count = crud_ohlcv.bulk_upsert_ohlcv_intraday(db, ohlcv_data_list)
                
                logger.info(f"OHLCV ë°ì´í„° ì €ì¥ ì™„ë£Œ: asset_id={asset_id}, interval={interval}, table={table_name}, added={added_count}ê°œ ë ˆì½”ë“œ")
                return True
                
            except Exception as e:
                logger.error(f"OHLCV ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: asset_id={asset_id}, interval={interval}, table={table_name}, error={e}", exc_info=True)
                return False

    async def _save_stock_financials(self, items: List[Dict[str, Any]]) -> bool:
        """ì£¼ì‹ ì¬ë¬´ ë°ì´í„° ì €ì¥ (ìŠ¤ëƒ…ìƒ·, ë³‘í•© ì—…ì„œíŠ¸)

        ê·œì¹™:
        - ë™ì¼í•œ asset_id + snapshot_date ë ˆì½”ë“œê°€ ìˆìœ¼ë©´ ì œê³µëœ ì»¬ëŸ¼ë§Œ ë®ì–´ì”€(None/ì—†ìŒì€ ë¬´ì‹œ)
        - ì—†ë˜ ë ˆì½”ë“œëŠ” ìƒˆë¡œ ìƒì„±
        """
        try:
            if not items:
                return True

            logger.info(f"ì£¼ì‹ ì¬ë¬´ ë°ì´í„° ì €ì¥: {len(items)}ê°œ ë ˆì½”ë“œ")

            async with self.get_db_session() as db:
                from ..models.asset import StockFinancial
                from datetime import datetime, date

                updatable_fields = {
                    "currency",
                    "market_cap",
                    "ebitda",
                    "shares_outstanding",
                    "pe_ratio",
                    "peg_ratio",
                    "beta",
                    "eps",
                    "dividend_yield",
                    "dividend_per_share",
                    "profit_margin_ttm",
                    "return_on_equity_ttm",
                    "revenue_ttm",
                    "price_to_book_ratio",
                    "week_52_high",
                    "week_52_low",
                    "day_50_moving_avg",
                    "day_200_moving_avg",
                    # ì¶”ê°€ ì¬ë¬´ ì§€í‘œ
                    "book_value",
                    "revenue_per_share_ttm",
                    "operating_margin_ttm",
                    "return_on_assets_ttm",
                    "gross_profit_ttm",
                    "quarterly_earnings_growth_yoy",
                    "quarterly_revenue_growth_yoy",
                    "analyst_target_price",
                    "trailing_pe",
                    "forward_pe",
                    "price_to_sales_ratio_ttm",
                    "ev_to_revenue",
                    "ev_to_ebitda",
                }

                for item in items:
                    try:
                        asset_id = item.get("asset_id") or item.get("assetId")
                        data = item.get("data") if isinstance(item, dict) and "data" in item else item
                        if not asset_id or not isinstance(data, dict):
                            continue

                        # ì˜ë¯¸ ìˆëŠ” ê°’ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ìŠ¤í‚µ (í†µí™” ì œì™¸)
                        meaningful_keys = [
                            "market_cap", "ebitda", "shares_outstanding", "pe_ratio", "peg_ratio",
                            "beta", "eps", "dividend_yield", "dividend_per_share", "profit_margin_ttm",
                            "return_on_equity_ttm", "revenue_ttm", "price_to_book_ratio",
                            "week_52_high", "week_52_low", "day_50_moving_avg", "day_200_moving_avg",
                        ]
                        if not any((data.get(k) is not None) for k in meaningful_keys):
                            # ì €ì¥í•  ì‹¤ì§ˆ ê°’ì´ ì—†ìœ¼ë©´ ê±´ë„ˆëœ€
                            continue

                        # snapshot_date íŒŒì‹±(ê°€ëŠ¥í•˜ë©´ ë‚ ì§œë§Œ ì €ì¥)
                        snapshot = data.get("snapshot_date") or data.get("snapshotDate")
                        parsed_snapshot = None
                        if snapshot:
                            try:
                                if isinstance(snapshot, str):
                                    # YYYY-MM-DD í˜¹ì€ ISO
                                    s = snapshot.split("T")[0]
                                    parsed_snapshot = datetime.strptime(s, "%Y-%m-%d").date()
                                elif isinstance(snapshot, datetime):
                                    parsed_snapshot = snapshot.date()
                            except Exception:
                                parsed_snapshot = None
                        if parsed_snapshot is None:
                            parsed_snapshot = datetime.utcnow().date()

                        existing: StockFinancial = (
                            db.query(StockFinancial)
                            .filter(StockFinancial.asset_id == asset_id)
                            .first()
                        )

                        if existing:
                            # ì„ íƒì  ë³‘í•© ì—…ë°ì´íŠ¸(None/ë¯¸ì¡´ì¬ í‚¤ëŠ” ë¬´ì‹œ)
                            for field in updatable_fields:
                                if field in data and data.get(field) is not None and hasattr(existing, field):
                                    setattr(existing, field, data.get(field))
                        else:
                            # ìƒì„± ì‹œì—ë„ ì œê³µëœ í•„ë“œë§Œ ì„¸íŒ…
                            new_kwargs = {"asset_id": asset_id, "snapshot_date": parsed_snapshot}
                            for field in updatable_fields:
                                val = data.get(field)
                                if val is not None:
                                    new_kwargs[field] = val
                            profile = StockFinancial(**new_kwargs)
                            db.add(profile)

                        db.commit()
                    except Exception as e:
                        logger.warning(f"ê°œë³„ ì£¼ì‹ ì¬ë¬´ ì €ì¥ ì‹¤íŒ¨(asset_id={item.get('asset_id')}): {e}")
                        db.rollback()
                        continue

            return True
        except Exception as e:
            logger.error(f"ì£¼ì‹ ì¬ë¬´ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    async def _save_stock_estimate(self, items: List[Dict[str, Any]]) -> bool:
        """ì£¼ì‹ ì¶”ì •ì¹˜ ë°ì´í„° ì €ì¥ (ë³‘í•© ì—…ì„œíŠ¸)

        ê·œì¹™:
        - ë™ì¼í•œ asset_id + fiscal_date ë ˆì½”ë“œê°€ ìˆìœ¼ë©´ ì œê³µëœ ì»¬ëŸ¼ë§Œ ë®ì–´ì”€(None/ì—†ìŒì€ ë¬´ì‹œ)
        - ì—†ë˜ ë ˆì½”ë“œëŠ” ìƒˆë¡œ ìƒì„±
        """
        try:
            if not items:
                return True

            logger.info(f"ì£¼ì‹ ì¶”ì •ì¹˜ ë°ì´í„° ì €ì¥: {len(items)}ê°œ ë ˆì½”ë“œ")

            async with self.get_db_session() as db:
                from ..models.asset import StockAnalystEstimate
                from datetime import datetime, date

                # DB ì»¬ëŸ¼ ìŠ¤í‚¤ë§ˆ ê¸°ì¤€ì˜ í•„ë“œ ì§‘í•©
                updatable_fields = {
                    "revenue_avg", "revenue_low", "revenue_high",
                    "eps_avg", "eps_low", "eps_high",
                    "revenue_analysts_count", "eps_analysts_count",
                    "ebitda_avg", "ebitda_low", "ebitda_high",
                    "ebit_avg", "ebit_low", "ebit_high",
                    "net_income_avg", "net_income_low", "net_income_high",
                    "sga_expense_avg", "sga_expense_low", "sga_expense_high",
                }

                for item in items:
                    try:
                        asset_id = item.get("asset_id") or item.get("assetId")
                        data = item.get("data") if isinstance(item, dict) and "data" in item else item
                        if not asset_id or not isinstance(data, dict):
                            continue

                        # ë‹¤ì–‘í•œ í‚¤ ì¼€ì´ìŠ¤ í—ˆìš©
                        fiscal_date = (
                            data.get("fiscal_date") or data.get("fiscalDate") or data.get("date")
                        )
                        parsed_date = None
                        if fiscal_date:
                            try:
                                if isinstance(fiscal_date, str):
                                    s = fiscal_date.split("T")[0]
                                    parsed_date = datetime.strptime(s, "%Y-%m-%d").date()
                                elif isinstance(fiscal_date, datetime):
                                    parsed_date = fiscal_date.date()
                            except Exception:
                                parsed_date = None
                        if parsed_date is None:
                            # ë‚ ì§œê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ (ì¶”ì •ì¹˜ëŠ” ë‚ ì§œ ê¸°ì¤€ ë³‘í•© í•„ìš”)
                            continue

                        existing: StockAnalystEstimate = (
                            db.query(StockAnalystEstimate)
                            .filter(StockAnalystEstimate.asset_id == asset_id, StockAnalystEstimate.fiscal_date == parsed_date)
                            .first()
                        )

                        if existing:
                            for field in updatable_fields:
                                if field in data and data.get(field) is not None and hasattr(existing, field):
                                    setattr(existing, field, data.get(field))
                        else:
                            new_kwargs = {"asset_id": asset_id, "fiscal_date": parsed_date}
                            for field in updatable_fields:
                                val = data.get(field)
                                if val is not None:
                                    new_kwargs[field] = val
                            est = StockAnalystEstimate(**new_kwargs)
                            db.add(est)

                        db.commit()
                    except Exception as e:
                        logger.warning(f"ê°œë³„ ì£¼ì‹ ì¶”ì •ì¹˜ ì €ì¥ ì‹¤íŒ¨(asset_id={item.get('asset_id')}): {e}")
                        db.rollback()
                        continue

            return True
        except Exception as e:
            logger.error(f"ì£¼ì‹ ì¶”ì •ì¹˜ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    async def _save_index_data(self, items: List[Dict[str, Any]]) -> bool:
        """ì§€ìˆ˜ ë°ì´í„° ì €ì¥"""
        logger.info(f"ì§€ìˆ˜ ë°ì´í„° ì €ì¥: {len(items)}ê°œ ë ˆì½”ë“œ")
        return True

    async def _save_technical_indicators(self, items: List[Dict[str, Any]]) -> bool:
        """ê¸°ìˆ ì  ì§€í‘œ ë°ì´í„° ì €ì¥"""
        logger.info(f"ê¸°ìˆ ì  ì§€í‘œ ë°ì´í„° ì €ì¥: {len(items)}ê°œ ë ˆì½”ë“œ")
        return True

    async def _save_onchain_metric(self, items: List[Dict[str, Any]]) -> bool:
        """ì˜¨ì²´ì¸ ë©”íŠ¸ë¦­ ë°ì´í„° ì €ì¥"""
        logger.info(f"ì˜¨ì²´ì¸ ë©”íŠ¸ë¦­ ë°ì´í„° ì €ì¥: {len(items)}ê°œ ë ˆì½”ë“œ")
        return True

    async def _update_asset_settings(self, payload: Dict[str, Any]) -> bool:
        """ìì‚° ì„¤ì • ì—…ë°ì´íŠ¸ (íë¥¼ í†µí•œ ê°„ë‹¨ ì„¤ì • ë°˜ì˜)"""
        logger.info(f"ìì‚° ì„¤ì • ì—…ë°ì´íŠ¸ íƒœìŠ¤í¬ ì²˜ë¦¬: {payload}")
        return True

    async def _log_stats(self):
        """ì²˜ë¦¬ í†µê³„ ë¡œê¹…"""
        if self.stats["last_processed"]:
            logger.info(
                f"Data Processor í†µê³„ - "
                f"ì‹¤ì‹œê°„: {self.stats['realtime_processed']}, "
                f"ë°°ì¹˜: {self.stats['batch_processed']}, "
                f"ì˜¤ë¥˜: {self.stats['errors']}"
            )

    async def start(self):
        """Data Processor ì‹œì‘"""
        logger.info("Data Processor ì„œë¹„ìŠ¤ ì‹œì‘")
        logger.info("Data Processor start() ë©”ì„œë“œ í˜¸ì¶œë¨")
        self.running = True
        
        # Redis ì—°ê²°
        if not await self._connect_redis():
            logger.error("Redis ì—°ê²° ì‹¤íŒ¨ë¡œ ì„œë¹„ìŠ¤ ì¢…ë£Œ")
            return
            
        logger.info("Redis ì—°ê²° ì„±ê³µ, ë©”ì¸ ë£¨í”„ ì‹œì‘")
        try:
            logger.info("Data Processor main loop started")
            while self.running:
                start_time = time.time()
                logger.debug("Processing cycle started")
                
                # ì‹¤ì‹œê°„ ë° ë°°ì¹˜ ë°ì´í„° ë™ì‹œ ì²˜ë¦¬
                logger.debug("ğŸ”„ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì‹œì‘")
                try:
                    logger.debug("ğŸ”„ asyncio.gather í˜¸ì¶œ ì „")
                    realtime_count, batch_count = await asyncio.gather(
                        self._process_realtime_streams(),
                        self._process_batch_queue(),
                        return_exceptions=True
                    )
                    logger.debug(f"ğŸ”„ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì™„ë£Œ: {realtime_count}")
                except Exception as e:
                    import traceback
                    logger.error(f"âŒ asyncio.gather ì˜¤ë¥˜: {e}")
                    logger.error(f"ğŸ” ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
                    realtime_count, batch_count = 0, 0
                
                # ê²°ê³¼ ì²˜ë¦¬
                if isinstance(realtime_count, Exception):
                    logger.error(f"ì‹¤ì‹œê°„ ì²˜ë¦¬ ì˜¤ë¥˜: {realtime_count}")
                    realtime_count = 0
                if isinstance(batch_count, Exception):
                    logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {batch_count}")
                    batch_count = 0
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                self.stats["realtime_processed"] += realtime_count
                self.stats["batch_processed"] += batch_count
                self.stats["last_processed"] = datetime.utcnow()
                
                # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚° ë° ëŒ€ê¸°
                processing_time = time.time() - start_time
                if processing_time < self.processing_interval:
                    await asyncio.sleep(self.processing_interval - processing_time)
                    
        except KeyboardInterrupt:
            logger.info("Data Processor ì„œë¹„ìŠ¤ ì¢…ë£Œ ìš”ì²­")
        except Exception as e:
            logger.error(f"Data Processor ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {e}")
        finally:
            self.running = False
            if self.redis_client:
                await self.redis_client.close()
            logger.info("Data Processor ì„œë¹„ìŠ¤ ì¢…ë£Œ")

    async def stop(self):
        """Data Processor ì¤‘ì§€"""
        self.running = False
        await self._log_stats()

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
try:
    logger.info("DataProcessor ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹œì‘...")
    data_processor = DataProcessor()
    logger.info("DataProcessor ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ")
except Exception as e:
    logger.error(f"DataProcessor ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
    raise

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("DataProcessor main() í•¨ìˆ˜ ì‹œì‘")
    await data_processor.start()

if __name__ == "__main__":
    asyncio.run(main())
