"""
Data Processor Service - ì¤‘ì•™í™”ëœ ë°ì´í„° ì²˜ë¦¬ ì„œë¹„ìŠ¤
Redis Streamê³¼ Queueì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ ê²€ì¦í•˜ê³  PostgreSQL DBì— ì €ì¥
"""
import asyncio
import json
import logging
import time
import datetime
from datetime import datetime, timezone, timedelta
from typing import Dict, Any, Optional, List
from contextlib import asynccontextmanager, contextmanager

import redis.asyncio as redis
from sqlalchemy.orm import Session

from ..core.config import GLOBAL_APP_CONFIGS
from ..models.asset import RealtimeQuote, RealtimeQuoteTimeDelay
from ..models.asset import Asset, OHLCVData, WorldAssetsRanking
from ..crud.asset import crud_ohlcv, crud_asset
from ..utils.logger import logger
from ..utils.redis_queue_manager import RedisQueueManager
from ..utils.helpers import safe_float

logger.info("DataProcessor ëª¨ë“ˆ import ì™„ë£Œ")

class DataProcessor:
    """
    ì¤‘ì•™í™”ëœ ë°ì´í„° ì²˜ë¦¬ ì„œë¹„ìŠ¤
    - Redis Streamì—ì„œ ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬
    - Redis Queueì—ì„œ ë°°ì¹˜ ë°ì´í„° ì²˜ë¦¬
    - ë°ì´í„° ê²€ì¦ ë° ë³€í™˜
    - PostgreSQL DB ì €ì¥
    - ì™¸ë¶€ WebSocket ì˜¤ë¥˜ ëŒ€ì‘ (ë‹¤ì¤‘ ë°±ì—… ì†ŒìŠ¤)
    """
    
    def __init__(self, config_manager=None, redis_queue_manager=None):
        logger.info("DataProcessor ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì¤‘...")
        self.redis_client: Optional[redis.Redis] = None
        self.running = False
        self.config_manager = config_manager # config_manager is now passed from run_data_processor
        self.redis_queue_manager = redis_queue_manager
        
        # ì´ˆê¸° ì„¤ì • ë¡œë“œ
        self._load_initial_configs()
        
        # ì²˜ë¦¬ í†µê³„ (ë¨¼ì € ì´ˆê¸°í™”)
        self.stats = {
            "realtime_processed": 0,
            "batch_processed": 0,
            "errors": 0,
            "last_processed": None
        }

        
        # Redis ì„¤ì •
        self.redis_host = GLOBAL_APP_CONFIGS.get("REDIS_HOST", "redis")
        self.redis_port = GLOBAL_APP_CONFIGS.get("REDIS_PORT", 6379)
        self.redis_db = GLOBAL_APP_CONFIGS.get("REDIS_DB", 0)
        self.redis_password = GLOBAL_APP_CONFIGS.get("REDIS_PASSWORD")
        # Redis ë¹„ë°€ë²ˆí˜¸ê°€ ë¹ˆ ë¬¸ìì—´ì´ë©´ Noneìœ¼ë¡œ ì„¤ì •
        if self.redis_password == "":
            self.redis_password = None
        
        # ì²˜ë¦¬ ì„¤ì • (DB ì„¤ì • ìš°ì„ , ê¸°ë³¸ê°’ fallback)
        self.batch_size = int(GLOBAL_APP_CONFIGS.get("REALTIME_BATCH_SIZE", 1000))
        
        # ì™¸ë¶€ WebSocket ì˜¤ë¥˜ ëŒ€ì‘ ì„¤ì •
        self.backup_sources = [
            "binance_websocket",
            "coinbase_websocket", 
            "kraken_websocket",
            "api_fallback"
        ]
        self.current_source_index = 0
        self.source_failures = {}
        self.max_failures_per_source = 5
        self.fallback_interval = 30  # 30ì´ˆë§ˆë‹¤ ë°±ì—… ì†ŒìŠ¤ ì‹œë„ (í˜„ì¬ ë¯¸ì‚¬ìš©)
        self.processing_interval = float(GLOBAL_APP_CONFIGS.get("REALTIME_PROCESSING_INTERVAL_SECONDS", 0.1)) # 1ì´ˆ -> 0.1ì´ˆ (100ms)
        self.time_window_minutes = int(GLOBAL_APP_CONFIGS.get("WEBSOCKET_TIME_WINDOW_MINUTES", 15))
        self.stream_block_ms = int(GLOBAL_APP_CONFIGS.get("REALTIME_STREAM_BLOCK_MS", 50)) # 100ms -> 50ms
        
        # í˜„ì¬ ì„¤ì •ê°’ ë¡œê·¸ ì¶œë ¥
        logger.info(f"âš™ï¸ DataProcessor ì„¤ì • ë¡œë“œ ì™„ë£Œ - ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°„ê²©: {self.processing_interval}ì´ˆ, ì‹œê°„ ìœˆë„ìš°: {self.time_window_minutes}ë¶„, ìŠ¤íŠ¸ë¦¼ ë¸”ë¡: {self.stream_block_ms}ms")
        
        # Redis Queue Manager (for batch queue + DLQ)
        self.queue_manager = self.redis_queue_manager
        
        # ê°€ê²© ë²”ìœ„ ê²€ì¦ ì„¤ì •
        self.price_ranges = self._initialize_price_ranges()
        # ìš°ì„  ìˆœìœ„: DB(ConfigManager) > GLOBAL_APP_CONFIGS
        self.max_retries = (config_manager.get_retry_attempts() if config_manager else GLOBAL_APP_CONFIGS.get("BATCH_PROCESSING_RETRY_ATTEMPTS", 5))
        try:
            self.max_retries = int(self.max_retries)
        except Exception:
            self.max_retries = 5
        self.retry_delay = 5  # ì´ˆ
        
        # ìŠ¤íŠ¸ë¦¼ ë° í ì„¤ì • (ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ í™œì„±í™”)
        self.realtime_streams = {
            "finnhub:realtime": "finnhub_processor_group",
            "alpaca:realtime": "alpaca_processor_group",
            "binance:realtime": "binance_processor_group",
            "coinbase:realtime": "coinbase_processor_group",
            "twelvedata:realtime": "twelvedata_processor_group",
            "swissquote:realtime": "swissquote_processor_group",
            # "tiingo:realtime": "tiingo_processor_group",  # ë¹„í™œì„±í™”
        }
        
        # ì•”í˜¸í™”í ì†ŒìŠ¤ ìš°ì„ ìˆœìœ„ ì •ì˜ (1=ìµœê³  ìš°ì„ ìˆœìœ„)
        self.crypto_source_priority = {
            'binance': 1,    # ë°”ì´ë‚¸ìŠ¤: ê±°ë˜ëŸ‰ 1ìœ„, ë°ì´í„° ê°±ì‹  ë¹ ë¦„
            'coinbase': 2,   # ì½”ì¸ë² ì´ìŠ¤: ê±°ë˜ëŸ‰ 2ìœ„, ì•ˆì •ì 
        }
        
        # ì†ŒìŠ¤ë³„ ë§ˆì§€ë§‰ ë°ì´í„° ìˆ˜ì‹  ì‹œê°„ ì¶”ì 
        self.source_last_seen = {}
        self.source_health_timeout = 30  # 30ì´ˆ ì´ìƒ ë°ì´í„° ì—†ìœ¼ë©´ ë¹„í™œì„±ìœ¼ë¡œ ê°„ì£¼ (5ì´ˆ â†’ 30ì´ˆë¡œ ì¦ê°€)
        self.batch_queue = "batch_data_queue"
        logger.info("DataProcessor ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ")
    
    def _load_initial_configs(self):
        """ì´ˆê¸° ì„¤ì •ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            # GLOBAL_APP_CONFIGS ë¡œë“œ
            from ..core.config import load_and_set_global_configs
            load_and_set_global_configs()
            logger.info("âœ… ì´ˆê¸° ì„¤ì • ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ì´ˆê¸° ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ê°’ìœ¼ë¡œ ê³„ì† ì§„í–‰
    
    def get_current_source(self):
        """í˜„ì¬ í™œì„± ë°ì´í„° ì†ŒìŠ¤ ë°˜í™˜"""
        return self.backup_sources[self.current_source_index]
    
    def mark_source_failure(self, source):
        """ë°ì´í„° ì†ŒìŠ¤ ì‹¤íŒ¨ ê¸°ë¡"""
        if source not in self.source_failures:
            self.source_failures[source] = 0
        self.source_failures[source] += 1
        logger.warning(f"ğŸš¨ ë°ì´í„° ì†ŒìŠ¤ ì‹¤íŒ¨: {source} (ì‹¤íŒ¨ íšŸìˆ˜: {self.source_failures[source]})")
        
        # ìµœëŒ€ ì‹¤íŒ¨ íšŸìˆ˜ ì´ˆê³¼ ì‹œ ë‹¤ìŒ ì†ŒìŠ¤ë¡œ ì „í™˜
        if self.source_failures[source] >= self.max_failures_per_source:
            self.switch_to_next_source()
    
    def mark_source_success(self, source):
        """ë°ì´í„° ì†ŒìŠ¤ ì„±ê³µ ê¸°ë¡"""
        if source in self.source_failures:
            self.source_failures[source] = 0
        logger.info(f"âœ… ë°ì´í„° ì†ŒìŠ¤ ë³µêµ¬: {source}")
    
    def switch_to_next_source(self):
        """ë‹¤ìŒ ë°±ì—… ì†ŒìŠ¤ë¡œ ì „í™˜"""
        old_source = self.get_current_source()
        self.current_source_index = (self.current_source_index + 1) % len(self.backup_sources)
        new_source = self.get_current_source()
        logger.warning(f"ğŸ”„ ë°ì´í„° ì†ŒìŠ¤ ì „í™˜: {old_source} â†’ {new_source}")
        
        logger.info(f"ğŸ”„ ë°ì´í„° ì†ŒìŠ¤ ì „í™˜: {old_source} â†’ {new_source}")
    
    async def get_backup_data(self, symbols):
        """ë°±ì—… ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        current_source = self.get_current_source()
        
        try:
            if current_source == "api_fallback":
                # API í´ë°±: REST APIë¡œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                return await self._fetch_from_api_fallback(symbols)
            else:
                # API í´ë°± ì†ŒìŠ¤ë“¤
                return await self._fetch_from_api_fallback(symbols)
                
        except Exception as e:
            logger.error(f"âŒ ë°±ì—… ë°ì´í„° ì†ŒìŠ¤ ì‹¤íŒ¨ ({current_source}): {e}")
            self.mark_source_failure(current_source)
            return None
    
    async def _fetch_from_api_fallback(self, symbols):
        """API í´ë°±ìœ¼ë¡œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        import aiohttp
        
        results = {}
        async with aiohttp.ClientSession() as session:
            for symbol in symbols:
                try:
                    # Binance API í´ë°±
                    url = f"https://api.binance.com/api/v3/ticker/24hr?symbol={symbol}"
                    async with session.get(url, timeout=5) as response:
                        if response.status == 200:
                            data = await response.json()
                            results[symbol] = {
                                'price': float(data['lastPrice']),
                                'change_amount': float(data['priceChange']),
                                'change_percent': float(data['priceChangePercent']),
                                'timestamp_utc': datetime.now(timezone.utc).isoformat(),
                                'data_source': 'binance_api_fallback'
                            }
                except Exception as e:
                    logger.warning(f"API í´ë°± ì‹¤íŒ¨ ({symbol}): {e}")
        
        if results:
            self.mark_source_success("api_fallback")
        return results
    

    def _update_source_health(self, source_name: str):
        """ì†ŒìŠ¤ë³„ ë§ˆì§€ë§‰ ë°ì´í„° ìˆ˜ì‹  ì‹œê°„ ì—…ë°ì´íŠ¸"""
        self.source_last_seen[source_name] = time.time()
        logger.info(f"ğŸ“Š {source_name} ì†ŒìŠ¤ í—¬ìŠ¤ ì—…ë°ì´íŠ¸: {datetime.now()}")
    
    def _get_active_crypto_source(self) -> str:
        """í˜„ì¬ í™œì„±í™”ëœ ì•”í˜¸í™”í ì†ŒìŠ¤ ê²°ì • (í˜ì¼ì˜¤ë²„ ë¡œì§)"""
        current_time = time.time()
        
        logger.debug(f"ğŸ” í˜ì¼ì˜¤ë²„ ë¡œì§ ì‹œì‘ - í˜„ì¬ ì‹œê°„: {current_time}")
        logger.debug(f"ğŸ“Š ì†ŒìŠ¤ í—¬ìŠ¤ ìƒíƒœ: {self.source_last_seen}")
        
        # ìš°ì„ ìˆœìœ„ ìˆœìœ¼ë¡œ ì†ŒìŠ¤ ìƒíƒœ í™•ì¸
        for source_name, priority in sorted(self.crypto_source_priority.items(), key=lambda x: x[1]):
            last_seen = self.source_last_seen.get(source_name, 0)
            time_since_last_seen = current_time - last_seen
            
            logger.debug(f"ğŸ” {source_name} ì²´í¬ - ë§ˆì§€ë§‰ ìˆ˜ì‹ : {last_seen}, ê²½ê³¼ ì‹œê°„: {time_since_last_seen:.1f}ì´ˆ")
            
            # 30ì´ˆ ì´ë‚´ì— ë°ì´í„°ë¥¼ ë°›ì•˜ë‹¤ë©´ í™œì„± ìƒíƒœ
            if time_since_last_seen <= self.source_health_timeout:
                logger.info(f"âœ… {source_name} í™œì„± ì†ŒìŠ¤ë¡œ ì„ íƒ (ìš°ì„ ìˆœìœ„: {priority}, ë§ˆì§€ë§‰ ìˆ˜ì‹ : {time_since_last_seen:.1f}ì´ˆ ì „)")
                return source_name
            else:
                logger.debug(f"âš ï¸ {source_name} ë¹„í™œì„± ìƒíƒœ (ë§ˆì§€ë§‰ ìˆ˜ì‹ : {time_since_last_seen:.1f}ì´ˆ ì „, ì„ê³„ê°’: {self.source_health_timeout}ì´ˆ)")
        
        # ëª¨ë“  ì†ŒìŠ¤ê°€ ë¹„í™œì„±ì¸ ê²½ìš°, ìš°ì„ ìˆœìœ„ê°€ ê°€ì¥ ë†’ì€ ì†ŒìŠ¤ ë°˜í™˜
        fallback_source = min(self.crypto_source_priority.items(), key=lambda x: x[1])[0]
        logger.warning(f"ğŸš¨ ëª¨ë“  ì•”í˜¸í™”í ì†ŒìŠ¤ ë¹„í™œì„±, {fallback_source}ë¡œ í˜ì¼ì˜¤ë²„")
        return fallback_source

    async def _connect_redis(self) -> bool:
        """Redis ì—°ê²° ì´ˆê¸°í™”"""
        try:
            if self.redis_client:
                await self.redis_client.ping()
                return True
                
            # Redis ì—°ê²°ì„ ë¹„ë°€ë²ˆí˜¸ ì—†ì´ ì‹œë„
            redis_url = f"redis://{self.redis_host}:{self.redis_port}/{self.redis_db}"
            
            self.redis_client = await redis.from_url(redis_url)
            await self.redis_client.ping()
            logger.info(f"Redis ì—°ê²° ì„±ê³µ: {self.redis_host}:{self.redis_port}")
            return True
            
        except Exception as e:
            logger.error(f"Redis ì—°ê²° ì‹¤íŒ¨: {e}")
            return False

    def _initialize_price_ranges(self) -> Dict[str, tuple]:
        """ìì‚°ë³„ ê°€ê²© ë²”ìœ„ ì´ˆê¸°í™” (ìµœì†Œê°’, ìµœëŒ€ê°’)"""
        return {
            # ETF
            'QQQ': (400, 800),      # QQQ: 400-800ë‹¬ëŸ¬
            'SPY': (300, 700),      # SPY: 300-700ë‹¬ëŸ¬
            'IWM': (150, 300),      # IWM: 150-300ë‹¬ëŸ¬
            'VTI': (200, 300),      # VTI: 200-300ë‹¬ëŸ¬
            
            # Tech Stocks
            'AAPL': (100, 300),     # AAPL: 100-300ë‹¬ëŸ¬
            'MSFT': (200, 500),     # MSFT: 200-500ë‹¬ëŸ¬
            'GOOGL': (100, 200),    # GOOGL: 100-200ë‹¬ëŸ¬
            'AMZN': (100, 200),     # AMZN: 100-200ë‹¬ëŸ¬
            'META': (200, 500),     # META: 200-500ë‹¬ëŸ¬
            'NVDA': (100, 1000),    # NVDA: 100-1000ë‹¬ëŸ¬
            'TSLA': (100, 500),     # TSLA: 100-500ë‹¬ëŸ¬
            'NFLX': (300, 800),     # NFLX: 300-800ë‹¬ëŸ¬
            
            # Financial
            'JPM': (100, 200),      # JPM: 100-200ë‹¬ëŸ¬
            'BAC': (20, 50),        # BAC: 20-50ë‹¬ëŸ¬
            'WFC': (30, 80),        # WFC: 30-80ë‹¬ëŸ¬
            
            # Healthcare
            'JNJ': (140, 200),      # JNJ: 140-200ë‹¬ëŸ¬
            'PFE': (20, 60),        # PFE: 20-60ë‹¬ëŸ¬
            'UNH': (400, 600),      # UNH: 400-600ë‹¬ëŸ¬
            
            # Energy
            'XOM': (80, 150),       # XOM: 80-150ë‹¬ëŸ¬
            'CVX': (100, 200),      # CVX: 100-200ë‹¬ëŸ¬
            
            # Consumer
            'WMT': (120, 200),      # WMT: 120-200ë‹¬ëŸ¬
            'PG': (130, 180),       # PG: 130-180ë‹¬ëŸ¬
            'KO': (50, 80),         # KO: 50-80ë‹¬ëŸ¬
            
            # Crypto (USD ê¸°ì¤€)
            'BTC': (20000, 100000), # BTC: 20K-100Kë‹¬ëŸ¬
            'ETH': (1000, 10000),   # ETH: 1K-10Kë‹¬ëŸ¬
            
            # Commodities
            'GOLD': (1800, 2500),   # GOLD: 1800-2500ë‹¬ëŸ¬
            'SILVER': (20, 50),     # SILVER: 20-50ë‹¬ëŸ¬
        }
    
    def _get_asset_ticker(self, asset_id: int) -> Optional[str]:
        """ìì‚° IDë¡œ í‹°ì»¤ ì¡°íšŒ"""
        try:
            # PostgreSQLì—ì„œ ì§ì ‘ ì¡°íšŒ
            from ..core.database import get_postgres_db
            pg_db = next(get_postgres_db())
            try:
                asset = pg_db.query(Asset).filter(Asset.asset_id == asset_id).first()
                return asset.ticker if asset else None
            finally:
                pg_db.close()
        except Exception as e:
            logger.warning(f"ìì‚° í‹°ì»¤ ì¡°íšŒ ì‹¤íŒ¨ asset_id={asset_id}: {e}")
            return None
    
    def _validate_price_range(self, asset_id: int, price: float, ticker: str = None) -> bool:
        """ìì‚°ë³„ ê°€ê²© ë²”ìœ„ ê²€ì¦"""
        try:
            # í‹°ì»¤ê°€ ì—†ìœ¼ë©´ ì¡°íšŒ
            if not ticker:
                ticker = self._get_asset_ticker(asset_id)
                if not ticker:
                    logger.warning(f"ğŸš¨ ìì‚° ì •ë³´ ì—†ìŒ: asset_id={asset_id}")
                    return False
            
            # ê°€ê²© ë²”ìœ„ í™•ì¸
            if ticker in self.price_ranges:
                min_price, max_price = self.price_ranges[ticker]
                if price < min_price or price > max_price:
                    logger.warning(f"ğŸš¨ ê°€ê²© ë²”ìœ„ ì´ˆê³¼: {ticker}={price:.2f}, "
                                  f"ì •ìƒë²”ìœ„={min_price}-{max_price}")
                    return False
                else:
                    logger.debug(f"âœ… ê°€ê²© ë²”ìœ„ ê²€ì¦ í†µê³¼: {ticker}={price:.2f}")
            else:
                # ì •ì˜ë˜ì§€ ì•Šì€ ìì‚°ì€ ê¸°ë³¸ ê²€ì¦ (ì–‘ìˆ˜)
                if price <= 0:
                    logger.warning(f"ğŸš¨ ê°€ê²©ì´ 0 ì´í•˜: {ticker}={price}")
                    return False
                logger.debug(f"âœ… ê¸°ë³¸ ê°€ê²© ê²€ì¦ í†µê³¼: {ticker}={price:.2f}")
            
            return True
            
        except Exception as e:
            logger.error(f"ê°€ê²© ë²”ìœ„ ê²€ì¦ ì‹¤íŒ¨ asset_id={asset_id}, price={price}: {e}")
            return False
    
    def _validate_realtime_quote(self, record_data: Dict[str, Any]) -> bool:
        """ì‹¤ì‹œê°„ ì¸ìš© ë°ì´í„° ì¢…í•© ê²€ì¦"""
        try:
            asset_id = record_data.get('asset_id')
            price = record_data.get('price')
            data_source = record_data.get('data_source', 'unknown')
            
            # ê¸°ë³¸ ë°ì´í„° ê²€ì¦
            if not asset_id or price is None:
                logger.warning(f"ğŸš¨ í•„ìˆ˜ ë°ì´í„° ëˆ„ë½: asset_id={asset_id}, price={price}")
                return False
            
            # ê°€ê²© ë²”ìœ„ ê²€ì¦
            if not self._validate_price_range(asset_id, price):
                return False
            
            logger.debug(f"âœ… ì‹¤ì‹œê°„ ì¸ìš© ê²€ì¦ í†µê³¼: asset_id={asset_id}, price={price:.2f}, source={data_source}")
            return True
            
        except Exception as e:
            logger.error(f"ì‹¤ì‹œê°„ ì¸ìš© ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False

    def _get_time_window(self, timestamp: datetime, interval_minutes: int = None) -> datetime:
        """ì§€ì •ëœ ë¶„ ë‹¨ìœ„ë¡œ ì‹œê°„ ìœˆë„ìš° ê³„ì‚° (ì„¤ì •ê°’ ë˜ëŠ” ê¸°ë³¸ 15ë¶„ ë‹¨ìœ„ë¡œ ë°˜ì˜¬ë¦¼)"""
        try:
            if interval_minutes is None:
                interval_minutes = self.time_window_minutes
            
            # ë¶„ ë‹¨ìœ„ë¡œ ë°˜ì˜¬ë¦¼ (12:07 -> 12:00, 12:22 -> 12:15)
            minute = (timestamp.minute // interval_minutes) * interval_minutes
            return timestamp.replace(minute=minute, second=0, microsecond=0)
        except Exception as e:
            logger.warning(f"ì‹œê°„ ìœˆë„ìš° ê³„ì‚° ì‹¤íŒ¨: {e}")
            return timestamp

    def _parse_timestamp(self, timestamp_str: str, provider: str = None) -> datetime:
        """íƒ€ì„ìŠ¤íƒ¬í”„ ë¬¸ìì—´ì„ íŒŒì‹±í•˜ê³  UTCë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        try:
            # ë¨¼ì € í‘œì¤€ ISO í˜•ì‹ìœ¼ë¡œ ì‹œë„
            parsed_time = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
            # UTCë¡œ ë³€í™˜
            if parsed_time.tzinfo is not None:
                return parsed_time.astimezone(timezone.utc).replace(tzinfo=None)
            return parsed_time
        except ValueError:
            try:
                # Unix timestamp (milliseconds) í˜•íƒœì¸ì§€ í™•ì¸
                if timestamp_str.isdigit() and len(timestamp_str) >= 10:
                    # ë°€ë¦¬ì´ˆ ë‹¨ìœ„ Unix timestampë¥¼ ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜
                    timestamp_ms = int(timestamp_str)
                    if len(timestamp_str) > 10:  # ë°€ë¦¬ì´ˆê°€ í¬í•¨ëœ ê²½ìš°
                        timestamp_seconds = timestamp_ms / 1000.0
                    else:  # ì´ˆ ë‹¨ìœ„ì¸ ê²½ìš°
                        timestamp_seconds = timestamp_ms
                    # Unix timestampëŠ” ì´ë¯¸ UTC ê¸°ì¤€ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    return datetime.fromtimestamp(timestamp_seconds)
                
                # ë§ˆì´í¬ë¡œì´ˆê°€ 6ìë¦¬ë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš° ì²˜ë¦¬
                if '.' in timestamp_str and len(timestamp_str.split('.')[1]) > 6:
                    # ë§ˆì´í¬ë¡œì´ˆë¥¼ 6ìë¦¬ë¡œ ìë¥´ê¸°
                    parts = timestamp_str.split('.')
                    if len(parts) == 2:
                        base_time = parts[0]
                        microseconds = parts[1][:6]  # 6ìë¦¬ë¡œ ìë¥´ê¸°
                        timezone_part = ''
                        if '-' in microseconds or '+' in microseconds:
                            # íƒ€ì„ì¡´ ì •ë³´ê°€ ë§ˆì´í¬ë¡œì´ˆì— í¬í•¨ëœ ê²½ìš°
                            for i, char in enumerate(microseconds):
                                if char in ['-', '+']:
                                    microseconds = microseconds[:i]
                                    timezone_part = parts[1][i:]
                                    break
                        timestamp_str = f"{base_time}.{microseconds}{timezone_part}"
                        parsed_time = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
                        # UTCë¡œ ë³€í™˜
                        if parsed_time.tzinfo is not None:
                            return parsed_time.astimezone(timezone.utc).replace(tzinfo=None)
                        return parsed_time
            except (ValueError, OSError):
                pass
            
            # ëª¨ë“  íŒŒì‹±ì´ ì‹¤íŒ¨í•˜ë©´ í˜„ì¬ UTC ì‹œê°„ ë°˜í™˜
            logger.warning(f"íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì‹± ì‹¤íŒ¨: {timestamp_str}, í˜„ì¬ UTC ì‹œê°„ ì‚¬ìš©")
            return datetime.utcnow()

    def _determine_actual_interval(self, current_ts: datetime, items: List[Dict], current_index: int) -> Optional[str]:
        """timestamp_utcë¥¼ ë¶„ì„í•´ì„œ ì‹¤ì œ ì£¼ê¸°ë¥¼ íŒë‹¨í•©ë‹ˆë‹¤."""
        try:
            if not isinstance(current_ts, datetime):
                return None
            
            # ì¼ë´‰ ë°ì´í„°ì˜ ê²½ìš° ê¸°ë³¸ì ìœ¼ë¡œ 1d ë°˜í™˜
            # ì£¼ë´‰/ì›”ë´‰ì€ ë³„ë„ì˜ ë¡œì§ìœ¼ë¡œ íŒë‹¨í•´ì•¼ í•¨
            
            # í˜„ì¬ ë°ì´í„°ê°€ ì›”ë§ì¸ì§€ í™•ì¸ (ì›”ì˜ ë§ˆì§€ë§‰ ë‚ )
            from calendar import monthrange
            year, month = current_ts.year, current_ts.month
            last_day_of_month = monthrange(year, month)[1]
            is_month_end = current_ts.day == last_day_of_month
            
            # í˜„ì¬ ë°ì´í„°ê°€ ì£¼ë§ì¸ì§€ í™•ì¸ (ê¸ˆìš”ì¼)
            is_friday = current_ts.weekday() == 4  # 4 = ê¸ˆìš”ì¼
            
            # ì£¼ë´‰/ì›”ë´‰ íŒë‹¨ì€ ë” ì •êµí•œ ë¡œì§ì´ í•„ìš”
            # í˜„ì¬ëŠ” ì¼ë´‰ ë°ì´í„°ë§Œ ì²˜ë¦¬í•˜ë¯€ë¡œ ê¸°ë³¸ì ìœ¼ë¡œ 1d ë°˜í™˜
            # TODO: í–¥í›„ ì£¼ë´‰/ì›”ë´‰ ë°ì´í„° ìˆ˜ì§‘ ì‹œ ë³„ë„ ë¡œì§ êµ¬í˜„
            
            # ì›”ë§ì´ë©´ì„œ ê¸ˆìš”ì¼ì¸ ê²½ìš°ì—ë§Œ ì›”ë´‰ìœ¼ë¡œ íŒë‹¨
            if is_month_end and is_friday:
                return "1m"  # ì›”ë§ ê¸ˆìš”ì¼ì´ë©´ ì›”ë´‰
            # ê¸ˆìš”ì¼ì´ì§€ë§Œ ì›”ë§ì´ ì•„ë‹Œ ê²½ìš°ëŠ” ì¼ë´‰ìœ¼ë¡œ ì²˜ë¦¬
            elif is_friday:
                return "1d"  # ê¸ˆìš”ì¼ì´ì§€ë§Œ ì›”ë§ì´ ì•„ë‹ˆë©´ ì¼ë´‰
            else:
                return "1d"  # ê¸°ë³¸ ì¼ë´‰
                
        except Exception as e:
            logger.warning(f"ì£¼ê¸° íŒë‹¨ ì‹¤íŒ¨: {e}")
            return "1d"  # ì‹¤íŒ¨ ì‹œì—ë„ ê¸°ë³¸ê°’ìœ¼ë¡œ 1d ë°˜í™˜

    @asynccontextmanager
    async def get_db_session(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
        from ..core.database import get_postgres_db
        db = next(get_postgres_db())
        try:
            yield db
        except Exception as e:
            db.rollback()
            raise
        finally:
            db.close()


    async def _process_realtime_streams(self) -> int:
        """ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ë°ì´í„° ì²˜ë¦¬ - Consumer Group ì‚¬ìš©"""
        logger.debug("ğŸš€ _process_realtime_streams ì‹œì‘")

        
        
        # WebSocket ì „ì†¡ í…ŒìŠ¤íŠ¸ (ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì‹œì‘ ì‹œ)
        logger.info("ğŸ§ª WebSocket ì „ì†¡ í…ŒìŠ¤íŠ¸ - ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì‹œì‘")
        
        if not self.redis_client:
            logger.info("Redis client not available for realtime streams")
            return 0
            
        processed_count = 0
        logger.info(f"Processing realtime streams: {list(self.realtime_streams.keys())}")
        
        try:
            logger.debug("âœ… try ë¸”ë¡ ì§„ì…")
            logger.debug("ğŸ”§ Consumer Group ìƒì„± ì‹œì‘")
            # Consumer Group ìƒì„± ë° TTL ì„¤ì • (ê° ìŠ¤íŠ¸ë¦¼ë³„)
            for stream_name in self.realtime_streams.keys():
                try:
                    group_name = self.realtime_streams[stream_name]
                    logger.debug(f"ğŸ”§ Consumer Group ìƒì„± ì‹œë„: {stream_name} -> {group_name}")
                    await self.redis_client.xgroup_create(
                        name=stream_name, 
                        groupname=group_name, 
                        id="0", 
                        mkstream=True
                    )
                    logger.info(f"âœ… Created consumer group {group_name} on {stream_name}")
                    
                    # ìƒì‚°ì ì¸¡ì—ì„œ MAXLENì„ ì ìš©í•˜ë¯€ë¡œ ì†Œë¹„ì ì¸¡ íŠ¸ë¦¼ì€ ì œê±°
                    
                except Exception as e:
                    if "BUSYGROUP" not in str(e):
                        logger.warning(f"âš ï¸ xgroup_create skip {stream_name}: {e}")
                    else:
                        logger.debug(f"â„¹ï¸ Consumer group {group_name} already exists on {stream_name}")
                        # ìƒì‚°ì ì¸¡ì—ì„œ MAXLENì„ ì ìš©í•˜ë¯€ë¡œ ì†Œë¹„ì ì¸¡ íŠ¸ë¦¼ì€ ì œê±°
            
            logger.debug("ğŸ“– Consumer Groupìœ¼ë¡œ ë°ì´í„° ì½ê¸° ì‹œì‘")
            # Consumer Groupìœ¼ë¡œ ë°ì´í„° ì½ê¸° (ê° ìŠ¤íŠ¸ë¦¼ë³„ë¡œ ê°œë³„ ì²˜ë¦¬)
            all_stream_data = []
            for stream_name in self.realtime_streams.keys():
                group_name = self.realtime_streams[stream_name]
                try:
                    logger.debug(f"ğŸ“– ìŠ¤íŠ¸ë¦¼ {stream_name} ì½ê¸° ì‹œë„ (group: {group_name})")
                    
                    # ìŠ¤íŠ¸ë¦¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                    stream_exists = await self.redis_client.exists(stream_name)
                    if not stream_exists:
                        logger.debug(f"ğŸ“­ ìŠ¤íŠ¸ë¦¼ {stream_name}ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ, ê±´ë„ˆëœ€")
                        continue
                    
                    stream_data = await self.redis_client.xreadgroup(
                        groupname=group_name,
                        consumername="data_processor_worker",
                        streams={stream_name: ">"},
                        count=self.batch_size,
                        block=self.stream_block_ms  # ì„¤ì • ê°€ëŠ¥í•œ ë¸”ë¡ ì‹œê°„
                    )
                    logger.info(f"ğŸ“– ìŠ¤íŠ¸ë¦¼ {stream_name} ì½ê¸° ê²°ê³¼: {len(stream_data) if stream_data else 0}ê°œ ë©”ì‹œì§€")
                    if stream_data:
                        all_stream_data.extend(stream_data)
                except Exception as e:
                    import traceback
                    logger.error(f"âŒ ìŠ¤íŠ¸ë¦¼ {stream_name} ì½ê¸° ì‹¤íŒ¨: {e}")
                    logger.error(f"ğŸ” ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
                    # ìŠ¤íŠ¸ë¦¼ë³„ ì˜¤ë¥˜ë¥¼ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  ê³„ì† ì§„í–‰
                    continue
            
            if not all_stream_data:
                return 0
                
            records_to_save = []
            ack_items = []
            
            # ìì‚° ì •ë³´ ìºì‹œ (ì„±ëŠ¥ ìµœì í™”)
            from ..core.database import get_postgres_db
            pg_db = next(get_postgres_db())
            try:
                assets = pg_db.query(Asset.ticker, Asset.asset_id).all()
                ticker_to_asset_id = {ticker: asset_id for ticker, asset_id in assets}
            finally:
                pg_db.close()
            
            # í˜„ì¬ í™œì„±í™”ëœ ì•”í˜¸í™”í ì†ŒìŠ¤ ê²°ì •
            active_crypto_source = self._get_active_crypto_source()
            logger.info(f"ğŸ¯ í˜„ì¬ í™œì„± ì•”í˜¸í™”í ì†ŒìŠ¤: {active_crypto_source}")

            # ë©”ì‹œì§€ ì²˜ë¦¬ (ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë³€ê²½)
            for stream_name_bytes, messages in all_stream_data:
                stream_name_str = stream_name_bytes.decode('utf-8') if isinstance(stream_name_bytes, bytes) else stream_name_bytes
                stream_name_str = stream_name.decode('utf-8') if isinstance(stream_name, bytes) else stream_name
                group_name = self.realtime_streams[stream_name_str]
                source_name = stream_name_str.split(':')[0]  # 'binance:realtime' -> 'binance'
                
                logger.info(f"ğŸ“¥ ìŠ¤íŠ¸ë¦¼ {stream_name_str}ì—ì„œ {len(messages)}ê°œ ë©”ì‹œì§€ ì²˜ë¦¬ ì‹œì‘")
                
                # ì‹¤ì œ ë©”ì‹œì§€ê°€ ìˆì„ ë•Œë§Œ ë§ˆì§€ë§‰ ìˆ˜ì‹  ì‹œê°„ ì—…ë°ì´íŠ¸
                if messages:
                    self._update_source_health(source_name)
                    logger.info(f"ğŸ“Š {source_name} ì†ŒìŠ¤ í—¬ìŠ¤ ì—…ë°ì´íŠ¸: {self.source_last_seen.get(source_name)}")
                else:
                    logger.info(f"ğŸ“­ {source_name} ì†ŒìŠ¤ì— ë©”ì‹œì§€ ì—†ìŒ, í—¬ìŠ¤ ì—…ë°ì´íŠ¸ ì•ˆí•¨")
                
                # ì•”í˜¸í™”í ìŠ¤íŠ¸ë¦¼ì¸ ê²½ìš°, í™œì„± ì†ŒìŠ¤ì˜ ë°ì´í„°ë§Œ ì²˜ë¦¬
                if source_name in self.crypto_source_priority:
                    if source_name != active_crypto_source:
                        # í™œì„± ì†ŒìŠ¤ê°€ ì•„ë‹ˆë©´ ë©”ì‹œì§€ëŠ” ì†Œë¹„í•˜ë˜, ë°ì´í„° ì²˜ë¦¬ëŠ” ê±´ë„ˆëœ€ (ì¤‘ë³µ ë°©ì§€)
                        logger.info(f"â­ï¸ {source_name} ë¹„í™œì„± ì†ŒìŠ¤, ë©”ì‹œì§€ ì†Œë¹„ë§Œ ìˆ˜í–‰ (í™œì„±: {active_crypto_source})")
                        if messages:
                            # ë©”ì‹œì§€ ACKë§Œ ìˆ˜í–‰í•˜ê³  ë°ì´í„° ì²˜ë¦¬ëŠ” ê±´ë„ˆëœ€
                            for message_id, _ in messages:
                                ack_items.append((stream_name_str, group_name, message_id))
                        continue  # ë‹¤ìŒ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ë„˜ì–´ê°
                    else:
                        logger.info(f"âœ… {source_name} í™œì„± ì†ŒìŠ¤, ë°ì´í„° ì²˜ë¦¬ ì§„í–‰")
                
                for message_id, message_data in messages:
                    try: # ê°œë³„ ë©”ì‹œì§€ ì²˜ë¦¬ ë£¨í”„
                        logger.debug(f"ğŸ” ë©”ì‹œì§€ {message_id} ì²˜ë¦¬ ì‹œì‘")
                        logger.debug(f"ğŸ“‹ ì›ë³¸ ë©”ì‹œì§€ ë°ì´í„°: {message_data}")
                        
                        # Redis ìŠ¤íŠ¸ë¦¼ ë°ì´í„° íŒŒì‹± (í”„ë¡œë°”ì´ë”ë³„ í˜•ì‹ ì²˜ë¦¬)
                        symbol = None
                        price = None
                        volume = None
                        raw_timestamp = None
                        provider = None
                        
                        # í‘œì¤€ í•„ë“œ ìŠ¤í‚¤ë§ˆ (ëª¨ë“  í”„ë¡œë°”ì´ë” ë™ì¼)
                        symbol = message_data.get(b'symbol', b'').decode('utf-8').upper()
                        price = safe_float(message_data.get(b'price', b'').decode('utf-8'))
                        volume = safe_float(message_data.get(b'volume', b'').decode('utf-8'))
                        raw_timestamp = message_data.get(b'raw_timestamp', b'').decode('utf-8')
                        provider = message_data.get(b'provider', b'unknown').decode('utf-8')
                        
                        logger.debug(f"ğŸ“Š íŒŒì‹±ëœ ë°ì´í„° - symbol: {symbol}, price: {price}, volume: {volume}, provider: {provider}")
                        
                        # ì´ì „ í˜•ì‹ í˜¸í™˜ì„± (data í•„ë“œê°€ ìˆëŠ” ê²½ìš°)
                        if not symbol and b'data' in message_data:
                            logger.debug("ğŸ”„ ì´ì „ í˜•ì‹ ë°ì´í„° ê°ì§€, JSON íŒŒì‹± ì‹œë„")
                            try:
                                data_json = json.loads(message_data[b'data'].decode('utf-8'))
                                symbol = data_json.get('symbol', '').upper()
                                price = safe_float(data_json.get('price'))
                                volume = safe_float(data_json.get('volume'))
                                raw_timestamp = str(data_json.get('raw_timestamp', ''))
                                provider = message_data.get(b'provider', b'finnhub').decode('utf-8')
                                logger.debug(f"âœ… JSON íŒŒì‹± ì„±ê³µ - symbol: {symbol}, price: {price}")
                            except (json.JSONDecodeError, KeyError) as e:
                                logger.warning(f"âŒ Legacy data JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                                continue
                        
                        if not symbol or price is None:
                            logger.warning(f"âš ï¸ í•„ìˆ˜ ë°ì´í„° ëˆ„ë½ - symbol: {symbol}, price: {price}")
                            continue
                            
                        # ì‹¬ë³¼ ì •ê·œí™” (BINANCE:BTCUSDT -> BTCUSDT)
                        original_symbol = symbol
                        if ':' in symbol:
                            symbol = symbol.split(':')[-1]
                            logger.debug(f"ğŸ”„ ì‹¬ë³¼ ì •ê·œí™”: {original_symbol} -> {symbol}")
                        
                        # Coinbase providerì˜ ê²½ìš° ì‹¬ë³¼ í˜•ì‹ ë³€í™˜ (ETH-USD -> ETHUSDT, DOGE-USD -> DOGEUSDT)
                        if provider == 'coinbase':
                            # ì¼ë°˜ ê·œì¹™: XXX-USD -> XXXUSDT, ì˜ˆì™¸ëŠ” ê°œë³„ ë§¤í•‘
                            if symbol.endswith('-USD') and len(symbol) > 4:
                                base = symbol[:-4]
                                symbol = f"{base}USDT"
                                logger.debug(f"ğŸ”„ Coinbase ì‹¬ë³¼ ì¼ë°˜ ë³€í™˜: {original_symbol} -> {symbol}")
                            # í•„ìš”ì‹œ ì˜ˆì™¸ ë§¤í•‘ ì¶”ê°€
                            coinbase_overrides = {
                                'WBTC-USD': 'WBTCUSDT',
                                'PAXG-USD': 'PAXGUSDT'
                            }
                            if original_symbol in coinbase_overrides:
                                symbol = coinbase_overrides[original_symbol]
                                logger.debug(f"ğŸ”„ Coinbase ì‹¬ë³¼ ì˜ˆì™¸ ë³€í™˜: {original_symbol} -> {symbol}")
                        
                        # ê³µí†µ ë³´ì •: ë² ì´ìŠ¤ ì‹¬ë³¼ì„ USDT í˜ì–´ë¡œ ë³´ì • (ì˜ˆ: BTC -> BTCUSDT)
                        # - ëŒ€ìƒ: binance, coinbase
                        # - ì¡°ê±´: ì´ë¯¸ USDT ì ‘ë¯¸ì‚¬ê°€ ì•„ë‹ˆê³ , ì½”ì¸ë² ì´ìŠ¤ì˜ -USD ê·œì¹™ì´ ì ìš©ë˜ì§€ ì•Šì€ ê²½ìš°
                        if provider in ('binance', 'coinbase'):
                            if not symbol.endswith('USDT') and '-' not in symbol:
                                candidate_usdt = f"{symbol}USDT"
                                if candidate_usdt in ticker_to_asset_id:
                                    logger.debug(f"ğŸ”„ ë² ì´ìŠ¤â†’USDT ë³´ì •: {symbol} -> {candidate_usdt}")
                                    symbol = candidate_usdt
                        
                        # Swissquote providerì˜ ê²½ìš° ì‹¬ë³¼ ì—­ì •ê·œí™” (XAU/USD -> GCUSD, XAG/USD -> SIUSD)
                        if provider == 'swissquote':
                            swissquote_mapping = {
                                'XAU/USD': 'GCUSD',
                                'XAG/USD': 'SIUSD'
                            }
                            if symbol in swissquote_mapping:
                                symbol = swissquote_mapping[symbol]
                                logger.debug(f"ğŸ”„ Swissquote ì‹¬ë³¼ ì—­ì •ê·œí™”: {original_symbol} -> {symbol}")
                            
                        asset_id = ticker_to_asset_id.get(symbol)
                        if not asset_id:
                            # Fallbacks for crypto symbols: try base/USDT/-USD forms
                            if provider in ('binance', 'coinbase'):
                                fallback_candidates = []
                                if symbol.endswith('USDT'):
                                    base = symbol[:-4]
                                    fallback_candidates = [base, f"{base}-USD"]
                                elif symbol.endswith('-USD'):
                                    base = symbol[:-4]
                                    fallback_candidates = [f"{base}USDT", base]
                                else:
                                    base = symbol
                                    fallback_candidates = [f"{base}USDT", f"{base}-USD", base]

                                for cand in fallback_candidates:
                                    if cand in ticker_to_asset_id:
                                        logger.debug(f"ğŸ”„ Fallback symbol mapping: {symbol} -> {cand}")
                                        symbol = cand
                                        asset_id = ticker_to_asset_id[cand]
                                        break

                        if not asset_id:
                            logger.warning(f"âŒ ìì‚° ë§¤ì¹­ ì‹¤íŒ¨ - symbol: {symbol} (ì‚¬ìš© ê°€ëŠ¥í•œ ìì‚°: {list(ticker_to_asset_id.keys())[:10]}...)")
                            continue
                            
                        logger.debug(f"âœ… ìì‚° ë§¤ì¹­ ì„±ê³µ - symbol: {symbol} -> asset_id: {asset_id}")
                        
                        # íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì‹± (UTCë¡œ ë³€í™˜)
                        timestamp_utc = self._parse_timestamp(raw_timestamp, provider) if raw_timestamp else datetime.utcnow()
                        logger.debug(f"â° íƒ€ì„ìŠ¤íƒ¬í”„: {timestamp_utc}")
                        

                        # --- ì¦‰ì‹œ ë°œí–‰ ë¡œì§ (Streaming) ---
                        websocket_data = {
                            "asset_id": asset_id,
                            "ticker": symbol,
                            "timestamp_utc": timestamp_utc,
                            "price": price,
                            "volume": volume,
                            "data_source": provider[:32]  # 32ì ì œí•œ
                        }
                        
                        # DB ì €ì¥ìš© ë°ì´í„° ì¤€ë¹„ (ticker í•„ë“œ ì œì™¸)
                        db_quote_data = websocket_data.copy()
                        del db_quote_data['ticker']
                        
                        # DB ì €ì¥ ëª©ë¡ì— ì¶”ê°€
                        records_to_save.append(db_quote_data)
                        ack_items.append((stream_name_str, group_name, message_id))
                        logger.debug(f"âœ… ë©”ì‹œì§€ {message_id} ì²˜ë¦¬ ì™„ë£Œ ë° ì €ì¥ ëŒ€ê¸°")

                    except Exception as e:
                        import traceback
                        logger.error(f"âŒ ìŠ¤íŠ¸ë¦¼ ë©”ì‹œì§€ {message_id} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        logger.error(f"ğŸ” ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
                        self.stats["errors"] += 1

            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            if records_to_save:
                logger.info(f"ğŸ’¾ {len(records_to_save)}ê°œ ë ˆì½”ë“œë¥¼ DBì— ì €ì¥ ì‹œì‘")
                save_success = await self._bulk_save_realtime_quotes(records_to_save)
                if save_success:
                    processed_count = len(records_to_save)
                    logger.info(f"âœ… DB ì €ì¥ ì„±ê³µ: {processed_count}ê°œ ë ˆì½”ë“œ")
                else:
                    logger.error("âŒ DB ì €ì¥ ì‹¤íŒ¨")
            else:
                logger.info("ğŸ“­ ì €ì¥í•  ë ˆì½”ë“œê°€ ì—†ìŒ")
            
            # ëª¨ë“  ë©”ì‹œì§€ ACK (í™œì„±/ë¹„í™œì„± ì†ŒìŠ¤ êµ¬ë¶„ ì—†ì´)
            if ack_items:
                ack_count = 0
                for stream_name, group_name, message_id in ack_items:
                    try:
                        await self.redis_client.xack(stream_name, group_name, message_id)
                        ack_count += 1
                    except Exception as e:
                        logger.warning(f"âŒ ACK ì‹¤íŒ¨ {stream_name}:{message_id}: {e}")
                logger.info(f"âœ… ACK ì™„ë£Œ: {ack_count}/{len(ack_items)}ê°œ ë©”ì‹œì§€")
            
            # ì£¼ê¸°ì  Stream TTL ì •ë¦¬ëŠ” ìƒì‚°ì MAXLEN ì ìš©ìœ¼ë¡œ ë¶ˆí•„ìš”
                    
        except Exception as e:
            import traceback
            # ì˜ˆì™¸ ë©”ì‹œì§€ê°€ ìŠ¤íŠ¸ë¦¼ ì´ë¦„ì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
            error_msg = str(e)
            if error_msg.startswith("b'") and error_msg.endswith("'"):
                logger.error(f"ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì¤‘ ìŠ¤íŠ¸ë¦¼ ì´ë¦„ ì˜¤ë¥˜: {error_msg}")
                logger.error("ì´ëŠ” ìŠ¤íŠ¸ë¦¼ ë°ì´í„° í˜•ì‹ ì˜¤ë¥˜ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.")
            else:
                logger.error(f"ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {error_msg}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            self.stats["errors"] += 1
            
        return processed_count

    async def _process_batch_queue(self) -> int:
        """ë°°ì¹˜ í ë°ì´í„° ì²˜ë¦¬"""
        if not self.redis_client:
            return 0
            
        processed_count = 0
        
        try:
            # íì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ìµœëŒ€ 100ê°œì”©)
            for _ in range(100):
                # Prefer RedisQueueManager pop; fallback to direct BLPOP
                task_wrapper = None
                if self.queue_manager:
                    task_wrapper = await self.queue_manager.pop_batch_task(timeout_seconds=1)
                else:
                    result = await self.redis_client.blpop(self.batch_queue, timeout=0.5)
                    if result:
                        _, task_data = result
                        try:
                            task_wrapper = json.loads(task_data)
                        except json.JSONDecodeError:
                            task_wrapper = None

                if not task_wrapper:
                    break
                
                # íƒœìŠ¤í¬ ì •ë³´ ë¡œê¹…
                task_type = task_wrapper.get("task_type", "unknown")
                logger.info(f"ë°°ì¹˜ íì—ì„œ íƒœìŠ¤í¬ ìˆ˜ì‹ : {task_type}")
                    
                # Retry loop per task
                attempts = 0
                while attempts <= self.max_retries:
                    attempts += 1
                    try:
                        success = await self._process_batch_task(task_wrapper)
                        if success:
                            logger.info(f"Task {task_wrapper.get('type')} processed successfully.")
                            processed_count += 1
                            break  # ì„±ê³µ ì‹œ ë£¨í”„ ì¢…ë£Œ
                        else:
                            # ì²˜ë¦¬ ë¡œì§ì—ì„œ Falseë¥¼ ë°˜í™˜í•œ ê²½ìš° (ì¼ì‹œì  ì˜¤ë¥˜ì¼ ìˆ˜ ìˆìŒ)
                            raise RuntimeError(f"Task processing for {task_wrapper.get('type')} returned False.")
                    except Exception as e:
                        logger.warning(f"Attempt {attempts}/{self.max_retries} failed for task {task_wrapper.get('type')}: {e}")
                        if attempts > self.max_retries:
                            # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ ì‹œ DLQë¡œ ì´ë™
                            try:
                                raw_json = json.dumps(task_wrapper, ensure_ascii=False)
                            except Exception:
                                raw_json = str(task_wrapper)
                            if self.queue_manager:
                                await self.queue_manager.move_to_dlq(raw_json, str(e))
                            logger.error(f"Task failed after max retries, moving to DLQ: {e}")
                            self.stats["errors"] += 1
                            break
                        # ì¬ì‹œë„ ì „ ì ì‹œ ëŒ€ê¸°
                        await asyncio.sleep(self.retry_delay)
                    
        except Exception as e:
            logger.error(f"ë°°ì¹˜ í ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            self.stats["errors"] += 1
            
        return processed_count

    async def _process_batch_task(self, task: Dict[str, Any]) -> bool:
        """ë°°ì¹˜ íƒœìŠ¤í¬ ì²˜ë¦¬"""
        try:
            task_type = task.get("type")
            payload = task.get("payload")
            
            if not task_type or not payload:
                return False
                
            # í‘œì¤€ í˜ì´ë¡œë“œ: {"items": [...]} ìš°ì„  ì‚¬ìš©, ì•„ë‹ˆë©´ ê¸°ì¡´ payloadë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë˜í•‘
            items = payload.get("items") if isinstance(payload, dict) else None
            if items is None:
                items = payload if isinstance(payload, list) else [payload]

            # íƒœìŠ¤í¬ íƒ€ì…ë³„ ì²˜ë¦¬ ë¡œì§ (ì‹ /êµ¬ í‚¤ ëª¨ë‘ ì§€ì›)
            if task_type == "stock_profile":
                return await self._save_stock_profile(items)
            elif task_type == "stock_financials":
                return await self._save_stock_financials(items)
            elif task_type == "stock_estimate":
                return await self._save_stock_estimate(items)
            elif task_type == "etf_info":
                return await self._save_etf_info(items)
            elif task_type in ("crypto_info", "crypto_data"):
                return await self._save_crypto_data(items)
            elif task_type in ("ohlcv_data", "ohlcv_day_data", "ohlcv_intraday_data"):
                # metadata ì •ë³´ ì¶”ì¶œ
                metadata = payload.get("metadata", {}) if isinstance(payload, dict) else {}
                logger.info(f"Processing {task_type} task: items_count={len(items)}, metadata={metadata}")
                return await self._save_ohlcv_data(items, metadata)
            elif task_type == "index_data":
                return await self._save_index_data(items)
            elif task_type == "technical_indicators":
                return await self._save_technical_indicators(items)
            elif task_type == "onchain_metric":
                return await self._save_onchain_metric(items)
            elif task_type == "world_assets_ranking":
                # metadata ì •ë³´ ì¶”ì¶œ
                metadata = payload.get("metadata", {}) if isinstance(payload, dict) else {}
                data_source = metadata.get('data_source', 'unknown')
                logger.info(f"ë°°ì¹˜ íƒœìŠ¤í¬ ì²˜ë¦¬ ì‹œì‘: world_assets_ranking, data_source: {data_source}, items: {len(items)}ê°œ")
                return await self._save_world_assets_ranking(items, metadata)
            elif task_type == "asset_settings_update":
                return await self._update_asset_settings(payload)
            else:
                logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” íƒœìŠ¤í¬ íƒ€ì…: {task_type}")
                return False
                
        except Exception as e:
            logger.error(f"ë°°ì¹˜ íƒœìŠ¤í¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return False


    async def _bulk_save_realtime_quotes(self, records: List[Dict[str, Any]]) -> bool:
        """ì‹¤ì‹œê°„ ì¸ìš© ë°ì´í„° ì¼ê´„ ì €ì¥ - PostgreSQL"""
        try:
            logger.info(f"ğŸ’¾ RealtimeQuote ì €ì¥ ì‹œì‘: {len(records)}ê°œ ë ˆì½”ë“œ")
            
            # PostgreSQL ì„¸ì…˜ ìƒì„±
            from ..core.database import get_postgres_db
            logger.debug("ğŸ”— PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ìƒì„± ì¤‘...")
            postgres_db = next(get_postgres_db())
            logger.debug("âœ… PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ìƒì„± ì™„ë£Œ")
            
            try:
                # ë°ì´í„° ê²€ì¦ ë° í•„í„°ë§
                validated_records = []
                validation_failed_count = 0
                
                for i, record_data in enumerate(records):
                    if self._validate_realtime_quote(record_data):
                        validated_records.append(record_data)
                    else:
                        validation_failed_count += 1
                        logger.warning(f"ğŸš¨ ê²€ì¦ ì‹¤íŒ¨ë¡œ ì œì™¸: asset_id={record_data.get('asset_id')}, "
                                      f"price={record_data.get('price')}, source={record_data.get('data_source')}")
                
                logger.info(f"âœ… ê²€ì¦ ì™„ë£Œ: {len(validated_records)}/{len(records)}ê°œ ë ˆì½”ë“œ í†µê³¼ "
                           f"(ì‹¤íŒ¨: {validation_failed_count}ê°œ)")
                
                if not validated_records:
                    logger.warning("ğŸš¨ ê²€ì¦ì„ í†µê³¼í•œ ë ˆì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
                    return False
                
                # ----- ë²Œí¬ UPSERTë¡œ ë¦¬íŒ©í„°ë§ -----
                from sqlalchemy.dialects.postgresql import insert
                from sqlalchemy import func
                # í™˜ê²½ì„¤ì •: ë°°ì¹˜ í¬ê¸°
                import os, time as _time
                BULK_UPSERT_ENABLED = os.getenv("BULK_UPSERT_ENABLED", "true").lower() == "true"
                BATCH_SIZE = int(os.getenv("BULK_BATCH_SIZE", "1000"))

                success_count = 0
                if not BULK_UPSERT_ENABLED:
                    logger.info("â„¹ï¸ BULK_UPSERT_ENABLED=false: ê¸°ì¡´ ë¡œì§ ìœ ì§€ê°€ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                    BATCH_SIZE = 1

                # ì§€ì—° í…Œì´ë¸” ëª¨ë¸ ì„í¬íŠ¸
                from ..models.asset import RealtimeQuoteTimeDelay as PGRealtimeQuoteTimeDelay

                # ìœ í‹¸: ìˆ«ìê°’ ì •ê·œí™”(ì˜¤ë²„í”Œë¡œ ë°©ì§€)
                def _sanitize_number(val, min_abs=0.0, max_abs=1e9, digits=8):
                    try:
                        if val is None:
                            return None
                        f = float(val)
                        if not (f == f) or f == float('inf') or f == float('-inf'):
                            return None
                        if abs(f) < min_abs:
                            f = 0.0
                        if abs(f) > max_abs:
                            return None
                        return round(f, digits)
                    except Exception:
                        return None

                # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¶„í•  ì²˜ë¦¬
                for start_idx in range(0, len(validated_records), BATCH_SIZE):
                    batch = validated_records[start_idx:start_idx + BATCH_SIZE]
                    if not batch:
                        continue

                    # ì‹¤ì‹œê°„ í…Œì´ë¸”ìš© ë°ì´í„°(ì¤‘ë³µ asset_idëŠ” ë§ˆì§€ë§‰ ë ˆì½”ë“œë¡œ ë®ì–´ì“°ê¸°)
                    dedup_rt = {}
                    for rec in batch:
                        r = rec.copy()
                        r['price'] = _sanitize_number(r.get('price'))
                        r['volume'] = _sanitize_number(r.get('volume'))
                        r['change_amount'] = _sanitize_number(r.get('change_amount'))
                        r['change_percent'] = _sanitize_number(r.get('change_percent'))
                        if r['price'] is None:
                            continue
                        dedup_rt[r['asset_id']] = r
                    realtime_rows = list(dedup_rt.values())

                    # ì§€ì—° í…Œì´ë¸”ìš© ë°ì´í„° (timestamp ìœˆë„ìš° ì ìš© + ì¤‘ë³µ í‚¤ ì œê±°)
                    delay_dedup = {}
                    for rec in batch:
                        d = rec.copy()
                        tw = self._get_time_window(rec['timestamp_utc'])
                        d['timestamp_utc'] = tw
                        d['data_interval'] = f"{self.time_window_minutes}m"
                        d['price'] = _sanitize_number(d.get('price'))
                        d['volume'] = _sanitize_number(d.get('volume'))
                        d['change_amount'] = _sanitize_number(d.get('change_amount'))
                        d['change_percent'] = _sanitize_number(d.get('change_percent'))
                        if d['price'] is None:
                            continue
                        key = (d['asset_id'], d['timestamp_utc'], d['data_source'])
                        delay_dedup[key] = d
                    delay_rows = list(delay_dedup.values())

                    start_ts = _time.time()
                    try:
                        # ì‹¤ì‹œê°„ í…Œì´ë¸” ë²Œí¬ UPSERT
                        realtime_stmt = insert(RealtimeQuote).values(realtime_rows)
                        realtime_stmt = realtime_stmt.on_conflict_do_update(
                            index_elements=['asset_id'],
                            set_={
                                'timestamp_utc': realtime_stmt.excluded.timestamp_utc,
                                'price': realtime_stmt.excluded.price,
                                'volume': realtime_stmt.excluded.volume,
                                'change_amount': realtime_stmt.excluded.change_amount,
                                'change_percent': realtime_stmt.excluded.change_percent,
                                'data_source': realtime_stmt.excluded.data_source,
                                'updated_at': func.now()
                            }
                        )
                        postgres_db.execute(realtime_stmt)

                        # ì§€ì—° í…Œì´ë¸” ë²Œí¬ UPSERT
                        delay_stmt = insert(PGRealtimeQuoteTimeDelay).values(delay_rows)
                        delay_stmt = delay_stmt.on_conflict_do_update(
                            index_elements=['asset_id', 'timestamp_utc', 'data_source'],
                            set_={
                                'price': delay_stmt.excluded.price,
                                'volume': delay_stmt.excluded.volume,
                                'change_amount': delay_stmt.excluded.change_amount,
                                'change_percent': delay_stmt.excluded.change_percent,
                                'updated_at': func.now()
                            }
                        )
                        postgres_db.execute(delay_stmt)

                        postgres_db.commit()
                        batch_dur = _time.time() - start_ts
                        success_count += len(batch)
                        logger.info(f"âœ… Bulk upsert ì™„ë£Œ size={len(batch)} took={batch_dur:.3f}s rps={len(batch)/batch_dur if batch_dur>0 else float('inf'):.1f}")
                    except Exception as e:
                        import traceback
                        logger.error(f"âŒ Bulk upsert ì‹¤íŒ¨(size={len(batch)}): {e}")
                        logger.error(f"ğŸ” ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
                        postgres_db.rollback()
                        # ë°°ì¹˜ ì‹¤íŒ¨ ì‹œ ê°œë³„ ì¬ì‹œë„(ê°„ë‹¨í•œ í´ë°±)
                        for i, record_data in enumerate(batch):
                            try:
                                single_stmt = insert(RealtimeQuote).values(**record_data)
                                single_stmt = single_stmt.on_conflict_do_update(
                                    index_elements=['asset_id'],
                                    set_={
                                        'timestamp_utc': single_stmt.excluded.timestamp_utc,
                                        'price': single_stmt.excluded.price,
                                        'volume': single_stmt.excluded.volume,
                                        'change_amount': single_stmt.excluded.change_amount,
                                        'change_percent': single_stmt.excluded.change_percent,
                                        'data_source': single_stmt.excluded.data_source,
                                        'updated_at': func.now()
                                    }
                                )
                                postgres_db.execute(single_stmt)

                                tw = self._get_time_window(record_data['timestamp_utc'])
                                delay_data = record_data.copy()
                                delay_data['timestamp_utc'] = tw
                                delay_data['data_interval'] = f"{self.time_window_minutes}m"
                                single_delay = insert(PGRealtimeQuoteTimeDelay).values(**delay_data)
                                single_delay = single_delay.on_conflict_do_update(
                                    index_elements=['asset_id', 'timestamp_utc', 'data_source'],
                                    set_={
                                        'price': single_delay.excluded.price,
                                        'volume': single_delay.excluded.volume,
                                        'change_amount': single_delay.excluded.change_amount,
                                        'change_percent': single_delay.excluded.change_percent,
                                        'updated_at': func.now()
                                    }
                                )
                                postgres_db.execute(single_delay)
                                postgres_db.commit()
                                success_count += 1
                            except Exception as se:
                                logger.error(f"âŒ í´ë°± ë‹¨ê±´ ì €ì¥ ì‹¤íŒ¨: {se}")
                                postgres_db.rollback()
                        
            finally:
                logger.debug("ğŸ”— PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ì¢…ë£Œ ì¤‘...")
                postgres_db.close()
                logger.debug("âœ… PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ì¢…ë£Œ ì™„ë£Œ")
                        
            logger.info(f"âœ… RealtimeQuote PostgreSQL ì €ì¥ ì™„ë£Œ: {success_count}/{len(records)}ê°œ ì„±ê³µ")
            return success_count > 0
        except Exception as e:
            import traceback
            logger.error(f"âŒ RealtimeQuote PostgreSQL ì €ì¥ ì‹¤íŒ¨: {e}")
            logger.error(f"ğŸ” ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            return False

    async def _save_stock_profile(self, items: List[Dict[str, Any]]) -> bool:
        """ì£¼ì‹ í”„ë¡œí•„ ë°ì´í„° ì €ì¥ (PostgreSQLë§Œ ì‚¬ìš©)"""
        try:
            if not items:
                return True

            logger.info(f"ì£¼ì‹ í”„ë¡œí•„ ë°ì´í„° ì €ì¥: {len(items)}ê°œ ë ˆì½”ë“œ")

            # PostgreSQL ì €ì¥
            from ..core.database import get_postgres_db
            pg_db = next(get_postgres_db())
            try:
                from ..models.asset import StockProfile as PGStockProfile
                from sqlalchemy.dialects.postgresql import insert as pg_insert
                from sqlalchemy import func
                
                for item in items:
                    try:
                        asset_id = item.get("asset_id") or item.get("assetId") or item.get("asset_id".lower())
                        data = item.get("data") if "data" in item else item
                        if not asset_id or not isinstance(data, dict):
                            continue

                        # ë§¤í•‘: CompanyProfileData -> StockProfile ì»¬ëŸ¼
                        company_name = data.get("name") or data.get("company_name")
                        # Descriptions (prefer explicit bilingual fields if provided)
                        description_en = data.get("description_en") or data.get("description")
                        description_ko = data.get("description_ko")
                        sector = data.get("sector")
                        industry = data.get("industry")
                        website = data.get("website")
                        employees_count = data.get("employees") or data.get("fullTimeEmployees")
                        country = data.get("country")
                        address = data.get("address")
                        city = data.get("city")
                        state = data.get("state")  # ì£¼/ë„
                        zip_code = data.get("zip_code") or data.get("zip")  # ìš°í¸ë²ˆí˜¸
                        ceo = data.get("ceo") or data.get("CEO")
                        phone = data.get("phone")
                        logo_image_url = data.get("logo_image_url") or data.get("image") or data.get("logo")
                        market_cap = data.get("market_cap") or data.get("marketCap")
                        # ê±°ë˜ì†Œ ë° ì‹ë³„ì ì •ë³´
                        exchange = data.get("exchange")
                        exchange_full_name = data.get("exchange_full_name") or data.get("exchangeFullName")
                        cik = data.get("cik")
                        isin = data.get("isin")
                        cusip = data.get("cusip")
                        # ipo_date íŒŒì‹±
                        ipo_date_val = data.get("ipoDate") or data.get("ipo_date")
                        ipo_date = None
                        if ipo_date_val:
                            try:
                                if isinstance(ipo_date_val, str):
                                    ipo_date = datetime.strptime(ipo_date_val.split("T")[0], "%Y-%m-%d").date()
                            except Exception:
                                ipo_date = None

                        # PostgreSQL UPSERT
                        pg_data = {
                            'asset_id': asset_id,
                            'company_name': company_name or "",
                            'description_en': description_en,
                            'description_ko': description_ko,
                            'sector': sector,
                            'industry': industry,
                            'website': website,
                            'employees_count': employees_count,
                            'country': country,
                            'address': address,
                            'city': city,
                            'state': state,
                            'zip_code': zip_code,
                            'ceo': ceo,
                            'phone': phone,
                            'logo_image_url': logo_image_url,
                            'market_cap': market_cap,
                            'ipo_date': ipo_date,
                            'exchange': exchange,
                            'exchange_full_name': exchange_full_name,
                            'cik': cik,
                            'isin': isin,
                            'cusip': cusip,
                        }
                        
                        # None ê°’ ì œê±°
                        pg_data = {k: v for k, v in pg_data.items() if v is not None}
                        
                        stmt = pg_insert(PGStockProfile).values(**pg_data)
                        stmt = stmt.on_conflict_do_update(
                            index_elements=['asset_id'],
                            set_={
                                'company_name': stmt.excluded.company_name,
                                'description_en': stmt.excluded.description_en,
                                'description_ko': stmt.excluded.description_ko,
                                'sector': stmt.excluded.sector,
                                'industry': stmt.excluded.industry,
                                'website': stmt.excluded.website,
                                'employees_count': stmt.excluded.employees_count,
                                'country': stmt.excluded.country,
                                'address': stmt.excluded.address,
                                'city': stmt.excluded.city,
                                'state': stmt.excluded.state,
                                'zip_code': stmt.excluded.zip_code,
                                'ceo': stmt.excluded.ceo,
                                'phone': stmt.excluded.phone,
                                'logo_image_url': stmt.excluded.logo_image_url,
                                'market_cap': stmt.excluded.market_cap,
                                'ipo_date': stmt.excluded.ipo_date,
                                'exchange': stmt.excluded.exchange,
                                'exchange_full_name': stmt.excluded.exchange_full_name,
                                'cik': stmt.excluded.cik,
                                'isin': stmt.excluded.isin,
                                'cusip': stmt.excluded.cusip,
                                'updated_at': func.now()
                            }
                        )
                        pg_db.execute(stmt)
                        logger.debug(f"[StockProfile] PostgreSQL ì €ì¥ ì™„ë£Œ: asset_id={asset_id}")
                        
                    except Exception as e:
                        logger.warning(f"ê°œë³„ ì£¼ì‹ í”„ë¡œí•„ ì €ì¥ ì‹¤íŒ¨(asset_id={item.get('asset_id')}): {e}")
                        continue
                
                pg_db.commit()
                logger.info(f"[StockProfile] PostgreSQL ì €ì¥ ì™„ë£Œ: {len(items)}ê°œ ë ˆì½”ë“œ")
                
            except Exception as e:
                pg_db.rollback()
                logger.error(f"[StockProfile] PostgreSQL ì €ì¥ ì‹¤íŒ¨: {e}")
                return False
            finally:
                pg_db.close()

            return True
        except Exception as e:
            logger.error(f"ì£¼ì‹ í”„ë¡œí•„ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    async def _save_etf_info(self, items: List[Dict[str, Any]]) -> bool:
        """ETF ì •ë³´ ë°ì´í„° ì €ì¥ (PostgreSQLë§Œ ì‚¬ìš©)"""
        try:
            if not items:
                return True

            logger.info(f"ETF ì •ë³´ ë°ì´í„° ì €ì¥: {len(items)}ê°œ ë ˆì½”ë“œ")

            # PostgreSQL ì €ì¥
            from ..core.database import get_postgres_db
            pg_db = next(get_postgres_db())
            try:
                from ..models.asset import ETFInfo as PGETFInfo
                from sqlalchemy.dialects.postgresql import insert as pg_insert
                from sqlalchemy import func
                from datetime import datetime, date

                for item in items:
                    try:
                        asset_id = item.get("asset_id") or item.get("assetId")
                        data = item.get("data") if isinstance(item, dict) and "data" in item else item
                        if not asset_id or not isinstance(data, dict):
                            continue

                        # snapshot_date íŒŒì‹±
                        snapshot = data.get("snapshot_date") or data.get("snapshotDate")
                        parsed_snapshot = None
                        if snapshot:
                            try:
                                if isinstance(snapshot, str):
                                    s = snapshot.split("T")[0]
                                    parsed_snapshot = datetime.strptime(s, "%Y-%m-%d").date()
                                elif isinstance(snapshot, datetime):
                                    parsed_snapshot = snapshot.date()
                            except Exception:
                                parsed_snapshot = None
                        if parsed_snapshot is None:
                            parsed_snapshot = datetime.utcnow().date()

                        # PostgreSQL UPSERT
                        pg_data = {
                            'asset_id': asset_id,
                            'snapshot_date': parsed_snapshot,
                            'net_assets': data.get("net_assets"),
                            'net_expense_ratio': data.get("net_expense_ratio"),
                            'portfolio_turnover': data.get("portfolio_turnover"),
                            'dividend_yield': data.get("dividend_yield"),
                            'inception_date': data.get("inception_date"),
                            'leveraged': data.get("leveraged"),
                            'sectors': data.get("sectors"),
                            'holdings': data.get("holdings")
                        }
                        
                        # None ê°’ ì œê±°
                        pg_data = {k: v for k, v in pg_data.items() if v is not None}
                        
                        stmt = pg_insert(PGETFInfo).values(**pg_data)
                        stmt = stmt.on_conflict_do_update(
                            index_elements=['asset_id'],
                            set_={
                                'snapshot_date': stmt.excluded.snapshot_date,
                                'net_assets': stmt.excluded.net_assets,
                                'net_expense_ratio': stmt.excluded.net_expense_ratio,
                                'portfolio_turnover': stmt.excluded.portfolio_turnover,
                                'dividend_yield': stmt.excluded.dividend_yield,
                                'inception_date': stmt.excluded.inception_date,
                                'leveraged': stmt.excluded.leveraged,
                                'sectors': stmt.excluded.sectors,
                                'holdings': stmt.excluded.holdings,
                                'updated_at': func.now()
                            }
                        )
                        pg_db.execute(stmt)
                        logger.debug(f"[ETFInfo] PostgreSQL ì €ì¥ ì™„ë£Œ: asset_id={asset_id}")
                        
                    except Exception as e:
                        logger.warning(f"ê°œë³„ ETF ì •ë³´ ì €ì¥ ì‹¤íŒ¨(asset_id={item.get('asset_id')}): {e}")
                        continue

                pg_db.commit()
                logger.info(f"[ETFInfo] PostgreSQL ì €ì¥ ì™„ë£Œ: {len(items)}ê°œ ë ˆì½”ë“œ")
                return True
                
            except Exception as e:
                pg_db.rollback()
                logger.error(f"[ETFInfo] PostgreSQL ì €ì¥ ì‹¤íŒ¨: {e}")
                return False
            finally:
                pg_db.close()
                
        except Exception as e:
            logger.error(f"ETF ì •ë³´ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    async def _save_crypto_data(self, items: List[Dict[str, Any]]) -> bool:
        """í¬ë¦½í†  ë°ì´í„° ì €ì¥ (PostgreSQLë§Œ ì‚¬ìš©)"""
        if not items:
            return True
            
        try:
            logger.info(f"í¬ë¦½í†  ë°ì´í„° ì €ì¥: {len(items)}ê°œ ë ˆì½”ë“œ")
            
            # PostgreSQL ì €ì¥
            from ..core.database import get_postgres_db
            pg_db = next(get_postgres_db())
            try:
                from ..models.asset import CryptoData as PGCryptoData
                from sqlalchemy.dialects.postgresql import insert as pg_insert
                from sqlalchemy import func
                
                saved_count = 0
                for item in items:
                    try:
                        # asset_id ì¶”ì¶œ
                        asset_id = item.get('asset_id')
                        if not asset_id:
                            logger.warning(f"crypto_data ì €ì¥ ì‹¤íŒ¨: asset_id ì—†ìŒ - {item}")
                            continue
                        
                        # CryptoData ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ë°ì´í„° ë³€í™˜
                        crypto_data_dict = {
                            'asset_id': asset_id,
                            'symbol': item.get('symbol', ''),
                            'name': item.get('name', ''),
                            'market_cap': item.get('market_cap'),
                            'circulating_supply': item.get('circulating_supply'),
                            'total_supply': item.get('total_supply'),
                            'max_supply': item.get('max_supply'),
                            'current_price': item.get('price') or item.get('current_price'),
                            'volume_24h': item.get('volume_24h'),
                            'percent_change_1h': item.get('percent_change_1h'),
                            'percent_change_24h': item.get('change_24h') or item.get('percent_change_24h'),
                            'percent_change_7d': item.get('percent_change_7d'),
                            'percent_change_30d': item.get('percent_change_30d'),
                            'cmc_rank': item.get('rank'),
                            'category': item.get('category'),
                            'description': item.get('description'),
                            'logo_url': item.get('logo_url'),
                            'website_url': item.get('website_url'),
                            'price': item.get('price'),
                            'slug': item.get('slug'),
                            'date_added': item.get('date_added'),
                            'platform': item.get('platform'),
                            'explorer': item.get('explorer'),
                            'source_code': item.get('source_code'),
                            'tags': item.get('tags'),
                            'is_active': True
                        }
                        
                        # None ê°’ ì œê±°
                        crypto_data_dict = {k: v for k, v in crypto_data_dict.items() if v is not None}
                        
                        # PostgreSQL UPSERT
                        stmt = pg_insert(PGCryptoData).values(**crypto_data_dict)
                        stmt = stmt.on_conflict_do_update(
                            index_elements=['asset_id'],
                            set_={
                                'symbol': stmt.excluded.symbol,
                                'name': stmt.excluded.name,
                                'market_cap': stmt.excluded.market_cap,
                                'circulating_supply': stmt.excluded.circulating_supply,
                                'total_supply': stmt.excluded.total_supply,
                                'max_supply': stmt.excluded.max_supply,
                                'current_price': stmt.excluded.current_price,
                                'volume_24h': stmt.excluded.volume_24h,
                                'percent_change_1h': stmt.excluded.percent_change_1h,
                                'percent_change_24h': stmt.excluded.percent_change_24h,
                                'percent_change_7d': stmt.excluded.percent_change_7d,
                                'percent_change_30d': stmt.excluded.percent_change_30d,
                                'cmc_rank': stmt.excluded.cmc_rank,
                                'category': stmt.excluded.category,
                                'description': stmt.excluded.description,
                                'logo_url': stmt.excluded.logo_url,
                                'website_url': stmt.excluded.website_url,
                                'price': stmt.excluded.price,
                                'slug': stmt.excluded.slug,
                                'date_added': stmt.excluded.date_added,
                                'platform': stmt.excluded.platform,
                                'explorer': stmt.excluded.explorer,
                                'source_code': stmt.excluded.source_code,
                                'tags': stmt.excluded.tags,
                                'is_active': stmt.excluded.is_active,
                                'last_updated': func.now()
                            }
                        )
                        pg_db.execute(stmt)
                        saved_count += 1
                        logger.debug(f"[CryptoData] PostgreSQL ì €ì¥ ì™„ë£Œ: asset_id={asset_id}, symbol={item.get('symbol')}")
                            
                    except Exception as e:
                        logger.error(f"crypto_data ì €ì¥ ì¤‘ ì˜¤ë¥˜: asset_id={item.get('asset_id')}, error={e}")
                        continue
                
                pg_db.commit()
                logger.info(f"[CryptoData] PostgreSQL ì €ì¥ ì™„ë£Œ: {saved_count}/{len(items)}ê°œ ë ˆì½”ë“œ")
                return saved_count > 0
                
            except Exception as e:
                pg_db.rollback()
                logger.error(f"[CryptoData] PostgreSQL ì €ì¥ ì‹¤íŒ¨: {e}")
                return False
            finally:
                pg_db.close()
            
        except Exception as e:
            logger.error(f"crypto_data ì €ì¥ ì¤‘ ì „ì²´ ì˜¤ë¥˜: {e}")
            return False

    async def _save_ohlcv_data(self, items: List[Dict[str, Any]], metadata: Dict[str, Any] = None) -> bool:
        """OHLCV ë°ì´í„° ì €ì¥ - ì¼ë´‰ê³¼ ì¸íŠ¸ë¼ë°ì´ ë°ì´í„°ë¥¼ ì ì ˆí•œ í…Œì´ë¸”ì— ë¶„ë¦¬ ì €ì¥"""
        if not items:
            return True
        
        # metadataì—ì„œ asset_id, interval, is_backfill ì¶”ì¶œ
        asset_id = metadata.get("asset_id") if metadata else None
        interval = metadata.get("interval") if metadata else None
        is_backfill = metadata.get("is_backfill", False) if metadata else False
        
        if not asset_id or not interval:
            logger.warning(f"OHLCV ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: asset_id={asset_id}, interval={interval} ì •ë³´ ë¶€ì¡±")
            return False
        
        # intervalì— ë”°ë¼ ì €ì¥í•  í…Œì´ë¸” ê²°ì • (ìš”ì²­ ì£¼ê¸° ê¸°ë°˜, ì´í›„ ì‹¤ì œ ì£¼ê¸° ê²€ì¦ë¡œì§ì—ì„œ ì¬ì¡°ì •)
        # 1d, 1w, 1mëŠ” ohlcv_day_data, ë‚˜ë¨¸ì§€ëŠ” ohlcv_intraday_data
        is_daily_request = interval in ["1d", "daily", "1w", "1m"] or interval is None
        table_name = "ohlcv_day_data" if is_daily_request else "ohlcv_intraday_data"
        
        logger.info(f"OHLCV ë°ì´í„° ì €ì¥ ì‹œì‘: asset_id={asset_id}, interval={interval}, table={table_name}, records={len(items)}")
        
        async with self.get_db_session() as db:
            try:
                # 0) ì‚¬ì „ ë°©ì–´: items ë‚´ timestamp_utcë¥¼ ë¨¼ì € í‘œì¤€í™” (UTC naive datetime)
                from datetime import datetime, timezone
                from app.utils.helpers import normalize_timestamp_to_date, normalize_timestamp_to_trading_hour
                
                def _normalize_ts_val(val: Any, is_daily_data: bool = False) -> Any:
                    try:
                        if isinstance(val, datetime):
                            if val.tzinfo is not None and val.tzinfo.utcoffset(val) is not None:
                                val = val.astimezone(timezone.utc).replace(tzinfo=None)
                            
                            # ì¼ë´‰ ë°ì´í„°ì˜ ê²½ìš° ë‚ ì§œë§Œìœ¼ë¡œ ì •ê·œí™” (00:00:00)
                            if is_daily_data:
                                return normalize_timestamp_to_date(val)
                            else:
                                return val.replace(microsecond=0)
                        
                        s = str(val)
                        if not s:
                            return val
                        if s.endswith('Z'):
                            s = s[:-1]
                        s = s.replace('T', ' ')
                        if '+' in s:
                            s = s.split('+')[0]
                        if '.' in s:
                            s = s.split('.', 1)[0]
                        
                        parsed_dt = datetime.strptime(s, "%Y-%m-%d %H:%M:%S")
                        
                        # ì¼ë´‰ ë°ì´í„°ì˜ ê²½ìš° ë‚ ì§œë§Œìœ¼ë¡œ ì •ê·œí™” (00:00:00)
                        if is_daily_data:
                            return normalize_timestamp_to_date(parsed_dt)
                        else:
                            return parsed_dt
                    except Exception:
                        return val

                for it in items:
                    if isinstance(it, dict) and 'timestamp_utc' in it:
                        it['timestamp_utc'] = _normalize_ts_val(it.get('timestamp_utc'), is_daily_data=is_daily_request)

                # DB ì €ì¥ì„ ìœ„í•´ Pydantic ëª¨ë¸ ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                from app.external_apis.base.schemas import OhlcvDataPoint
                ohlcv_list = [OhlcvDataPoint(**item) for item in items]

                # OHLCV ë°ì´í„°ì— asset_idì™€ data_interval ì¶”ê°€
                # PostgreSQL TIMESTAMP ì»¬ëŸ¼ê³¼ í˜¸í™˜ë˜ë„ë¡ timestamp_utcëŠ” "YYYY-MM-DD HH:MM:SS" ë˜ëŠ” naive UTC datetimeìœ¼ë¡œ ì „ë‹¬
                from datetime import datetime, timezone

                ohlcv_data_list = []
                # ì¸íŠ¸ë¼ë°ì´ ìš”ì²­ì‹œ ì‹¤ì œ ì£¼ê¸° ë¶ˆì¼ì¹˜ ê°ì§€ë¥¼ ìœ„í•œ í”Œë˜ê·¸
                intraday_request = not is_daily_request
                wrong_interval_count = 0
                for i, ohlcv_item in enumerate(ohlcv_list):
                    # model_dump(mode='python')ì„ ì‚¬ìš©í•˜ì—¬ datetimeì„ ê·¸ëŒ€ë¡œ ìœ ì§€
                    item_dict = ohlcv_item.model_dump(mode='python')

                    ts = item_dict.get('timestamp_utc')
                    # Pydanticì—ì„œ datetimeìœ¼ë¡œ ìœ ì§€ëœ ê²½ìš°ë§Œ ì²˜ë¦¬
                    if isinstance(ts, datetime):
                        # tz-awareì´ë©´ UTCë¡œ ë³€í™˜ í›„ naiveë¡œ ë§Œë“¤ê¸°
                        if ts.tzinfo is not None and ts.tzinfo.utcoffset(ts) is not None:
                            ts = ts.astimezone(timezone.utc).replace(tzinfo=None)
                        # ì´ˆ ë‹¨ìœ„ë¡œ ë§ì¶”ê¸° (ë§ˆì´í¬ë¡œì´ˆ ì œê±°)
                        ts = ts.replace(microsecond=0)
                        item_dict['timestamp_utc'] = ts
                    else:
                        # í˜¹ì‹œ ë¬¸ìì—´ë¡œ ë“¤ì–´ì˜¨ ê²½ìš° ì•ˆì „í•˜ê²Œ íŒŒì‹±í•´ UTC naiveë¡œ ë³€í™˜
                        try:
                            # ì§€ì› í¬ë§·: 2025-08-05T04:00:00Z, 2025-08-05 04:00:00+00:00, ë“±
                            s = str(ts)
                            if s.endswith('Z'):
                                s = s[:-1]
                            # ê³µë°±/"T" ëª¨ë‘ í—ˆìš©
                            s = s.replace('T', ' ')
                            # íƒ€ì„ì¡´ ì œê±°
                            if '+' in s:
                                s = s.split('+')[0]
                            if '.' in s:
                                base, frac = s.split('.', 1)
                                s = base
                            parsed = datetime.strptime(s, "%Y-%m-%d %H:%M:%S")
                            item_dict['timestamp_utc'] = parsed
                        except Exception:
                            # ë§ˆì§€ë§‰ ìˆ˜ë‹¨: ê·¸ëŒ€ë¡œ ë‘ë˜, ë’¤ì˜ CRUDì—ì„œ ì‹¤íŒ¨í•˜ë©´ ìŠ¤í‚µë  ê²ƒ
                            pass

                    item_dict['asset_id'] = asset_id
                    
                    # ì¼ë´‰ ë°ì´í„°ëŠ” í•­ìƒ '1d'ë¡œ ì„¤ì • (ì£¼ë´‰/ì›”ë´‰ì€ ë³„ë„ ë°°ì¹˜ ì‘ì—…ì—ì„œ ìƒì„±)
                    if is_daily_request:
                        item_dict['data_interval'] = '1d'
                    else:
                        # ì¸íŠ¸ë¼ë°ì´ ë°ì´í„°ì˜ ê²½ìš° ì›ë˜ interval ì‚¬ìš©
                        item_dict['data_interval'] = interval if interval else '1h'

                    ohlcv_data_list.append(item_dict)

                    # ì¸íŠ¸ë¼ë°ì´ ìš”ì²­ì¸ë° ì‹¤ì œê°€ 1d/1w/1më©´ ì¹´ìš´íŠ¸
                    if intraday_request and item_dict['data_interval'] in ["1d", "daily", "1w", "1m"]:
                        wrong_interval_count += 1

                # ì¸íŠ¸ë¼ë°ì´ ìš”ì²­ì— ëŒ€í•œ ê°€ë“œ: ì „ì²´ì˜ ê³¼ë°˜ì´ ì¼ë´‰/ì£¼ë´‰ì´ë©´ ë¦¬ë¼ìš°íŒ…
                if intraday_request and wrong_interval_count > 0:
                    if wrong_interval_count >= max(1, len(ohlcv_data_list) // 2):
                        logger.warning(
                            f"ì¸íŠ¸ë¼ë°ì´ ìš”ì²­(interval={interval})ì´ì§€ë§Œ ì‹¤ì œ ë°ì´í„°ì˜ ê³¼ë°˜({wrong_interval_count}/{len(ohlcv_data_list)})ê°€ ì¼ë´‰/ì£¼ë´‰ì…ë‹ˆë‹¤. ì¼ë´‰ í…Œì´ë¸”ë¡œ ë¦¬ë¼ìš°íŒ…í•©ë‹ˆë‹¤.")
                        table_name = "ohlcv_day_data"
                        is_daily_request = True
                
                # CRUDë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì €ì¥ - PostgreSQL
                from app.crud.asset import crud_ohlcv
                logger.debug("[OHLCV] PostgreSQL upsert ì‹œì‘: count=%s, daily=%s", len(ohlcv_data_list), is_daily_request)
                if is_daily_request:
                    added_count = crud_ohlcv.bulk_upsert_ohlcv_daily(db, ohlcv_data_list)
                else:
                    added_count = crud_ohlcv.bulk_upsert_ohlcv_intraday(db, ohlcv_data_list)
                logger.info(f"[OHLCV] PostgreSQL ì €ì¥ ì™„ë£Œ: asset_id={asset_id}, interval={interval}, table={table_name}, added={added_count}ê°œ")
                
                # PostgreSQL ì´ì¤‘ ì €ì¥
                try:
                    from ..core.database import get_postgres_db
                    pg_db = next(get_postgres_db())
                    try:
                        # ëª¨ë¸ ë° ì¶©ëŒ í‚¤ ê²°ì •
                        if is_daily_request:
                            from ..models.asset import OHLCVData as PGDay
                            model = PGDay
                            conflict_cols = ['asset_id', 'timestamp_utc']
                        else:
                            from ..models.asset import OHLCVIntradayData as PGIntraday
                            model = PGIntraday
                            conflict_cols = ['asset_id', 'timestamp_utc', 'data_interval']

                        from sqlalchemy.dialects.postgresql import insert as pg_insert
                        from sqlalchemy import func

                        # í—ˆìš© ì»¬ëŸ¼(ëª¨ë¸ ìŠ¤í‚¤ë§ˆ) ì§‘í•©
                        allowed_columns = set(model.__table__.columns.keys())

                        upserted = 0
                        for row in ohlcv_data_list:
                            logger.debug("[OHLCV] PostgreSQL UPSERT row: keys=%s", {k: row.get(k) for k in ('asset_id','timestamp_utc','data_interval') if k in row})

                            # ëª¨ë¸ ìŠ¤í‚¤ë§ˆì— ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì‚¬ìš©
                            filtered_row = {k: v for k, v in row.items() if k in allowed_columns}
                            # ëˆ„ë½ ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ë””ë²„ê·¸ ê¸°ë¡
                            if len(filtered_row) != len(row):
                                missing = [k for k in row.keys() if k not in allowed_columns]
                                logger.debug(f"[OHLCV] PostgreSQL ëª¨ë¸ì— ì—†ëŠ” ì»¬ëŸ¼ ì œì™¸: {missing}")

                            stmt = pg_insert(model).values(**filtered_row)
                            # ì—…ë°ì´íŠ¸ ì»¬ëŸ¼: ì¶©ëŒ í‚¤ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€(ëª¨ë¸ì— ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ) + updated_at
                            update_set = {}
                            for k in filtered_row.keys():
                                if k in conflict_cols:
                                    continue
                                try:
                                    update_set[k] = getattr(stmt.excluded, k)
                                except AttributeError:
                                    # ëª¨ë¸ì— ì‹¤ì œë¡œ ì—†ê±°ë‚˜ excluded ì ‘ê·¼ ë¶ˆê°€
                                    logger.debug(f"[OHLCV] PostgreSQL excludedì— ì—†ëŠ” ì»¬ëŸ¼ ìŠ¤í‚µ: {k}")
                            update_set['updated_at'] = func.now()

                            stmt = stmt.on_conflict_do_update(index_elements=conflict_cols, set_=update_set)
                            pg_db.execute(stmt)
                            upserted += 1
                        pg_db.commit()
                        logger.info(f"[OHLCV] PostgreSQL ì €ì¥ ì™„ë£Œ: upserted={upserted} rows, daily={is_daily_request}")
                    except Exception as e:
                        pg_db.rollback()
                        logger.warning(f"[OHLCV] PostgreSQL ì €ì¥ ì‹¤íŒ¨, ë¡¤ë°± ìˆ˜í–‰: {e}")
                        # DLQ ì ì¬ (ì›ë³¸ ëª©ë¡ì„ ì¶•ì•½í•˜ì—¬ ê¸°ë¡)
                        try:
                            if self.queue_manager:
                                payload = {
                                    'type': 'ohlcv_postgresql_failed',
                                    'items': ohlcv_data_list[:50],
                                    'meta': {
                                        'is_daily': is_daily_request,
                                        'interval': interval,
                                        'reason': str(e)
                                    }
                                }
                                import json as _json
                                self.queue_manager.move_to_dlq(_json.dumps(payload, ensure_ascii=False, default=str), str(e))
                                logger.error("[OHLCV] DLQ ì ì¬ ì™„ë£Œ (PostgreSQL ì‹¤íŒ¨)")
                        except Exception as de:
                            logger.error(f"[OHLCV] DLQ ì ì¬ ì‹¤íŒ¨: {de}")
                    finally:
                        pg_db.close()
                except Exception as e:
                    logger.warning(f"[OHLCV] PostgreSQL ì—°ê²°/ê²½ë¡œ ì´ˆê¸°í™” ì‹¤íŒ¨(ë¬´ì‹œ): {e}")

                return True
                
            except Exception as e:
                logger.error(f"OHLCV ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: asset_id={asset_id}, interval={interval}, table={table_name}, error={e}", exc_info=True)
                return False

    async def _save_stock_financials(self, items: List[Dict[str, Any]]) -> bool:
        """ì£¼ì‹ ì¬ë¬´ ë°ì´í„° ì €ì¥ (ìŠ¤ëƒ…ìƒ·, ë³‘í•© ì—…ì„œíŠ¸)

        ê·œì¹™:
        - ë™ì¼í•œ asset_id + snapshot_date ë ˆì½”ë“œê°€ ìˆìœ¼ë©´ ì œê³µëœ ì»¬ëŸ¼ë§Œ ë®ì–´ì”€(None/ì—†ìŒì€ ë¬´ì‹œ)
        - ì—†ë˜ ë ˆì½”ë“œëŠ” ìƒˆë¡œ ìƒì„±
        """
        try:
            if not items:
                return True

            logger.info(f"ì£¼ì‹ ì¬ë¬´ ë°ì´í„° ì €ì¥: {len(items)}ê°œ ë ˆì½”ë“œ")

            # PostgreSQL ì €ì¥
            from ..core.database import get_postgres_db
            pg_db = next(get_postgres_db())
            try:
                from ..models.asset import StockFinancial as PGStockFinancial
                from sqlalchemy.dialects.postgresql import insert as pg_insert
                from sqlalchemy import func
                from datetime import datetime, date

                updatable_fields = {
                    "currency",
                    "market_cap",
                    "ebitda",
                    "shares_outstanding",
                    "pe_ratio",
                    "peg_ratio",
                    "beta",
                    "eps",
                    "dividend_yield",
                    "dividend_per_share",
                    "profit_margin_ttm",
                    "return_on_equity_ttm",
                    "revenue_ttm",
                    "price_to_book_ratio",
                    "week_52_high",
                    "week_52_low",
                    "day_50_moving_avg",
                    "day_200_moving_avg",
                    # ì¶”ê°€ ì¬ë¬´ ì§€í‘œ
                    "book_value",
                    "revenue_per_share_ttm",
                    "operating_margin_ttm",
                    "return_on_assets_ttm",
                    "gross_profit_ttm",
                    "quarterly_earnings_growth_yoy",
                    "quarterly_revenue_growth_yoy",
                    "analyst_target_price",
                    "trailing_pe",
                    "forward_pe",
                    "price_to_sales_ratio_ttm",
                    "ev_to_revenue",
                    "ev_to_ebitda",
                }

                for item in items:
                    try:
                        asset_id = item.get("asset_id") or item.get("assetId")
                        data = item.get("data") if isinstance(item, dict) and "data" in item else item
                        if not asset_id or not isinstance(data, dict):
                            continue

                        # ì˜ë¯¸ ìˆëŠ” ê°’ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ìŠ¤í‚µ (í†µí™” ì œì™¸)
                        meaningful_keys = [
                            "market_cap", "ebitda", "shares_outstanding", "pe_ratio", "peg_ratio",
                            "beta", "eps", "dividend_yield", "dividend_per_share", "profit_margin_ttm",
                            "return_on_equity_ttm", "revenue_ttm", "price_to_book_ratio",
                            "week_52_high", "week_52_low", "day_50_moving_avg", "day_200_moving_avg",
                        ]
                        if not any((data.get(k) is not None) for k in meaningful_keys):
                            # ì €ì¥í•  ì‹¤ì§ˆ ê°’ì´ ì—†ìœ¼ë©´ ê±´ë„ˆëœ€
                            continue

                        # snapshot_date íŒŒì‹±(ê°€ëŠ¥í•˜ë©´ ë‚ ì§œë§Œ ì €ì¥)
                        snapshot = data.get("snapshot_date") or data.get("snapshotDate")
                        parsed_snapshot = None
                        if snapshot:
                            try:
                                if isinstance(snapshot, str):
                                    # YYYY-MM-DD í˜¹ì€ ISO
                                    s = snapshot.split("T")[0]
                                    parsed_snapshot = datetime.strptime(s, "%Y-%m-%d").date()
                                elif isinstance(snapshot, datetime):
                                    parsed_snapshot = snapshot.date()
                            except Exception:
                                parsed_snapshot = None
                        if parsed_snapshot is None:
                            parsed_snapshot = datetime.utcnow().date()

                        # PostgreSQL UPSERT
                        pg_data = {
                            'asset_id': asset_id,
                            'snapshot_date': parsed_snapshot,
                            'currency': data.get('currency'),
                            'market_cap': data.get('market_cap'),
                            'ebitda': data.get('ebitda'),
                            'shares_outstanding': data.get('shares_outstanding'),
                            'pe_ratio': data.get('pe_ratio'),
                            'peg_ratio': data.get('peg_ratio'),
                            'beta': data.get('beta'),
                            'eps': data.get('eps'),
                            'dividend_yield': data.get('dividend_yield'),
                            'dividend_per_share': data.get('dividend_per_share'),
                            'profit_margin_ttm': data.get('profit_margin_ttm'),
                            'return_on_equity_ttm': data.get('return_on_equity_ttm'),
                            'revenue_ttm': data.get('revenue_ttm'),
                            'price_to_book_ratio': data.get('price_to_book_ratio'),
                            'week_52_high': data.get('week_52_high'),
                            'week_52_low': data.get('week_52_low'),
                            'day_50_moving_avg': data.get('day_50_moving_avg'),
                            'day_200_moving_avg': data.get('day_200_moving_avg'),
                            'book_value': data.get('book_value'),
                            'revenue_per_share_ttm': data.get('revenue_per_share_ttm'),
                            'operating_margin_ttm': data.get('operating_margin_ttm'),
                            'return_on_assets_ttm': data.get('return_on_assets_ttm'),
                            'gross_profit_ttm': data.get('gross_profit_ttm'),
                            'quarterly_earnings_growth_yoy': data.get('quarterly_earnings_growth_yoy'),
                            'quarterly_revenue_growth_yoy': data.get('quarterly_revenue_growth_yoy'),
                            'analyst_target_price': data.get('analyst_target_price'),
                            'trailing_pe': data.get('trailing_pe'),
                            'forward_pe': data.get('forward_pe'),
                            'price_to_sales_ratio_ttm': data.get('price_to_sales_ratio_ttm'),
                            'ev_to_revenue': data.get('ev_to_revenue'),
                            'ev_to_ebitda': data.get('ev_to_ebitda'),
                        }
                        
                        # None ê°’ ì œê±°
                        pg_data = {k: v for k, v in pg_data.items() if v is not None}
                        
                        stmt = pg_insert(PGStockFinancial).values(**pg_data)
                        stmt = stmt.on_conflict_do_update(
                            index_elements=['asset_id'],
                            set_={
                                'snapshot_date': stmt.excluded.snapshot_date,
                                'currency': stmt.excluded.currency,
                                'market_cap': stmt.excluded.market_cap,
                                'ebitda': stmt.excluded.ebitda,
                                'shares_outstanding': stmt.excluded.shares_outstanding,
                                'pe_ratio': stmt.excluded.pe_ratio,
                                'peg_ratio': stmt.excluded.peg_ratio,
                                'beta': stmt.excluded.beta,
                                'eps': stmt.excluded.eps,
                                'dividend_yield': stmt.excluded.dividend_yield,
                                'dividend_per_share': stmt.excluded.dividend_per_share,
                                'profit_margin_ttm': stmt.excluded.profit_margin_ttm,
                                'return_on_equity_ttm': stmt.excluded.return_on_equity_ttm,
                                'revenue_ttm': stmt.excluded.revenue_ttm,
                                'price_to_book_ratio': stmt.excluded.price_to_book_ratio,
                                'week_52_high': stmt.excluded.week_52_high,
                                'week_52_low': stmt.excluded.week_52_low,
                                'day_50_moving_avg': stmt.excluded.day_50_moving_avg,
                                'day_200_moving_avg': stmt.excluded.day_200_moving_avg,
                                'book_value': stmt.excluded.book_value,
                                'revenue_per_share_ttm': stmt.excluded.revenue_per_share_ttm,
                                'operating_margin_ttm': stmt.excluded.operating_margin_ttm,
                                'return_on_assets_ttm': stmt.excluded.return_on_assets_ttm,
                                'gross_profit_ttm': stmt.excluded.gross_profit_ttm,
                                'quarterly_earnings_growth_yoy': stmt.excluded.quarterly_earnings_growth_yoy,
                                'quarterly_revenue_growth_yoy': stmt.excluded.quarterly_revenue_growth_yoy,
                                'analyst_target_price': stmt.excluded.analyst_target_price,
                                'trailing_pe': stmt.excluded.trailing_pe,
                                'forward_pe': stmt.excluded.forward_pe,
                                'price_to_sales_ratio_ttm': stmt.excluded.price_to_sales_ratio_ttm,
                                'ev_to_revenue': stmt.excluded.ev_to_revenue,
                                'ev_to_ebitda': stmt.excluded.ev_to_ebitda,
                                'updated_at': func.now()
                            }
                        )
                        pg_db.execute(stmt)
                        logger.debug(f"[StockFinancial] PostgreSQL ì €ì¥ ì™„ë£Œ: asset_id={asset_id}")
                        
                    except Exception as e:
                        logger.warning(f"ê°œë³„ ì£¼ì‹ ì¬ë¬´ ì €ì¥ ì‹¤íŒ¨(asset_id={item.get('asset_id')}): {e}")
                        continue

                pg_db.commit()
                logger.info(f"[StockFinancial] PostgreSQL ì €ì¥ ì™„ë£Œ: {len(items)}ê°œ ë ˆì½”ë“œ")
                return True
                
            except Exception as e:
                pg_db.rollback()
                logger.error(f"[StockFinancial] PostgreSQL ì €ì¥ ì‹¤íŒ¨: {e}")
                return False
            finally:
                pg_db.close()
                
        except Exception as e:
            logger.error(f"ì£¼ì‹ ì¬ë¬´ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    async def _save_stock_estimate(self, items: List[Dict[str, Any]]) -> bool:
        """ì£¼ì‹ ì¶”ì •ì¹˜ ë°ì´í„° ì €ì¥ (ë³‘í•© ì—…ì„œíŠ¸)

        ê·œì¹™:
        - ë™ì¼í•œ asset_id + fiscal_date ë ˆì½”ë“œê°€ ìˆìœ¼ë©´ ì œê³µëœ ì»¬ëŸ¼ë§Œ ë®ì–´ì”€(None/ì—†ìŒì€ ë¬´ì‹œ)
        - ì—†ë˜ ë ˆì½”ë“œëŠ” ìƒˆë¡œ ìƒì„±
        """
        try:
            if not items:
                return True

            logger.info(f"ì£¼ì‹ ì¶”ì •ì¹˜ ë°ì´í„° ì €ì¥: {len(items)}ê°œ ë ˆì½”ë“œ")

            # PostgreSQL ì €ì¥
            from ..core.database import get_postgres_db
            pg_db = next(get_postgres_db())
            try:
                from ..models.asset import StockAnalystEstimate as PGStockAnalystEstimate
                from sqlalchemy.dialects.postgresql import insert as pg_insert
                from sqlalchemy import func
                from datetime import datetime, date

                # DB ì»¬ëŸ¼ ìŠ¤í‚¤ë§ˆ ê¸°ì¤€ì˜ í•„ë“œ ì§‘í•©
                updatable_fields = {
                    "revenue_avg", "revenue_low", "revenue_high",
                    "eps_avg", "eps_low", "eps_high",
                    "revenue_analysts_count", "eps_analysts_count",
                    "ebitda_avg", "ebitda_low", "ebitda_high",
                    "ebit_avg", "ebit_low", "ebit_high",
                    "net_income_avg", "net_income_low", "net_income_high",
                    "sga_expense_avg", "sga_expense_low", "sga_expense_high",
                }

                for item in items:
                    try:
                        asset_id = item.get("asset_id") or item.get("assetId")
                        data = item.get("data") if isinstance(item, dict) and "data" in item else item
                        if not asset_id or not isinstance(data, dict):
                            continue

                        # ë‹¤ì–‘í•œ í‚¤ ì¼€ì´ìŠ¤ í—ˆìš©
                        fiscal_date = (
                            data.get("fiscal_date") or data.get("fiscalDate") or data.get("date")
                        )
                        parsed_date = None
                        if fiscal_date:
                            try:
                                if isinstance(fiscal_date, str):
                                    s = fiscal_date.split("T")[0]
                                    parsed_date = datetime.strptime(s, "%Y-%m-%d").date()
                                elif isinstance(fiscal_date, datetime):
                                    parsed_date = fiscal_date.date()
                            except Exception:
                                parsed_date = None
                        if parsed_date is None:
                            # ë‚ ì§œê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ (ì¶”ì •ì¹˜ëŠ” ë‚ ì§œ ê¸°ì¤€ ë³‘í•© í•„ìš”)
                            continue

                        # PostgreSQL UPSERT
                        pg_data = {
                            'asset_id': asset_id,
                            'fiscal_date': parsed_date,
                            'revenue_avg': data.get('revenue_avg'),
                            'revenue_low': data.get('revenue_low'),
                            'revenue_high': data.get('revenue_high'),
                            'eps_avg': data.get('eps_avg'),
                            'eps_low': data.get('eps_low'),
                            'eps_high': data.get('eps_high'),
                            'revenue_analysts_count': data.get('revenue_analysts_count'),
                            'eps_analysts_count': data.get('eps_analysts_count'),
                            'ebitda_avg': data.get('ebitda_avg'),
                            'ebitda_low': data.get('ebitda_low'),
                            'ebitda_high': data.get('ebitda_high'),
                            'ebit_avg': data.get('ebit_avg'),
                            'ebit_low': data.get('ebit_low'),
                            'ebit_high': data.get('ebit_high'),
                            'net_income_avg': data.get('net_income_avg'),
                            'net_income_low': data.get('net_income_low'),
                            'net_income_high': data.get('net_income_high'),
                            'sga_expense_avg': data.get('sga_expense_avg'),
                            'sga_expense_low': data.get('sga_expense_low'),
                            'sga_expense_high': data.get('sga_expense_high'),
                        }
                        
                        # None ê°’ ì œê±°
                        pg_data = {k: v for k, v in pg_data.items() if v is not None}
                        
                        stmt = pg_insert(PGStockAnalystEstimate).values(**pg_data)
                        stmt = stmt.on_conflict_do_update(
                            index_elements=['asset_id', 'fiscal_date'],
                            set_={
                                'revenue_avg': stmt.excluded.revenue_avg,
                                'revenue_low': stmt.excluded.revenue_low,
                                'revenue_high': stmt.excluded.revenue_high,
                                'eps_avg': stmt.excluded.eps_avg,
                                'eps_low': stmt.excluded.eps_low,
                                'eps_high': stmt.excluded.eps_high,
                                'revenue_analysts_count': stmt.excluded.revenue_analysts_count,
                                'eps_analysts_count': stmt.excluded.eps_analysts_count,
                                'ebitda_avg': stmt.excluded.ebitda_avg,
                                'ebitda_low': stmt.excluded.ebitda_low,
                                'ebitda_high': stmt.excluded.ebitda_high,
                                'ebit_avg': stmt.excluded.ebit_avg,
                                'ebit_low': stmt.excluded.ebit_low,
                                'ebit_high': stmt.excluded.ebit_high,
                                'net_income_avg': stmt.excluded.net_income_avg,
                                'net_income_low': stmt.excluded.net_income_low,
                                'net_income_high': stmt.excluded.net_income_high,
                                'sga_expense_avg': stmt.excluded.sga_expense_avg,
                                'sga_expense_low': stmt.excluded.sga_expense_low,
                                'sga_expense_high': stmt.excluded.sga_expense_high,
                                'updated_at': func.now()
                            }
                        )
                        pg_db.execute(stmt)
                        logger.debug(f"[StockAnalystEstimate] PostgreSQL ì €ì¥ ì™„ë£Œ: asset_id={asset_id}")
                        
                    except Exception as e:
                        logger.warning(f"ê°œë³„ ì£¼ì‹ ì¶”ì •ì¹˜ ì €ì¥ ì‹¤íŒ¨(asset_id={item.get('asset_id')}): {e}")
                        continue

                pg_db.commit()
                logger.info(f"[StockAnalystEstimate] PostgreSQL ì €ì¥ ì™„ë£Œ: {len(items)}ê°œ ë ˆì½”ë“œ")
                return True
                
            except Exception as e:
                pg_db.rollback()
                logger.error(f"[StockAnalystEstimate] PostgreSQL ì €ì¥ ì‹¤íŒ¨: {e}")
                return False
            finally:
                pg_db.close()
                
        except Exception as e:
            logger.error(f"ì£¼ì‹ ì¶”ì •ì¹˜ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    async def _save_index_data(self, items: List[Dict[str, Any]]) -> bool:
        """ì§€ìˆ˜ ë°ì´í„° ì €ì¥ - PostgreSQL"""
        try:
            if not items:
                return True
                
            logger.info(f"ì§€ìˆ˜ ë°ì´í„° ì €ì¥: {len(items)}ê°œ ë ˆì½”ë“œ")
            
            # PostgreSQL ì €ì¥
            async with self.get_db_session() as db:
                # TODO: IndexData ëª¨ë¸ì´ êµ¬í˜„ë˜ë©´ ì‹¤ì œ ì €ì¥ ë¡œì§ ì¶”ê°€
                logger.debug(f"[IndexData] PostgreSQL ì €ì¥ ì¤€ë¹„: {len(items)}ê°œ ë ˆì½”ë“œ")
                db.commit()
            
            return True
        except Exception as e:
            logger.error(f"ì§€ìˆ˜ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    async def _save_world_assets_ranking(self, items: List[Dict[str, Any]], metadata: Dict[str, Any]) -> bool:
        """ì„¸ê³„ ìì‚° ë­í‚¹ ë°ì´í„° ì €ì¥ - PostgreSQL"""
        try:
            if not items:
                logger.warning("ì„¸ê³„ ìì‚° ë­í‚¹ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return True
                
            data_source = metadata.get('data_source', 'unknown')
            collection_date = metadata.get('collection_date', 'unknown')
            logger.info(f"ì„¸ê³„ ìì‚° ë­í‚¹ ë°ì´í„° ì €ì¥ ì‹œì‘: {len(items)}ê°œ ë ˆì½”ë“œ, data_source: {data_source}, collection_date: {collection_date}")
            
            # PostgreSQL ì €ì¥ (UPSERT ë¡œì§)
            async with self.get_db_session() as db:
                saved_count = 0
                failed_count = 0
                for item in items:
                    try:
                        ranking_date = metadata.get('collection_date', datetime.now().date())
                        if isinstance(ranking_date, str):
                            ranking_date = datetime.fromisoformat(ranking_date).date()
                        
                        data_source = metadata.get('data_source', 'unknown')
                        ticker = item.get('ticker')
                        
                        # asset_idê°€ ì—†ìœ¼ë©´ assets í…Œì´ë¸”ì—ì„œ ì°¾ê¸°
                        asset_id = item.get('asset_id')
                        asset_type_id = item.get('asset_type_id')
                        if not asset_id and ticker and asset_type_id:
                            try:
                                from ..models.asset import Asset
                                existing_asset = db.query(Asset).filter(
                                    Asset.ticker == ticker,
                                    Asset.asset_type_id == asset_type_id
                                ).first()
                                if existing_asset:
                                    asset_id = existing_asset.asset_id
                                    logger.debug(f"Found asset_id {asset_id} for ticker {ticker}")
                            except Exception as e:
                                logger.error(f"Error looking up asset_id for {ticker}: {e}")
                        
                        # PostgreSQL UPSERT using INSERT ... ON CONFLICT DO UPDATE
                        try:
                            # INSERT ì‹œë„
                            world_asset = WorldAssetsRanking(
                                rank=item.get('rank'),
                                name=item.get('name'),
                                ticker=item.get('ticker'),
                                market_cap_usd=item.get('market_cap_usd'),
                                price_usd=item.get('price_usd'),
                                daily_change_percent=item.get('daily_change_percent'),
                                country=item.get('country'),
                                asset_type_id=asset_type_id,
                                asset_id=asset_id,
                                ranking_date=ranking_date,
                                data_source=data_source
                            )
                            db.add(world_asset)
                            db.commit()
                            logger.debug(f"[WorldAssetsRanking] ì‚½ì…: {ticker} ({data_source})")
                        except Exception as e:
                            # ì¤‘ë³µ í‚¤ ì—ëŸ¬ì¸ ê²½ìš° UPDATE
                            if "Duplicate entry" in str(e) or "1062" in str(e):
                                db.rollback()
                                existing = db.query(WorldAssetsRanking).filter(
                                    WorldAssetsRanking.ranking_date == ranking_date,
                                    WorldAssetsRanking.ticker == ticker,
                                    WorldAssetsRanking.data_source == data_source
                                ).first()
                                
                                if existing:
                                    existing.rank = item.get('rank')
                                    existing.name = item.get('name')
                                    existing.market_cap_usd = item.get('market_cap_usd')
                                    existing.price_usd = item.get('price_usd')
                                    existing.daily_change_percent = item.get('daily_change_percent')
                                    existing.country = item.get('country')
                                    existing.asset_type_id = asset_type_id
                                    existing.asset_id = asset_id
                                    existing.last_updated = datetime.now()
                                    db.commit()
                                    logger.debug(f"[WorldAssetsRanking] ì—…ë°ì´íŠ¸: {ticker} ({data_source})")
                            else:
                                db.rollback()
                                raise e
                        
                        saved_count += 1
                        
                    except Exception as e:
                        failed_count += 1
                        logger.error(f"WorldAssetsRanking ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}, item: {item}")
                        continue
                
                db.commit()
                logger.info(f"[WorldAssetsRanking] PostgreSQL ì €ì¥ ì™„ë£Œ: {saved_count}ê°œ ì„±ê³µ, {failed_count}ê°œ ì‹¤íŒ¨")
            
                # PostgreSQL ì´ì¤‘ ì €ì¥
                try:
                    from ..core.database import get_postgres_db
                    pg_db = next(get_postgres_db())
                    try:
                        pg_saved_count = 0
                        pg_failed_count = 0
                        for item in items:
                            try:
                                ranking_date = metadata.get('collection_date', datetime.now().date())
                                if isinstance(ranking_date, str):
                                    ranking_date = datetime.fromisoformat(ranking_date).date()
                                
                                data_source = metadata.get('data_source', 'unknown')
                                ticker = item.get('ticker')
                                
                                # PostgreSQL UPSERT using ON CONFLICT
                                from sqlalchemy.dialects.postgresql import insert as pg_insert
                                
                                pg_data = {
                                    'rank': item.get('rank'),
                                    'name': item.get('name'),
                                    'ticker': item.get('ticker'),
                                    'market_cap_usd': item.get('market_cap_usd'),
                                    'price_usd': item.get('price_usd'),
                                    'daily_change_percent': item.get('daily_change_percent'),
                                    'country': item.get('country'),
                                    'asset_type_id': item.get('asset_type_id'),
                                    'asset_id': item.get('asset_id'),
                                    'ranking_date': ranking_date,
                                    'data_source': data_source,
                                    'last_updated': datetime.now()
                                }
                                
                                stmt = pg_insert(WorldAssetsRanking).values(**pg_data)
                                stmt = stmt.on_conflict_do_update(
                                    index_elements=['ranking_date', 'ticker', 'data_source'],
                                    set_={
                                        'rank': stmt.excluded.rank,
                                        'name': stmt.excluded.name,
                                        'market_cap_usd': stmt.excluded.market_cap_usd,
                                        'price_usd': stmt.excluded.price_usd,
                                        'daily_change_percent': stmt.excluded.daily_change_percent,
                                        'country': stmt.excluded.country,
                                        'asset_type_id': stmt.excluded.asset_type_id,
                                        'asset_id': stmt.excluded.asset_id,
                                        'last_updated': stmt.excluded.last_updated
                                    }
                                )
                                pg_db.execute(stmt)
                                logger.debug(f"[WorldAssetsRanking PG] UPSERT: {ticker} ({data_source})")
                                
                                pg_saved_count += 1
                                
                            except Exception as e:
                                pg_failed_count += 1
                                logger.error(f"PostgreSQL WorldAssetsRanking ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}, item: {item}")
                                continue
                        
                        pg_db.commit()
                        logger.info(f"[WorldAssetsRanking] PostgreSQL ì €ì¥ ì™„ë£Œ: {pg_saved_count}ê°œ ì„±ê³µ, {pg_failed_count}ê°œ ì‹¤íŒ¨")
                        
                    except Exception as e:
                        logger.error(f"PostgreSQL WorldAssetsRanking ì €ì¥ ì‹¤íŒ¨: {e}")
                        pg_db.rollback()
                    finally:
                        pg_db.close()
                except Exception as e:
                    logger.error(f"PostgreSQL ì—°ê²° ì‹¤íŒ¨: {e}")
            
            return True
            
        except Exception as e:
            logger.error(f"WorldAssetsRanking ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    async def _save_technical_indicators(self, items: List[Dict[str, Any]]) -> bool:
        """ê¸°ìˆ ì  ì§€í‘œ ë°ì´í„° ì €ì¥ - PostgreSQLë§Œ ì‚¬ìš©"""
        try:
            if not items:
                return True
                
            logger.info(f"ê¸°ìˆ ì  ì§€í‘œ ë°ì´í„° ì €ì¥: {len(items)}ê°œ ë ˆì½”ë“œ")
            
            # PostgreSQLë§Œ ì‚¬ìš©
            logger.debug(f"[TechnicalIndicator] PostgreSQL ì €ì¥ ì¤€ë¹„: {len(items)}ê°œ ë ˆì½”ë“œ")
            
            # PostgreSQL ì´ì¤‘ ì €ì¥
            try:
                from ..core.database import get_postgres_db
                pg_db = next(get_postgres_db())
                try:
                    # TODO: TechnicalIndicator ëª¨ë¸ì´ êµ¬í˜„ë˜ë©´ ì‹¤ì œ ì €ì¥ ë¡œì§ ì¶”ê°€
                    logger.debug(f"[TechnicalIndicator] PostgreSQL ì €ì¥ ì¤€ë¹„: {len(items)}ê°œ ë ˆì½”ë“œ")
                    pg_db.commit()
                    logger.debug(f"[TechnicalIndicator] PostgreSQL ì €ì¥ ì™„ë£Œ")
                except Exception as e:
                    pg_db.rollback()
                    logger.warning(f"[TechnicalIndicator] PostgreSQL ì €ì¥ ì‹¤íŒ¨: {e}")
                finally:
                    pg_db.close()
            except Exception as e:
                logger.warning(f"[TechnicalIndicator] PostgreSQL ì—°ê²° ì‹¤íŒ¨: {e}")
            
            return True
        except Exception as e:
            logger.error(f"ê¸°ìˆ ì  ì§€í‘œ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    async def _save_onchain_metric(self, items: List[Dict[str, Any]]) -> bool:
        """ì˜¨ì²´ì¸ ë©”íŠ¸ë¦­ ë°ì´í„° ì €ì¥ - PostgreSQLë§Œ ì‚¬ìš©"""
        try:
            if not items:
                logger.info("[OnchainMetric] ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return True
                
            logger.info(f"[OnchainMetric] ë°ì´í„° ì €ì¥ ì‹œì‘: {len(items)}ê°œ ë ˆì½”ë“œ")
            
            # ë°ì´í„° ê²€ì¦ ë° í†µê³„ ìˆ˜ì§‘
            valid_items = 0
            invalid_items = 0
            missing_asset_id = 0
            missing_timestamp = 0
            metric_stats = {}
            
            for item in items:
                # í•„ìˆ˜ í•„ë“œ ê²€ì¦
                if not item.get('asset_id'):
                    missing_asset_id += 1
                    logger.warning(f"[OnchainMetric] asset_id ëˆ„ë½: {item}")
                    continue
                    
                if not item.get('timestamp_utc'):
                    missing_timestamp += 1
                    logger.warning(f"[OnchainMetric] timestamp_utc ëˆ„ë½: {item}")
                    continue
                
                # ë©”íŠ¸ë¦­ë³„ í†µê³„ ìˆ˜ì§‘
                for key in ['mvrv_z_score', 'nupl', 'sopr', 'hashrate', 'difficulty']:
                    if key in item and item[key] is not None:
                        if key not in metric_stats:
                            metric_stats[key] = {'count': 0, 'min': float('inf'), 'max': float('-inf')}
                        metric_stats[key]['count'] += 1
                        metric_stats[key]['min'] = min(metric_stats[key]['min'], float(item[key]))
                        metric_stats[key]['max'] = max(metric_stats[key]['max'], float(item[key]))
                
                valid_items += 1
            
            invalid_items = len(items) - valid_items
            
            # ê²€ì¦ ê²°ê³¼ ë¡œê·¸
            logger.info(f"[OnchainMetric] ë°ì´í„° ê²€ì¦ ì™„ë£Œ:")
            logger.info(f"  - ì´ ë ˆì½”ë“œ: {len(items)}ê°œ")
            logger.info(f"  - ìœ íš¨í•œ ë ˆì½”ë“œ: {valid_items}ê°œ")
            logger.info(f"  - ë¬´íš¨í•œ ë ˆì½”ë“œ: {invalid_items}ê°œ")
            logger.info(f"  - asset_id ëˆ„ë½: {missing_asset_id}ê°œ")
            logger.info(f"  - timestamp ëˆ„ë½: {missing_timestamp}ê°œ")
            
            # ë©”íŠ¸ë¦­ë³„ í†µê³„ ë¡œê·¸
            for metric, stats in metric_stats.items():
                logger.info(f"  - {metric}: {stats['count']}ê°œ (ë²”ìœ„: {stats['min']:.4f} ~ {stats['max']:.4f})")
            
            if valid_items == 0:
                logger.error("[OnchainMetric] ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ì–´ ì €ì¥ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                return False
            
            # PostgreSQLë§Œ ì‚¬ìš©
            
            # PostgreSQL ì €ì¥
            logger.info(f"[OnchainMetric] PostgreSQL ì €ì¥ ì‹œì‘...")
            try:
                from ..core.database import get_postgres_db
                pg_db = next(get_postgres_db())
                try:
                    from ..models.asset import CryptoMetric as PGCryptoMetric
                    from sqlalchemy.dialects.postgresql import insert as pg_insert
                    from sqlalchemy import func
                    
                    pg_saved_count = 0
                    pg_failed_count = 0
                    
                    for i, item in enumerate(items):
                        try:
                            # í•„ìˆ˜ í•„ë“œ ì¬ê²€ì¦
                            if not item.get('asset_id') or not item.get('timestamp_utc'):
                                pg_failed_count += 1
                                continue
                            
                            # HODL Age ë¶„í¬ë¥¼ JSONìœ¼ë¡œ ë³€í™˜
                            hodl_age_distribution = {}
                            hodl_age_keys = [
                                'hodl_age_0d_1d', 'hodl_age_1d_1w', 'hodl_age_1w_1m', 'hodl_age_1m_3m',
                                'hodl_age_3m_6m', 'hodl_age_6m_1y', 'hodl_age_1y_2y', 'hodl_age_2y_3y',
                                'hodl_age_3y_4y', 'hodl_age_4y_5y', 'hodl_age_5y_7y', 'hodl_age_7y_10y',
                                'hodl_age_10y'
                            ]
                            
                            hodl_age_count = 0
                            for key in hodl_age_keys:
                                if key in item and item[key] is not None:
                                    json_key = key.replace('hodl_age_', '')
                                    hodl_age_distribution[json_key] = float(item[key])
                                    hodl_age_count += 1
                            
                            logger.debug(f"[OnchainMetric PG] ë ˆì½”ë“œ {i+1}/{len(items)}: asset_id={item.get('asset_id')}, "
                                       f"timestamp={item.get('timestamp_utc')}, hodl_age_points={hodl_age_count}")
                            
                            # PostgreSQL UPSERT
                            pg_data = {
                                'asset_id': item.get('asset_id'),
                                'timestamp_utc': item.get('timestamp_utc'),
                                'hodl_age_distribution': hodl_age_distribution if hodl_age_distribution else None,
                                'hashrate': item.get('hashrate'),
                                'difficulty': item.get('difficulty'),
                                'miner_reserves': item.get('miner_reserves'),
                                'realized_cap': item.get('realized_cap'),
                                'mvrv_z_score': item.get('mvrv_z_score'),
                                'realized_price': item.get('realized_price'),
                                'sopr': item.get('sopr'),
                                'nupl': item.get('nupl'),
                                'cdd_90dma': item.get('cdd_90dma'),
                                'true_market_mean': item.get('true_market_mean'),
                                'nrpl_btc': item.get('nrpl_btc'),
                                'aviv': item.get('aviv'),
                                'thermo_cap': item.get('thermo_cap'),
                                'hodl_waves_supply': item.get('hodl_waves_supply'),
                                'etf_btc_total': item.get('etf_btc_total'),
                                'etf_btc_flow': item.get('etf_btc_flow'),
                                
                                # Futures ë°ì´í„° (JSON í˜•íƒœ: {"total": ..., "exchanges": {...}})
                                'open_interest_futures': item.get('open_interest_futures')
                            }
                            
                            # None ê°’ ì œê±°
                            pg_data = {k: v for k, v in pg_data.items() if v is not None}
                            
                            stmt = pg_insert(PGCryptoMetric).values(**pg_data)
                            stmt = stmt.on_conflict_do_update(
                                index_elements=['asset_id', 'timestamp_utc'],
                                set_={
                                    'hodl_age_distribution': stmt.excluded.hodl_age_distribution,
                                    'hashrate': stmt.excluded.hashrate,
                                    'difficulty': stmt.excluded.difficulty,
                                    'miner_reserves': stmt.excluded.miner_reserves,
                                    'realized_cap': stmt.excluded.realized_cap,
                                    'mvrv_z_score': stmt.excluded.mvrv_z_score,
                                    'realized_price': stmt.excluded.realized_price,
                                    'sopr': stmt.excluded.sopr,
                                    'nupl': stmt.excluded.nupl,
                                    'cdd_90dma': stmt.excluded.cdd_90dma,
                                    'true_market_mean': stmt.excluded.true_market_mean,
                                    'nrpl_btc': stmt.excluded.nrpl_btc,
                                    'aviv': stmt.excluded.aviv,
                                    'thermo_cap': stmt.excluded.thermo_cap,
                                    'hodl_waves_supply': stmt.excluded.hodl_waves_supply,
                                    'etf_btc_total': stmt.excluded.etf_btc_total,
                                    'etf_btc_flow': stmt.excluded.etf_btc_flow,
                                    'open_interest_futures': stmt.excluded.open_interest_futures,
                                    'updated_at': func.now()
                                }
                            )
                            pg_db.execute(stmt)
                            pg_saved_count += 1
                            
                        except Exception as e:
                            pg_failed_count += 1
                            logger.error(f"[OnchainMetric PG] ë ˆì½”ë“œ {i+1} ì €ì¥ ì‹¤íŒ¨: {e}, ë°ì´í„°: {item}")
                            continue
                    
                    pg_db.commit()
                    logger.info(f"[OnchainMetric] PostgreSQL ì €ì¥ ì™„ë£Œ: {pg_saved_count}ê°œ ì„±ê³µ, {pg_failed_count}ê°œ ì‹¤íŒ¨")
                    
                except Exception as e:
                    logger.error(f"[OnchainMetric PG] ì €ì¥ ì‹¤íŒ¨: {e}")
                    pg_db.rollback()
                finally:
                    pg_db.close()
            except Exception as e:
                logger.error(f"[OnchainMetric PG] ì—°ê²° ì‹¤íŒ¨: {e}")
            
            # ìµœì¢… ê²°ê³¼ ìš”ì•½
            logger.info(f"[OnchainMetric] ì €ì¥ ì™„ë£Œ ìš”ì•½:")
            logger.info(f"  - MySQL: ì‚­ì œë¨")
            logger.info(f"  - PostgreSQL: {pg_saved_count}ê°œ ì„±ê³µ, {pg_failed_count}ê°œ ì‹¤íŒ¨")
            logger.info(f"  - ì „ì²´ ì„±ê³µë¥ : {(pg_saved_count / len(items) * 100):.1f}%")
            
            return True
        except Exception as e:
            logger.error(f"[OnchainMetric] ì €ì¥ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
            return False

    async def _update_asset_settings(self, payload: Dict[str, Any]) -> bool:
        """ìì‚° ì„¤ì • ì—…ë°ì´íŠ¸ (íë¥¼ í†µí•œ ê°„ë‹¨ ì„¤ì • ë°˜ì˜)"""
        logger.info(f"ìì‚° ì„¤ì • ì—…ë°ì´íŠ¸ íƒœìŠ¤í¬ ì²˜ë¦¬: {payload}")
        return True

    async def _log_stats(self):
        """ì²˜ë¦¬ í†µê³„ ë¡œê¹…"""
        if self.stats["last_processed"]:
            logger.info(
                f"Data Processor í†µê³„ - "
                f"ì‹¤ì‹œê°„: {self.stats['realtime_processed']}, "
                f"ë°°ì¹˜: {self.stats['batch_processed']}, "
                f"ì˜¤ë¥˜: {self.stats['errors']}"
            )

    async def start(self):
        """Data Processor ì‹œì‘"""
        logger.info("Data Processor ì„œë¹„ìŠ¤ ì‹œì‘")
        logger.info("Data Processor start() ë©”ì„œë“œ í˜¸ì¶œë¨")
        self.running = True
        
        # Redis ì—°ê²°
        if not await self._connect_redis():
            logger.error("Redis ì—°ê²° ì‹¤íŒ¨ë¡œ ì„œë¹„ìŠ¤ ì¢…ë£Œ")
            return
            
        logger.info("Redis ì—°ê²° ì„±ê³µ, ë©”ì¸ ë£¨í”„ ì‹œì‘")
        try:
            logger.info("Data Processor main loop started")
            while self.running:
                start_time = time.time()
                logger.debug("Processing cycle started")
                
                # ì‹¤ì‹œê°„ ë° ë°°ì¹˜ ë°ì´í„° ë™ì‹œ ì²˜ë¦¬
                logger.debug("ğŸ”„ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì‹œì‘")
                try:
                    logger.debug("ğŸ”„ asyncio.gather í˜¸ì¶œ ì „")
                    gather_result = await asyncio.gather(
                        self._process_realtime_streams(),
                        self._process_batch_queue(),
                        return_exceptions=True
                    )
                    # Ensure tuple unpack with defaults and numeric types
                    realtime_count = 0
                    batch_count = 0
                    if isinstance(gather_result, (list, tuple)):
                        if len(gather_result) > 0 and not isinstance(gather_result[0], Exception):
                            realtime_count = int(gather_result[0] or 0)
                        if len(gather_result) > 1 and not isinstance(gather_result[1], Exception):
                            batch_count = int(gather_result[1] or 0)
                    logger.debug(f"ğŸ”„ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì™„ë£Œ: {realtime_count}")
                except Exception as e:
                    import traceback
                    logger.error(f"âŒ asyncio.gather ì˜¤ë¥˜: {e}")
                    logger.error(f"ğŸ” ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
                    realtime_count, batch_count = 0, 0
                
                # ê²°ê³¼ ì²˜ë¦¬
                if isinstance(realtime_count, Exception):
                    logger.error(f"ì‹¤ì‹œê°„ ì²˜ë¦¬ ì˜¤ë¥˜: {realtime_count}")
                    realtime_count = 0
                if isinstance(batch_count, Exception):
                    logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {batch_count}")
                    batch_count = 0
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                self.stats["realtime_processed"] += realtime_count
                self.stats["batch_processed"] += batch_count
                self.stats["last_processed"] = datetime.utcnow()
                
                # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚° ë° ëŒ€ê¸°
                processing_time = time.time() - start_time
                if processing_time < self.processing_interval:
                    await asyncio.sleep(self.processing_interval - processing_time)
                    
        except KeyboardInterrupt:
            logger.info("Data Processor ì„œë¹„ìŠ¤ ì¢…ë£Œ ìš”ì²­")
        except Exception as e:
            logger.error(f"Data Processor ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {e}")
        finally:
            self.running = False
            if self.redis_client:
                await self.redis_client.close()
            logger.info("Data Processor ì„œë¹„ìŠ¤ ì¢…ë£Œ")

    async def stop(self):
        """Data Processor ì¤‘ì§€"""
        self.running = False
        await self._log_stats()

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
try:
    logger.info("DataProcessor ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹œì‘...")
    data_processor = DataProcessor()
    logger.info("DataProcessor ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ")
except Exception as e:
    logger.error(f"DataProcessor ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
    raise

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("DataProcessor main() í•¨ìˆ˜ ì‹œì‘")
    await data_processor.start()

if __name__ == "__main__":
    asyncio.run(main())
