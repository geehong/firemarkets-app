"""
Data Processor Service - ì¤‘ì•™í™”ëœ ë°ì´í„° ì²˜ë¦¬ ì„œë¹„ìŠ¤
Redis Streamê³¼ Queueì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ ê²€ì¦í•˜ê³  PostgreSQL DBì— ì €ìž¥
Refactored to use Validator, Adapter, Repository, and Consumer components.
"""
import asyncio
import json
import logging
import time
import datetime
from typing import Dict, List, Any, Optional
from datetime import timezone

import redis.asyncio as redis

from ..core.config import GLOBAL_APP_CONFIGS
from ..models.asset import Asset
from ..core.database import get_postgres_db
from ..utils.logger import logger
from ..utils.redis_queue_manager import RedisQueueManager

# New components
from .processor.validator import DataValidator
from .processor.adapters import AdapterFactory
from .processor.repository import DataRepository
from .processor.consumer import StreamConsumer

class DataProcessor:
    """
    ì¤‘ì•™í™”ëœ ë°ì´í„° ì²˜ë¦¬ ì„œë¹„ìŠ¤
    - Redis Streamì—ì„œ ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ (StreamConsumer ìœ„ìž„)
    - Redis Queueì—ì„œ ë°°ì¹˜ ë°ì´í„° ì²˜ë¦¬
    - ë°ì´í„° ê²€ì¦ ë° ë³€í™˜ (DataValidator ìœ„ìž„)
    - PostgreSQL DB ì €ìž¥ (DataRepository ìœ„ìž„)
    """

    def __init__(self, config_manager=None, redis_queue_manager=None):
        self.config_manager = config_manager
        self.queue_manager = redis_queue_manager
        self.running = False
        self.task = None
        
        # ì„¤ì • ë¡œë“œ
        self._load_initial_configs()
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.validator = DataValidator()
        self.adapter_factory = AdapterFactory(self.validator)
        self.repository = DataRepository(self.validator)
        
        # Redis ì„¤ì •
        self.redis_host = GLOBAL_APP_CONFIGS.get("REDIS_HOST", "redis")
        self.redis_port = GLOBAL_APP_CONFIGS.get("REDIS_PORT", 6379)
        self.redis_db = GLOBAL_APP_CONFIGS.get("REDIS_DB", 0)
        self.redis_password = GLOBAL_APP_CONFIGS.get("REDIS_PASSWORD")
        self.redis_url = f"redis://{self.redis_host}:{self.redis_port}/{self.redis_db}"
        
        # StreamConsumer ì´ˆê¸°í™”
        self.stream_consumer = StreamConsumer(
            redis_url=self.redis_url,
            adapter_factory=self.adapter_factory,
            repository=self.repository,
            batch_size=GLOBAL_APP_CONFIGS.get("BATCH_SIZE", 100)
        )
        
        self.redis_client = None # ì§ì ‘ ì‚¬ìš© ìµœì†Œí™”, Consumerê°€ ê´€ë¦¬
        
        # ë°°ì¹˜ í ì„¤ì •
        self.batch_queue = "batch_data_queue"
        self.max_retries = 3
        self.retry_delay = 1.0

        # í†µê³„
        self.stats = {
            "processed_count": 0,
            "errors": 0,
            "start_time": time.time()
        }
        
        # Failover ê´€ë ¨ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        self.backup_sources = ["api_fallback"]
        self.current_source_index = 0
        self.source_failures = {}
        self.max_failures_per_source = 5

        logger.info("DataProcessor (Refactored) ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ")

    def _load_initial_configs(self):
        """ì´ˆê¸° ì„¤ì •ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            from ..core.config import load_and_set_global_configs
            load_and_set_global_configs()
            logger.info("âœ… ì´ˆê¸° ì„¤ì • ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ì´ˆê¸° ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")

    async def start(self):
        """ì„œë¹„ìŠ¤ ì‹œìž‘"""
        self.running = True
        logger.info("ðŸš€ DataProcessor ì„œë¹„ìŠ¤ ì‹œìž‘")
        
        # Redis ì—°ê²° (Consumer ë‚´ë¶€ì—ì„œ ì²˜ë¦¬í•˜ì§€ë§Œ, ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì—¬ê¸°ì„œë„ í•„ìš”í•  ìˆ˜ ìžˆìŒ)
        # ë°°ì¹˜ ì²˜ë¦¬ëŠ” queue_managerë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ì§ì ‘ ì—°ê²°
        if not self.queue_manager:
             self.redis_client = await redis.from_url(self.redis_url)

        # StreamConsumer ì—°ê²°
        await self.stream_consumer.connect()
        
        # ìžì‚° ë§µ ë¡œë“œ ë° ì£¼ìž…
        await self._refresh_asset_map()

        # ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ë¥¼ ë³„ë„ taskë¡œ ì‹¤í–‰ (ë°°ì¹˜ ì²˜ë¦¬ì™€ ë³‘ë ¬ ë™ìž‘)
        stream_task = asyncio.create_task(self._stream_processing_loop())
        logger.info("ðŸ“¡ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ íƒœìŠ¤í¬ ì‹œìž‘")

        # ë©”ì¸ ë£¨í”„ (ë°°ì¹˜ ì²˜ë¦¬ ì „ë‹´)
        loop_count = 0
        while self.running:
            try:
                loop_count += 1
                
                # ë°°ì¹˜ í ì²˜ë¦¬
                batch_count = await self._process_batch_queue()
                
                if batch_count > 0:
                    self.stats["processed_count"] += batch_count
                    logger.info(f"âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {batch_count}ê°œ íƒœìŠ¤í¬")
                
                # ì£¼ê¸°ì ìœ¼ë¡œ í†µê³„ ë¡œê¹… (ì•½ 10ì´ˆë§ˆë‹¤ = 20 loops @ 0.5s sleep)
                if loop_count % 20 == 0:
                    elapsed = time.time() - self.stats["start_time"]
                    logger.info(f"ðŸ“Š ì²˜ë¦¬ í†µê³„: ì´ {self.stats['processed_count']}ê°œ ì²˜ë¦¬, ì—ëŸ¬ {self.stats['errors']}ê°œ, ì‹¤í–‰ ì‹œê°„ {elapsed:.0f}ì´ˆ")
                
                # CPU ì‚¬ìš©ëŸ‰ ì¡°ì ˆ
                if batch_count == 0:
                    await asyncio.sleep(0.5)  # Idle ëŒ€ê¸°
                else:
                    await asyncio.sleep(0.05)  # ì²˜ë¦¬ í›„ ì§§ì€ ëŒ€ê¸°
                    
            except Exception as e:
                logger.error(f"DataProcessor ë©”ì¸ ë£¨í”„ ì˜¤ë¥˜: {e}", exc_info=True)
                self.stats["errors"] += 1
                await asyncio.sleep(1)
        
        # ì¢…ë£Œ ì‹œ ìŠ¤íŠ¸ë¦¼ taskë„ ì·¨ì†Œ
        stream_task.cancel()
        try:
            await stream_task
        except asyncio.CancelledError:
            pass

    async def _stream_processing_loop(self):
        """ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ë£¨í”„ (ë³„ë„ taskë¡œ ì‹¤í–‰)"""
        logger.info("ðŸ“¡ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ë£¨í”„ ì‹œìž‘")
        while self.running:
            try:
                stream_count = await self.stream_consumer.process_streams()
                if stream_count > 0:
                    self.stats["processed_count"] += stream_count
                    logger.info(f"ðŸ“ˆ ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬: {stream_count}ê°œ ë ˆì½”ë“œ")
                # ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ê°„ê²© (process_streams ë‚´ë¶€ì—ì„œ ì´ë¯¸ ëŒ€ê¸°í•˜ë¯€ë¡œ ì§§ê²Œ)
                await asyncio.sleep(0.1)
            except asyncio.CancelledError:
                logger.info("ðŸ“¡ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ë£¨í”„ ì¢…ë£Œ")
                break
            except Exception as e:
                logger.error(f"ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}", exc_info=True)
                self.stats["errors"] += 1
                await asyncio.sleep(2)  # ì—ëŸ¬ ì‹œ ë” ê¸´ ëŒ€ê¸°

    async def stop(self):
        """ì„œë¹„ìŠ¤ ì¢…ë£Œ"""
        self.running = False
        logger.info("ðŸ›‘ DataProcessor ì„œë¹„ìŠ¤ ì¢…ë£Œ ì¤‘...")
        if self.redis_client:
            await self.redis_client.close()

    async def _refresh_asset_map(self):
        """ìžì‚° ID ë§¤í•‘ ê°±ì‹ """
        try:
            pg_db = next(get_postgres_db())
            try:
                assets = pg_db.query(Asset.ticker, Asset.asset_id).all()
                asset_map = {ticker: asset_id for ticker, asset_id in assets}
                self.stream_consumer.set_asset_map(asset_map)
                logger.info(f"ìžì‚° ë§µ ê°±ì‹  ì™„ë£Œ: {len(asset_map)}ê°œ")
            finally:
                pg_db.close()
        except Exception as e:
            logger.error(f"ìžì‚° ë§µ ê°±ì‹  ì‹¤íŒ¨: {e}")

    async def _process_batch_queue(self) -> int:
        """ë°°ì¹˜ í ë°ì´í„° ì²˜ë¦¬"""
        processed_count = 0
        try:
            # íì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ìµœëŒ€ 100ê°œì”©)
            for i in range(100):
                task_wrapper = None
                if self.queue_manager:
                    task_wrapper = await self.queue_manager.pop_batch_task(timeout_seconds=0.1)
                elif self.redis_client:
                    result = await self.redis_client.blpop(self.batch_queue, timeout=0.1)
                    if result:
                        _, task_data = result
                        try:
                            task_wrapper = json.loads(task_data)
                        except json.JSONDecodeError:
                            pass

                if not task_wrapper:
                    break
                
                # íƒœìŠ¤í¬ ì²˜ë¦¬
                task_type = task_wrapper.get('type', 'unknown')
                success = await self._process_batch_task(task_wrapper)
                if success:
                    processed_count += 1
                    # ë°°ì¹˜ íƒœìŠ¤í¬ ì„±ê³µ ë¡œê·¸ (OHLCV, macrotrends ë“± ì£¼ìš” íƒ€ìž…ë§Œ)
                    if task_type in ('ohlcv_day_data', 'ohlcv_intraday_data', 'macrotrends_financials'):
                        payload = task_wrapper.get('payload', {})
                        items_count = len(payload.get('items', [])) if isinstance(payload, dict) else 0
                        logger.info(f"âœ… ë°°ì¹˜ íƒœìŠ¤í¬ ì²˜ë¦¬ ì„±ê³µ: {task_type} ({items_count}ê±´)")
                else:
                    # ì‹¤íŒ¨ ì‹œ DLQ ë“± ì²˜ë¦¬ (ì—¬ê¸°ì„œëŠ” ë¡œê·¸ë§Œ)
                    logger.warning(f"ë°°ì¹˜ íƒœìŠ¤í¬ ì²˜ë¦¬ ì‹¤íŒ¨: {task_type}")
                
                # CPU ë¶€í•˜ ì™„í™”: 10ê°œ ì²˜ë¦¬ë§ˆë‹¤ ì§§ì€ ëŒ€ê¸°
                if (i + 1) % 10 == 0:
                    await asyncio.sleep(0.001)
                    
        except Exception as e:
            logger.error(f"ë°°ì¹˜ í ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            self.stats["errors"] += 1
            
        return processed_count

    async def _process_batch_task(self, task: Dict[str, Any]) -> bool:
        """ë°°ì¹˜ íƒœìŠ¤í¬ ì²˜ë¦¬"""
        try:
            task_type = task.get("type")
            payload = task.get("payload")
            
            if not task_type or not payload:
                return False
                
            items = payload.get("items") if isinstance(payload, dict) else None
            if items is None:
                items = payload if isinstance(payload, list) else [payload]

            # Repositoryë¡œ ìœ„ìž„
            if task_type == "stock_profile":
                return await self.repository.save_stock_profile(items)
            elif task_type == "stock_financials":
                return await self.repository.save_stock_financials(items)
            elif task_type == "stock_estimate":
                return await self.repository.save_stock_estimate(items)
            elif task_type == "etf_info":
                return await self.repository.save_etf_info(items)
            elif task_type in ("crypto_info", "crypto_data"):
                return await self.repository.save_crypto_data(items)
            elif task_type in ("ohlcv_data", "ohlcv_day_data", "ohlcv_intraday_data"):
                meta = {}
                if isinstance(payload, dict):
                    meta = payload.get("metadata") or payload.get("meta") or {}
                if not meta:
                    meta = task.get("meta") or {}
                return await self.repository.save_ohlcv_data(items, meta)
            elif task_type == "world_assets_ranking":
                return await self.repository.save_world_assets_ranking(items, task.get("meta", {}))
            elif task_type == "macrotrends_financials":
                return await self.repository.save_macrotrends_financials(items)
            else:
                logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” íƒœìŠ¤í¬ íƒ€ìž…: {task_type}")
                return False
                
        except Exception as e:
            logger.error(f"ë°°ì¹˜ íƒœìŠ¤í¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return False

    # Failover methods (kept for compatibility if needed, though usage should be checked)
    def get_current_source(self):
        return self.backup_sources[self.current_source_index]
    
    def mark_source_failure(self, source):
        if source not in self.source_failures:
            self.source_failures[source] = 0
        self.source_failures[source] += 1
        if self.source_failures[source] >= self.max_failures_per_source:
            self.switch_to_next_source()
            
    def switch_to_next_source(self):
        self.current_source_index = (self.current_source_index + 1) % len(self.backup_sources)
