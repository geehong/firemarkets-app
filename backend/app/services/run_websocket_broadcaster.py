"""
WebSocket Broadcaster Service
- Connects to Redis Pub/Sub.
- Listens for real-time quote messages.
- Forwards messages to the main backend service via an internal Socket.IO connection.
"""

import asyncio
import json
import logging
import os
import signal
import sys
import time
from pathlib import Path
from datetime import datetime, timezone, timedelta
from typing import Dict, Optional, List

import redis.asyncio as redis
from redis import exceptions
import socketio

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(project_root))

from app.core.config import GLOBAL_APP_CONFIGS, load_and_set_global_configs
from app.utils.helpers import safe_float

# ë¡œê¹… ì„¤ì • (í™˜ê²½ë³€ìˆ˜ë¡œ ìƒì„¸ ë¡œê·¸ ì œì–´)
verbose = os.getenv("BROADCASTER_VERBOSE", "false").lower() == "true"
log_level = logging.DEBUG if verbose else logging.INFO

logging.basicConfig(level=log_level, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")

import sys
_sh = logging.StreamHandler(sys.stdout)
_sh.setLevel(log_level)
_sh.setFormatter(logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s"))
root_logger = logging.getLogger()
root_logger.handlers = []
root_logger.addHandler(_sh)
root_logger.setLevel(log_level)

logger = logging.getLogger("WebSocketBroadcaster")
logger.setLevel(log_level)

# ì „ì—­ ì„¤ì • ë¡œë“œ
try:
    load_and_set_global_configs()
    logger.info("âœ… Global configurations loaded.")
except Exception as e:
    logger.error(f"âŒ Failed to load global configurations: {e}")

# Socket.IO í´ë¼ì´ì–¸íŠ¸ ì„¤ì • (ë°±ì—”ë“œ ì„œë²„ì— ì—°ê²°)
sio_client = socketio.AsyncClient(logger=verbose, engineio_logger=verbose)

# --- Broadcaster ì „ìš© ìƒíƒœ ë° ìºì‹œ ê´€ë¦¬ ---
prev_close_cache: Dict[int, float] = {}
last_cache_refresh: Optional[datetime] = None
cache_refresh_interval = timedelta(minutes=60)

ticker_to_asset_id_cache: Dict[str, int] = {}
last_asset_cache_refresh: Optional[datetime] = None
asset_cache_refresh_interval = timedelta(minutes=10)

# REALTIME_STREAMS ì„¤ì •ì´ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ê¸°ë³¸ê°’
default_streams = ["binance:realtime", "coinbase:realtime", "finnhub:realtime", "alpaca:realtime", "swissquote:realtime"]
stream_names = GLOBAL_APP_CONFIGS.get("REALTIME_STREAMS", default_streams)
realtime_streams = {
    stream: f"{stream.split(':')[0]}_broadcaster_group" for stream in stream_names
}



@sio_client.event
async def connect():
    logger.info("âœ… 'backend' ì„œë¹„ìŠ¤ì— ì„±ê³µì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")


@sio_client.event
async def disconnect():
    logger.warning("ğŸ”Œ 'backend' ì„œë¹„ìŠ¤ì™€ì˜ ì—°ê²°ì´ ëŠì–´ì¡ŒìŠµë‹ˆë‹¤.")


async def _refresh_asset_cache():
    """DBì—ì„œ Ticker -> Asset ID ë§µì„ ê°€ì ¸ì™€ ìºì‹œí•©ë‹ˆë‹¤."""
    global ticker_to_asset_id_cache, last_asset_cache_refresh
    logger.debug("ğŸ”„ Ticker-AssetID ìºì‹œ ê°±ì‹  ì‹œì‘...")
    from app.core.database import get_async_session_local
    from app.models.asset import Asset
    from sqlalchemy.future import select

    session_local = get_async_session_local()
    async with session_local() as session:
        try:
            result = await session.execute(select(Asset.ticker, Asset.asset_id))
            ticker_to_asset_id_cache = {ticker: asset_id for ticker, asset_id in result.all()}
            last_asset_cache_refresh = datetime.now(timezone.utc)
            logger.info(f"âœ… Ticker-AssetID ìºì‹œ ê°±ì‹  ì™„ë£Œ: {len(ticker_to_asset_id_cache)}ê°œ ìì‚°")
        except Exception as e:
            logger.error(f"âŒ Ticker-AssetID ìºì‹œ ê°±ì‹  ì‹¤íŒ¨: {e}")

async def _refresh_prev_close_cache():
    """DBì—ì„œ ì „ì¼ ì¢…ê°€ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ìºì‹œí•©ë‹ˆë‹¤."""
    global prev_close_cache, last_cache_refresh
    logger.debug("ğŸ”„ ì „ì¼ ì¢…ê°€ ìºì‹œ ê°±ì‹  ì‹œì‘...")
    from app.core.database import get_async_session_local
    from sqlalchemy import text

    session_local = get_async_session_local()
    async with session_local() as session:
        try:
            query = text("""
                WITH latest_ohlcv AS (
                    SELECT asset_id, close_price, ROW_NUMBER() OVER(PARTITION BY asset_id ORDER BY timestamp_utc DESC) as rn
                    FROM ohlcv_day_data
                )
                SELECT asset_id, close_price FROM latest_ohlcv WHERE rn = 1;
            """)
            result = await session.execute(query)
            rows = result.fetchall()
            prev_close_cache = {row[0]: float(row[1]) for row in rows if row[1] is not None}
            last_cache_refresh = datetime.now(timezone.utc)
            logger.info(f"âœ… ì „ì¼ ì¢…ê°€ ìºì‹œ ê°±ì‹  ì™„ë£Œ: {len(prev_close_cache)}ê°œ ìì‚°")
        except Exception as e:
            logger.error(f"âŒ ì „ì¼ ì¢…ê°€ ìºì‹œ ê°±ì‹  ì‹¤íŒ¨: {e}")

async def _calculate_change(asset_id: int, current_price: float) -> tuple:
    """ìºì‹œë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ê²© ë³€ë™ë¥ ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
    prev_close = prev_close_cache.get(asset_id)
    if prev_close is not None:
        try:
            change_amount = current_price - prev_close
            change_percent = (change_amount / prev_close) * 100.0 if prev_close != 0 else 0.0
            return change_amount, change_percent
        except (TypeError, ValueError):
            return None, None
    return None, None

async def listen_to_redis_and_broadcast():
    """Redis Streamì„ êµ¬ë…í•˜ê³  ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ë°±ì—”ë“œë¡œ ì „ì†¡í•˜ëŠ” ë©”ì¸ ë¡œì§"""
    redis_host = GLOBAL_APP_CONFIGS.get("REDIS_HOST", "redis")
    redis_port = GLOBAL_APP_CONFIGS.get("REDIS_PORT", 6379)
    redis_password = GLOBAL_APP_CONFIGS.get("REDIS_PASSWORD")
    redis_url = f"redis://{redis_host}:{redis_port}"
    if redis_password:
        redis_url = f"redis://:{redis_password}@{redis_host}:{redis_port}"

    logger.info(f"ğŸ”— Redis Stream ë¦¬ìŠ¤ë„ˆ ì‹œì‘: {redis_url}")

    while True:
        logger.debug("[Broadcaster] Main loop tick - preparing to connect to Redis")
        redis_client = None
        try:
            logger.debug("[Broadcaster] Connecting to Redis...")
            redis_client = await redis.from_url(redis_url)
            logger.debug("[Broadcaster] Redis connected. Creating/ensuring consumer groups...")
            for stream_name, group_name in realtime_streams.items():
                logger.debug(f"[Broadcaster] Ensure group '{group_name}' on stream '{stream_name}'")
                try:
                    await redis_client.xgroup_create(name=stream_name, groupname=group_name, id="0", mkstream=True)
                    logger.info(f"âœ… Consumer Group ìƒì„±: {group_name} on {stream_name}")
                except exceptions.ResponseError as e:
                    if "BUSYGROUP" in str(e):
                        logger.debug(f"â„¹ï¸ Consumer Group '{group_name}'ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
                    else:
                        logger.error(f"Consumer Group '{group_name}' ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)

            logger.debug("[Broadcaster] Entering Redis read loop...")
            while True:
                # ì£¼ê¸°ì ìœ¼ë¡œ ìºì‹œ ê°±ì‹ 
                now = datetime.now(timezone.utc)
                if not last_asset_cache_refresh or (now - last_asset_cache_refresh) > asset_cache_refresh_interval:
                    logger.debug("[Broadcaster] Refreshing asset cache...")
                    await _refresh_asset_cache()
                if not last_cache_refresh or (now - last_cache_refresh) > cache_refresh_interval:
                    logger.debug("[Broadcaster] Refreshing prev_close cache...")
                    await _refresh_prev_close_cache()

                # ê° ìŠ¤íŠ¸ë¦¼ì„ ìˆœíšŒí•˜ë©° ë°ì´í„° ì½ê¸°
                all_messages = []
                for stream_name, group_name in realtime_streams.items():
                    try:
                        logger.debug(f"[Broadcaster] XREADGROUP from '{stream_name}' as '{group_name}'...")
                        # ê° ìŠ¤íŠ¸ë¦¼ì—ì„œ ê°œë³„ì ìœ¼ë¡œ ë°ì´í„° ì½ê¸°
                        stream_data = await redis_client.xreadgroup(
                            groupname=group_name,
                            consumername=f"consumer-{int(time.time())}",
                            streams={stream_name: ">"},
                            count=100,
                            block=10  # ì§§ì€ ë¸”ë¡œí‚¹ ì‹œê°„
                        )
                        logger.debug(f"[Broadcaster] XREADGROUP result for '{stream_name}': {len(stream_data) if stream_data else 0} batches")
                        if stream_data:
                            # (stream_name, messages) íŠœí”Œì„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                            all_messages.extend(stream_data)
                    except exceptions.ResponseError as e:
                        logger.error(f"ìŠ¤íŠ¸ë¦¼ '{stream_name}' ì½ê¸° ì˜¤ë¥˜: {e}", exc_info=True)
                        continue

                for stream_name_bytes, messages in all_messages:
                    stream_name = stream_name_bytes.decode('utf-8') if isinstance(stream_name_bytes, bytes) else stream_name_bytes
                    group_name = realtime_streams.get(stream_name)

                    for message_id, message_data in messages:
                        try:
                            symbol = message_data.get(b'symbol', b'').decode('utf-8').upper()
                            price = safe_float(message_data.get(b'price', b'').decode('utf-8'))
                            volume = safe_float(message_data.get(b'volume', b'').decode('utf-8'))
                            provider = message_data.get(b'provider', b'unknown').decode('utf-8')

                            if not symbol or price is None:
                                continue

                            asset_id = ticker_to_asset_id_cache.get(symbol)
                            if not asset_id:
                                continue

                            change_amount, change_percent = await _calculate_change(asset_id, price)

                            quote_data = {
                                "asset_id": asset_id,
                                "ticker": symbol,
                                "timestamp_utc": datetime.now(timezone.utc).isoformat(),
                                "price": price,
                                "volume": volume,
                                "change_amount": change_amount,
                                "change_percent": change_percent,
                                "data_source": provider
                            }

                            if sio_client.connected:
                                await sio_client.emit('broadcast_quote', quote_data)
                                logger.info(f"ğŸš€ ë°±ì—”ë“œë¡œ '{symbol}' ë°ì´í„° ì „ì†¡ ì™„ë£Œ")
                            else:
                                logger.warning("ë°±ì—”ë“œì™€ ì—°ê²°ë˜ì§€ ì•Šì•„ ë©”ì‹œì§€ë¥¼ ì „ì†¡í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                            # ë©”ì‹œì§€ ACK
                            await redis_client.xack(stream_name, group_name, message_id)

                        except Exception as e:
                            logger.exception(f"âŒ ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                            # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ACK ì²˜ë¦¬í•˜ì—¬ ë¬´í•œ ë£¨í”„ ë°©ì§€
                            await redis_client.xack(stream_name, group_name, message_id)

        except asyncio.CancelledError:
            logger.info("ğŸ›‘ Redis listener task was cancelled.")
            raise
        except (exceptions.ConnectionError, exceptions.TimeoutError) as e:
            logger.exception(f"âŒ Redis Stream ì—°ê²° ì˜¤ë¥˜: {e}")
        except Exception as e:
            logger.exception(f"âŒ Redis ë¦¬ìŠ¤ë„ˆ ë£¨í”„ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜: {e}")
        finally:
            logger.error("ğŸ”Œ Redis ì—°ê²° ì •ë¦¬ ë° 5ì´ˆ í›„ ì¬ì‹œë„...")
            if redis_client:
                try:
                    await redis_client.close()
                except Exception as redis_err:
                    logger.exception(f"Redis í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {redis_err}")
            await asyncio.sleep(5)


async def main():
    """ì„œë¹„ìŠ¤ ì‹œì‘ì """
    backend_url = "http://backend:8000"  # Docker ë„¤íŠ¸ì›Œí¬ ë‚´ë¶€ í†µì‹ 

    # ë°±ì—”ë“œì— ì—°ê²° ì‹œë„
    while not sio_client.connected:
        try:
            await sio_client.connect(backend_url, transports=["websocket"])
        except Exception as e:
            logger.critical(f"âŒ ë°±ì—”ë“œ({backend_url})ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 5ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤. ì˜¤ë¥˜: {e}")
            await asyncio.sleep(5)

    # Redis ë¦¬ìŠ¤ë„ˆ ì‹œì‘
    listener_task = asyncio.create_task(listen_to_redis_and_broadcast())

    def _signal_handler(*_):
        logger.info("SIGINT ë˜ëŠ” SIGTERM ìˆ˜ì‹ , ì¢…ë£Œí•©ë‹ˆë‹¤...")
        # íƒœìŠ¤í¬ë¥¼ ì§ì ‘ ì·¨ì†Œ
        if not listener_task.done():
            listener_task.cancel()

    loop = asyncio.get_running_loop()
    loop.add_signal_handler(signal.SIGINT, _signal_handler)
    loop.add_signal_handler(signal.SIGTERM, _signal_handler)

    try:
        # íƒœìŠ¤í¬ê°€ ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸°
        await listener_task
    except asyncio.CancelledError:
        logger.info("Listener task successfully cancelled.")
    finally:
        # ì •ë¦¬
        if sio_client.connected:
            await sio_client.disconnect()
        logger.info("ğŸ‘‹ Broadcaster ì„œë¹„ìŠ¤ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    asyncio.run(main())
